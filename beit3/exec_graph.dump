== Translation ==
Source graph:
Tensor_2: shape = [768, 3, 16, 16], stride = [768, 256, 16, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_3: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_5: shape = [1, 1, 768], stride = [768, 768, 1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_6: shape = [199, 768], stride = [768, 1], offset = 0, size = 152832, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_8: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_9: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_12: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_13: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_16: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_17: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_20: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_21: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_24: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_25: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_28: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_29: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_32: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_33: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_34: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_35: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_36: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_37: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_44: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_45: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_48: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_49: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_52: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_53: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_56: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_57: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_60: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_61: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_64: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_65: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_68: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_69: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_72: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_73: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_74: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_75: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_76: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_77: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_84: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_85: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_88: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_89: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_92: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_93: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_96: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_97: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_100: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_101: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_104: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_105: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_108: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_109: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_112: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_113: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_114: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_115: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_116: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_117: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_124: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_125: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_128: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_129: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_132: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_133: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_136: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_137: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_140: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_141: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_144: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_145: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_148: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_149: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_152: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_153: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_154: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_155: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_156: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_157: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_164: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_165: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_168: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_169: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_172: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_173: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_176: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_177: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_180: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_181: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_184: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_185: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_188: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_189: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_192: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_193: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_194: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_195: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_196: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_197: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_204: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_205: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_208: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_209: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_212: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_213: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_216: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_217: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_220: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_221: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_224: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_225: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_228: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_229: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_232: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_233: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_234: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_235: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_236: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_237: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_244: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_245: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_248: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_249: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_252: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_253: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_256: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_257: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_260: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_261: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_264: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_265: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_268: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_269: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_272: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_273: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_274: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_275: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_276: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_277: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_284: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_285: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_288: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_289: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_292: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_293: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_296: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_297: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_300: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_301: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_304: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_305: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_308: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_309: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_312: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_313: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_314: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_315: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_316: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_317: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_324: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_325: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_328: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_329: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_332: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_333: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_336: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_337: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_340: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_341: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_344: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_345: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_348: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_349: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_352: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_353: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_354: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_355: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_356: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_357: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_364: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_365: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_368: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_369: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_372: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_373: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_376: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_377: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_380: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_381: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_384: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_385: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_388: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_389: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_392: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_393: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_394: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_395: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_396: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_397: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_404: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_405: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_408: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_409: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_412: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_413: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_416: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_417: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_420: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_421: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_424: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_425: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_428: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_429: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_432: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_433: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_434: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_435: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_436: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_437: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_444: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_445: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_448: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_449: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_452: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_453: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_456: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_457: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_460: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_461: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_464: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_465: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_468: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_469: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_472: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_473: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_474: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_475: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_476: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_477: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_484: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_485: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_488: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_489: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_490: shape = [1000, 768], stride = [768, 1], offset = 0, size = 768000, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_491: shape = [1000], stride = [1], offset = 0, size = 1000, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_492: shape = [128, 3, 224, 224], stride = [150528, 50176, 224, 1], offset = 0, size = 19267584, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_493: shape = [128], stride = [1], offset = 0, size = 128, datatype = int64, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_494: operator = FlipForward, inputs = [Tensor_492], outputs = [Tensor_495], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dims: 0]
Tensor_495: shape = [128, 3, 224, 224], stride = [150528, 50176, 224, 1], offset = 0, size = 19267584, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_496: operator = ElemwiseUnaryForward, inputs = [Tensor_495], outputs = [Tensor_497], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 0.49995169]
Tensor_497: shape = [128, 3, 224, 224], stride = [150528, 50176, 224, 1], offset = 0, size = 19267584, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_498: operator = ElemwiseUnaryForward, inputs = [Tensor_492], outputs = [Tensor_499], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 0.500048339]
Tensor_499: shape = [128, 3, 224, 224], stride = [150528, 50176, 224, 1], offset = 0, size = 19267584, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_500: operator = ElemwiseBinaryForward, inputs = [Tensor_499, Tensor_497], outputs = [Tensor_501], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_501: shape = [128, 3, 224, 224], stride = [150528, 50176, 224, 1], offset = 0, size = 19267584, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_502: operator = ReshapeForward, inputs = [Tensor_493], outputs = [Tensor_503], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 1 stride: 1 stride: 1]
Tensor_503: shape = [128, 1], stride = [1, 1], offset = 0, size = 128, datatype = int64, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_504: shape = [128, 1000], stride = [1000, 1], offset = 0, size = 128000, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_505: operator = ElemwiseUnaryForward, inputs = [Tensor_504], outputs = [Tensor_506], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_FILL scal1_float: 0.0001]
Tensor_506: shape = [128, 1000], stride = [1000, 1], offset = 0, size = 128000, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_507: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_508: operator = ReshapeForward, inputs = [Tensor_507], outputs = [Tensor_509], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 1 stride: 0 stride: 0]
Tensor_509: shape = [128, 1], stride = [0, 0], offset = 0, size = 1, datatype = float32, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_510: operator = ScatterForward, inputs = [Tensor_506, Tensor_503, Tensor_509], outputs = [Tensor_511], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dim: 1]
Tensor_511: shape = [128, 1000], stride = [1000, 1], offset = 0, size = 128000, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_512: operator = FlipForward, inputs = [Tensor_493], outputs = [Tensor_513], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dims: 0]
Tensor_513: shape = [128], stride = [1], offset = 0, size = 128, datatype = int64, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_514: operator = ReshapeForward, inputs = [Tensor_513], outputs = [Tensor_515], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 1 stride: 1 stride: 1]
Tensor_515: shape = [128, 1], stride = [1, 1], offset = 0, size = 128, datatype = int64, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_516: shape = [128, 1000], stride = [1000, 1], offset = 0, size = 128000, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_517: operator = ElemwiseUnaryForward, inputs = [Tensor_516], outputs = [Tensor_518], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_FILL scal1_float: 0.0001]
Tensor_518: shape = [128, 1000], stride = [1000, 1], offset = 0, size = 128000, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_519: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_520: operator = ReshapeForward, inputs = [Tensor_519], outputs = [Tensor_521], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 1 stride: 0 stride: 0]
Tensor_521: shape = [128, 1], stride = [0, 0], offset = 0, size = 1, datatype = float32, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_522: operator = ScatterForward, inputs = [Tensor_518, Tensor_515, Tensor_521], outputs = [Tensor_523], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dim: 1]
Tensor_523: shape = [128, 1000], stride = [1000, 1], offset = 0, size = 128000, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_524: operator = ElemwiseUnaryForward, inputs = [Tensor_511], outputs = [Tensor_525], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 0.500048339]
Tensor_525: shape = [128, 1000], stride = [1000, 1], offset = 0, size = 128000, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_526: operator = ElemwiseUnaryForward, inputs = [Tensor_523], outputs = [Tensor_527], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 0.49995169]
Tensor_527: shape = [128, 1000], stride = [1000, 1], offset = 0, size = 128000, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_528: operator = ElemwiseBinaryForward, inputs = [Tensor_525, Tensor_527], outputs = [Tensor_529], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_529: shape = [128, 1000], stride = [1000, 1], offset = 0, size = 128000, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_530: operator = TypecastForward, inputs = [Tensor_501], outputs = [Tensor_531], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_531: shape = [128, 3, 224, 224], stride = [150528, 50176, 224, 1], offset = 0, size = 19267584, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_532: operator = TypecastForward, inputs = [Tensor_2], outputs = [Tensor_533], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_533: shape = [768, 3, 16, 16], stride = [768, 256, 16, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_534: operator = TypecastForward, inputs = [Tensor_3], outputs = [Tensor_535], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_535: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_536: operator = Convolution2dForward, inputs = [Tensor_531, Tensor_533, Tensor_535], outputs = [Tensor_537], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: [stride_h: 16 stride_w: 16 dilation_h: 1 dilation_w: 1 groups: 1]
Tensor_537: shape = [128, 768, 14, 14], stride = [150528, 196, 14, 1], offset = 0, size = 19267584, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_538: operator = ReshapeForward, inputs = [Tensor_537], outputs = [Tensor_539], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 768 shape: 196 stride: 150528 stride: 196 stride: 1]
Tensor_539: shape = [128, 768, 196], stride = [150528, 196, 1], offset = 0, size = 19267584, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_540: operator = ReshapeForward, inputs = [Tensor_539], outputs = [Tensor_541], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 196 shape: 768 stride: 150528 stride: 1 stride: 196]
Tensor_541: shape = [128, 196, 768], stride = [150528, 1, 196], offset = 0, size = 19267584, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_542: operator = ReshapeForward, inputs = [Tensor_5], outputs = [Tensor_543], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 1 shape: 768 stride: 0 stride: 768 stride: 1]
Tensor_543: shape = [128, 1, 768], stride = [0, 768, 1], offset = 0, size = 768, datatype = float32, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_544: operator = TypecastForward, inputs = [Tensor_541], outputs = [Tensor_545], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_545: shape = [128, 196, 768], stride = [150528, 768, 1], offset = 0, size = 19267584, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_546: operator = CatForward, inputs = [Tensor_543, Tensor_545], outputs = [Tensor_547], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [dim: 1]
Tensor_547: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_548: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_549: operator = ElemwiseUnaryForward, inputs = [Tensor_548], outputs = [Tensor_550], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_FILL scal1_float: 0]
Tensor_550: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_551: operator = TypecastForward, inputs = [Tensor_550], outputs = [Tensor_552], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_BOOLEAN]
Tensor_552: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = boolean, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_553: operator = ElemwiseUnaryForward, inputs = [Tensor_547], outputs = [Tensor_554], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 1]
Tensor_554: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_555: operator = ArangeForward, inputs = [], outputs = [Tensor_556], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [start: 2 end: 199 step: 1 output_dtype: INT64]
Tensor_556: shape = [197], stride = [1], offset = 0, size = 197, datatype = int64, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_557: operator = ReshapeForward, inputs = [Tensor_556], outputs = [Tensor_558], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1 shape: 197 stride: 197 stride: 1]
Tensor_558: shape = [1, 197], stride = [197, 1], offset = 0, size = 197, datatype = int64, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_559: operator = EmbeddingForward, inputs = [Tensor_6, Tensor_558], outputs = [Tensor_560, nullptr], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [norm_type: 2 padding_idx: -1]
Tensor_560: shape = [1, 197, 768], stride = [151296, 768, 1], offset = 0, size = 151296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_561: operator = ReshapeForward, inputs = [Tensor_560], outputs = [Tensor_562], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 0 stride: 768 stride: 1]
Tensor_562: shape = [128, 197, 768], stride = [0, 768, 1], offset = 0, size = 151296, datatype = float32, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_563: operator = ElemwiseBinaryForward, inputs = [Tensor_554, Tensor_562], outputs = [Tensor_564], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_564: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_565: operator = ReshapeForward, inputs = [Tensor_552], outputs = [Tensor_566], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 1 stride: 197 stride: 1 stride: 1]
Tensor_566: shape = [128, 197, 1], stride = [197, 1, 1], offset = 0, size = 25216, datatype = boolean, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_567: operator = TypecastForward, inputs = [Tensor_566], outputs = [Tensor_568], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_568: shape = [128, 197, 1], stride = [197, 1, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_569: operator = ElemwiseUnaryForward, inputs = [Tensor_568], outputs = [Tensor_570], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_RSUB scal1_float: 1]
Tensor_570: shape = [128, 197, 1], stride = [197, 1, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_571: operator = ReshapeForward, inputs = [Tensor_570], outputs = [Tensor_572], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 197 stride: 1 stride: 0]
Tensor_572: shape = [128, 197, 768], stride = [197, 1, 0], offset = 0, size = 25216, datatype = float32, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_573: operator = ElemwiseBinaryForward, inputs = [Tensor_564, Tensor_572], outputs = [Tensor_574], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_MUL]
Tensor_574: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_575: operator = LayernormForward, inputs = [Tensor_574, Tensor_28, Tensor_29], outputs = [Tensor_576, Tensor_577, Tensor_578], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_576: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_577: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_578: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_579: operator = TypecastForward, inputs = [Tensor_576], outputs = [Tensor_580], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_580: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_581: operator = TypecastForward, inputs = [Tensor_16], outputs = [Tensor_582], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_582: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_583: operator = TypecastForward, inputs = [Tensor_17], outputs = [Tensor_584], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_584: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_585: operator = LinearForward, inputs = [Tensor_580, Tensor_582, Tensor_584], outputs = [Tensor_586], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_586: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_587: operator = TypecastForward, inputs = [Tensor_576], outputs = [Tensor_588], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_588: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_589: operator = TypecastForward, inputs = [Tensor_8], outputs = [Tensor_590], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_590: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_591: operator = TypecastForward, inputs = [Tensor_9], outputs = [Tensor_592], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_592: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_593: operator = LinearForward, inputs = [Tensor_588, Tensor_590, Tensor_592], outputs = [Tensor_594], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_594: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_595: operator = TypecastForward, inputs = [Tensor_576], outputs = [Tensor_596], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_596: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_597: operator = TypecastForward, inputs = [Tensor_12], outputs = [Tensor_598], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_598: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_599: operator = TypecastForward, inputs = [Tensor_13], outputs = [Tensor_600], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_600: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_601: operator = LinearForward, inputs = [Tensor_596, Tensor_598, Tensor_600], outputs = [Tensor_602], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_602: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_603: operator = ElemwiseUnaryForward, inputs = [Tensor_586], outputs = [Tensor_604], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 0.125]
Tensor_604: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_605: operator = ReshapeForward, inputs = [Tensor_604], outputs = [Tensor_606], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 12 shape: 64 stride: 151296 stride: 768 stride: 64 stride: 1]
Tensor_606: shape = [128, 197, 12, 64], stride = [151296, 768, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_607: operator = ReshapeForward, inputs = [Tensor_606], outputs = [Tensor_608], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 64 stride: 151296 stride: 64 stride: 768 stride: 1]
Tensor_608: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_609: operator = ReshapeForward, inputs = [Tensor_594], outputs = [Tensor_610], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 12 shape: 64 stride: 151296 stride: 768 stride: 64 stride: 1]
Tensor_610: shape = [128, 197, 12, 64], stride = [151296, 768, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_611: operator = ReshapeForward, inputs = [Tensor_610], outputs = [Tensor_612], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 64 stride: 151296 stride: 64 stride: 768 stride: 1]
Tensor_612: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_613: operator = ReshapeForward, inputs = [Tensor_602], outputs = [Tensor_614], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 12 shape: 64 stride: 151296 stride: 768 stride: 64 stride: 1]
Tensor_614: shape = [128, 197, 12, 64], stride = [151296, 768, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_615: operator = ReshapeForward, inputs = [Tensor_614], outputs = [Tensor_616], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 64 stride: 151296 stride: 64 stride: 768 stride: 1]
Tensor_616: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_617: operator = ContiguousForward, inputs = [Tensor_608], outputs = [Tensor_618], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_618: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_619: operator = ReshapeForward, inputs = [Tensor_618], outputs = [Tensor_620], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_620: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_621: operator = ContiguousForward, inputs = [Tensor_612], outputs = [Tensor_622], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_622: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_623: operator = ReshapeForward, inputs = [Tensor_622], outputs = [Tensor_624], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_624: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_625: operator = ContiguousForward, inputs = [Tensor_616], outputs = [Tensor_626], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_626: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_627: operator = ReshapeForward, inputs = [Tensor_626], outputs = [Tensor_628], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_628: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_629: operator = ReshapeForward, inputs = [Tensor_624], outputs = [Tensor_630], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 64 shape: 197 stride: 12608 stride: 1 stride: 64]
Tensor_630: shape = [1536, 64, 197], stride = [12608, 1, 64], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_631: operator = BatchedMatmulForward, inputs = [Tensor_620, Tensor_630], outputs = [Tensor_632], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_632: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_633: operator = ReshapeForward, inputs = [Tensor_632], outputs = [Tensor_634], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 197 stride: 465708 stride: 38809 stride: 197 stride: 1]
Tensor_634: shape = [128, 12, 197, 197], stride = [465708, 38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_635: operator = ReshapeForward, inputs = [Tensor_552], outputs = [Tensor_636], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 1 shape: 197 stride: 197 stride: 197 stride: 1]
Tensor_636: shape = [128, 1, 197], stride = [197, 197, 1], offset = 0, size = 25216, datatype = boolean, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_637: operator = ReshapeForward, inputs = [Tensor_636], outputs = [Tensor_638], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 1 shape: 1 shape: 197 stride: 197 stride: 197 stride: 197 stride: 1]
Tensor_638: shape = [128, 1, 1, 197], stride = [197, 197, 197, 1], offset = 0, size = 25216, datatype = boolean, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_639: operator = ReshapeForward, inputs = [Tensor_638], outputs = [Tensor_640], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 197 stride: 197 stride: 0 stride: 0 stride: 1]
Tensor_640: shape = [128, 12, 197, 197], stride = [197, 0, 0, 1], offset = 0, size = 25216, datatype = boolean, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_641: operator = MaskedFillForward, inputs = [Tensor_634, Tensor_640], outputs = [Tensor_642], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [value_float: -inf]
Tensor_642: shape = [128, 12, 197, 197], stride = [465708, 38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_643: operator = ReshapeForward, inputs = [Tensor_642], outputs = [Tensor_644], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 197 stride: 38809 stride: 197 stride: 1]
Tensor_644: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_645: operator = TypecastForward, inputs = [Tensor_644], outputs = [Tensor_646], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_646: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_647: operator = SoftmaxForward, inputs = [Tensor_646], outputs = [Tensor_648], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dim: 2]
Tensor_648: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_649: operator = TypecastForward, inputs = [Tensor_648], outputs = [Tensor_650], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_650: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_651: operator = BatchedMatmulForward, inputs = [Tensor_650, Tensor_628], outputs = [Tensor_652], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_652: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_653: operator = ReshapeForward, inputs = [Tensor_652], outputs = [Tensor_654], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 197 shape: 1536 shape: 64 stride: 64 stride: 12608 stride: 1]
Tensor_654: shape = [197, 1536, 64], stride = [64, 12608, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_655: operator = ContiguousForward, inputs = [Tensor_654], outputs = [Tensor_656], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_656: shape = [197, 1536, 64], stride = [98304, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_657: operator = ReshapeForward, inputs = [Tensor_656], outputs = [Tensor_658], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 197 shape: 128 shape: 768 stride: 98304 stride: 768 stride: 1]
Tensor_658: shape = [197, 128, 768], stride = [98304, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_659: operator = ReshapeForward, inputs = [Tensor_658], outputs = [Tensor_660], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 768 stride: 98304 stride: 1]
Tensor_660: shape = [128, 197, 768], stride = [768, 98304, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_661: operator = TypecastForward, inputs = [Tensor_660], outputs = [Tensor_662], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_662: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_663: operator = LayernormForward, inputs = [Tensor_662, Tensor_24, Tensor_25], outputs = [Tensor_664, Tensor_665, Tensor_666], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_664: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_665: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_666: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_667: operator = TypecastForward, inputs = [Tensor_664], outputs = [Tensor_668], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_668: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_669: operator = TypecastForward, inputs = [Tensor_20], outputs = [Tensor_670], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_670: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_671: operator = TypecastForward, inputs = [Tensor_21], outputs = [Tensor_672], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_672: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_673: operator = LinearForward, inputs = [Tensor_668, Tensor_670, Tensor_672], outputs = [Tensor_674], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_674: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_679: operator = ElemwiseUnaryForward, inputs = [Tensor_574], outputs = [Tensor_680], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 1]
Tensor_680: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_681: operator = TypecastForward, inputs = [Tensor_674], outputs = [Tensor_682], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_682: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_683: operator = ElemwiseBinaryForward, inputs = [Tensor_680, Tensor_682], outputs = [Tensor_684], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_684: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_685: operator = LayernormForward, inputs = [Tensor_684, Tensor_44, Tensor_45], outputs = [Tensor_686, Tensor_687, Tensor_688], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_686: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_687: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_688: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_689: operator = ReshapeForward, inputs = [Tensor_686], outputs = [Tensor_690], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 25216 shape: 768 stride: 768 stride: 1]
Tensor_690: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_691: operator = TypecastForward, inputs = [Tensor_690], outputs = [Tensor_692], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_692: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_693: operator = TypecastForward, inputs = [Tensor_32], outputs = [Tensor_694], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_694: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_695: operator = TypecastForward, inputs = [Tensor_33], outputs = [Tensor_696], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_696: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_697: operator = LinearForward, inputs = [Tensor_692, Tensor_694, Tensor_696], outputs = [Tensor_698], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_698: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_699: operator = TypecastForward, inputs = [Tensor_698], outputs = [Tensor_700], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_700: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_701: operator = ElemwiseUnaryForward, inputs = [Tensor_700], outputs = [Tensor_702], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_GELU]
Tensor_702: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_703: operator = TypecastForward, inputs = [Tensor_702], outputs = [Tensor_704], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_704: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_705: operator = TypecastForward, inputs = [Tensor_704], outputs = [Tensor_706], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_706: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_707: operator = LayernormForward, inputs = [Tensor_706, Tensor_36, Tensor_37], outputs = [Tensor_708, Tensor_709, Tensor_710], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_708: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_709: shape = [25216], stride = [1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_710: shape = [25216], stride = [1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_711: operator = TypecastForward, inputs = [Tensor_708], outputs = [Tensor_712], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_712: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_713: operator = TypecastForward, inputs = [Tensor_34], outputs = [Tensor_714], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_714: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_715: operator = TypecastForward, inputs = [Tensor_35], outputs = [Tensor_716], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_716: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_717: operator = LinearForward, inputs = [Tensor_712, Tensor_714, Tensor_716], outputs = [Tensor_718], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_718: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_719: operator = ReshapeForward, inputs = [Tensor_718], outputs = [Tensor_720], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 151296 stride: 768 stride: 1]
Tensor_720: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_721: operator = ElemwiseUnaryForward, inputs = [Tensor_684], outputs = [Tensor_722], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 1]
Tensor_722: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_723: operator = TypecastForward, inputs = [Tensor_720], outputs = [Tensor_724], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_724: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_725: operator = ElemwiseBinaryForward, inputs = [Tensor_722, Tensor_724], outputs = [Tensor_726], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_726: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_727: operator = LayernormForward, inputs = [Tensor_726, Tensor_68, Tensor_69], outputs = [Tensor_728, Tensor_729, Tensor_730], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_728: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_729: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_730: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_731: operator = TypecastForward, inputs = [Tensor_728], outputs = [Tensor_732], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_732: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_733: operator = TypecastForward, inputs = [Tensor_56], outputs = [Tensor_734], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_734: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_735: operator = TypecastForward, inputs = [Tensor_57], outputs = [Tensor_736], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_736: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_737: operator = LinearForward, inputs = [Tensor_732, Tensor_734, Tensor_736], outputs = [Tensor_738], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_738: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_739: operator = TypecastForward, inputs = [Tensor_728], outputs = [Tensor_740], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_740: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_741: operator = TypecastForward, inputs = [Tensor_48], outputs = [Tensor_742], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_742: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_743: operator = TypecastForward, inputs = [Tensor_49], outputs = [Tensor_744], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_744: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_745: operator = LinearForward, inputs = [Tensor_740, Tensor_742, Tensor_744], outputs = [Tensor_746], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_746: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_747: operator = TypecastForward, inputs = [Tensor_728], outputs = [Tensor_748], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_748: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_749: operator = TypecastForward, inputs = [Tensor_52], outputs = [Tensor_750], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_750: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_751: operator = TypecastForward, inputs = [Tensor_53], outputs = [Tensor_752], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_752: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_753: operator = LinearForward, inputs = [Tensor_748, Tensor_750, Tensor_752], outputs = [Tensor_754], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_754: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_755: operator = ElemwiseUnaryForward, inputs = [Tensor_738], outputs = [Tensor_756], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 0.125]
Tensor_756: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_757: operator = ReshapeForward, inputs = [Tensor_756], outputs = [Tensor_758], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 12 shape: 64 stride: 151296 stride: 768 stride: 64 stride: 1]
Tensor_758: shape = [128, 197, 12, 64], stride = [151296, 768, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_759: operator = ReshapeForward, inputs = [Tensor_758], outputs = [Tensor_760], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 64 stride: 151296 stride: 64 stride: 768 stride: 1]
Tensor_760: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_761: operator = ReshapeForward, inputs = [Tensor_746], outputs = [Tensor_762], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 12 shape: 64 stride: 151296 stride: 768 stride: 64 stride: 1]
Tensor_762: shape = [128, 197, 12, 64], stride = [151296, 768, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_763: operator = ReshapeForward, inputs = [Tensor_762], outputs = [Tensor_764], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 64 stride: 151296 stride: 64 stride: 768 stride: 1]
Tensor_764: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_765: operator = ReshapeForward, inputs = [Tensor_754], outputs = [Tensor_766], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 12 shape: 64 stride: 151296 stride: 768 stride: 64 stride: 1]
Tensor_766: shape = [128, 197, 12, 64], stride = [151296, 768, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_767: operator = ReshapeForward, inputs = [Tensor_766], outputs = [Tensor_768], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 64 stride: 151296 stride: 64 stride: 768 stride: 1]
Tensor_768: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_769: operator = ContiguousForward, inputs = [Tensor_760], outputs = [Tensor_770], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_770: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_771: operator = ReshapeForward, inputs = [Tensor_770], outputs = [Tensor_772], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_772: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_773: operator = ContiguousForward, inputs = [Tensor_764], outputs = [Tensor_774], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_774: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_775: operator = ReshapeForward, inputs = [Tensor_774], outputs = [Tensor_776], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_776: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_777: operator = ContiguousForward, inputs = [Tensor_768], outputs = [Tensor_778], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_778: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_779: operator = ReshapeForward, inputs = [Tensor_778], outputs = [Tensor_780], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_780: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_781: operator = ReshapeForward, inputs = [Tensor_776], outputs = [Tensor_782], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 64 shape: 197 stride: 12608 stride: 1 stride: 64]
Tensor_782: shape = [1536, 64, 197], stride = [12608, 1, 64], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_783: operator = BatchedMatmulForward, inputs = [Tensor_772, Tensor_782], outputs = [Tensor_784], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_784: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_785: operator = ReshapeForward, inputs = [Tensor_784], outputs = [Tensor_786], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 197 stride: 465708 stride: 38809 stride: 197 stride: 1]
Tensor_786: shape = [128, 12, 197, 197], stride = [465708, 38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_787: operator = ReshapeForward, inputs = [Tensor_552], outputs = [Tensor_788], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 1 shape: 197 stride: 197 stride: 197 stride: 1]
Tensor_788: shape = [128, 1, 197], stride = [197, 197, 1], offset = 0, size = 25216, datatype = boolean, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_789: operator = ReshapeForward, inputs = [Tensor_788], outputs = [Tensor_790], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 1 shape: 1 shape: 197 stride: 197 stride: 197 stride: 197 stride: 1]
Tensor_790: shape = [128, 1, 1, 197], stride = [197, 197, 197, 1], offset = 0, size = 25216, datatype = boolean, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_791: operator = ReshapeForward, inputs = [Tensor_790], outputs = [Tensor_792], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 197 stride: 197 stride: 0 stride: 0 stride: 1]
Tensor_792: shape = [128, 12, 197, 197], stride = [197, 0, 0, 1], offset = 0, size = 25216, datatype = boolean, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_793: operator = MaskedFillForward, inputs = [Tensor_786, Tensor_792], outputs = [Tensor_794], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [value_float: -inf]
Tensor_794: shape = [128, 12, 197, 197], stride = [465708, 38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_795: operator = ReshapeForward, inputs = [Tensor_794], outputs = [Tensor_796], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 197 stride: 38809 stride: 197 stride: 1]
Tensor_796: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_797: operator = TypecastForward, inputs = [Tensor_796], outputs = [Tensor_798], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_798: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_799: operator = SoftmaxForward, inputs = [Tensor_798], outputs = [Tensor_800], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dim: 2]
Tensor_800: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_801: operator = TypecastForward, inputs = [Tensor_800], outputs = [Tensor_802], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_802: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_803: operator = BatchedMatmulForward, inputs = [Tensor_802, Tensor_780], outputs = [Tensor_804], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_804: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_805: operator = ReshapeForward, inputs = [Tensor_804], outputs = [Tensor_806], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 197 shape: 1536 shape: 64 stride: 64 stride: 12608 stride: 1]
Tensor_806: shape = [197, 1536, 64], stride = [64, 12608, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_807: operator = ContiguousForward, inputs = [Tensor_806], outputs = [Tensor_808], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_808: shape = [197, 1536, 64], stride = [98304, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_809: operator = ReshapeForward, inputs = [Tensor_808], outputs = [Tensor_810], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 197 shape: 128 shape: 768 stride: 98304 stride: 768 stride: 1]
Tensor_810: shape = [197, 128, 768], stride = [98304, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_811: operator = ReshapeForward, inputs = [Tensor_810], outputs = [Tensor_812], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 768 stride: 98304 stride: 1]
Tensor_812: shape = [128, 197, 768], stride = [768, 98304, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_813: operator = TypecastForward, inputs = [Tensor_812], outputs = [Tensor_814], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_814: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_815: operator = LayernormForward, inputs = [Tensor_814, Tensor_64, Tensor_65], outputs = [Tensor_816, Tensor_817, Tensor_818], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_816: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_817: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_818: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_819: operator = TypecastForward, inputs = [Tensor_816], outputs = [Tensor_820], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_820: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_821: operator = TypecastForward, inputs = [Tensor_60], outputs = [Tensor_822], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_822: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_823: operator = TypecastForward, inputs = [Tensor_61], outputs = [Tensor_824], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_824: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_825: operator = LinearForward, inputs = [Tensor_820, Tensor_822, Tensor_824], outputs = [Tensor_826], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_826: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_831: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_832: operator = ElemwiseUnaryForward, inputs = [Tensor_831], outputs = [Tensor_833], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_ADD scal1_float: 0.986363649]
Tensor_833: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_834: operator = ElemwiseUnaryForward, inputs = [Tensor_833], outputs = [Tensor_835], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_FLOOR]
Tensor_835: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_836: operator = ElemwiseUnaryForward, inputs = [Tensor_826], outputs = [Tensor_837], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_DIV scal1_float: 0.986363649]
Tensor_837: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_838: operator = ReshapeForward, inputs = [Tensor_835], outputs = [Tensor_839], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 1 stride: 0 stride: 0]
Tensor_839: shape = [128, 197, 768], stride = [1, 0, 0], offset = 0, size = 128, datatype = float16, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_840: operator = ElemwiseBinaryForward, inputs = [Tensor_837, Tensor_839], outputs = [Tensor_841], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_MUL]
Tensor_841: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_842: operator = ElemwiseUnaryForward, inputs = [Tensor_726], outputs = [Tensor_843], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 1]
Tensor_843: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_844: operator = TypecastForward, inputs = [Tensor_841], outputs = [Tensor_845], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_845: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_846: operator = ElemwiseBinaryForward, inputs = [Tensor_843, Tensor_845], outputs = [Tensor_847], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_847: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_848: operator = LayernormForward, inputs = [Tensor_847, Tensor_84, Tensor_85], outputs = [Tensor_849, Tensor_850, Tensor_851], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_849: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_850: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_851: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_852: operator = ReshapeForward, inputs = [Tensor_849], outputs = [Tensor_853], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 25216 shape: 768 stride: 768 stride: 1]
Tensor_853: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_854: operator = TypecastForward, inputs = [Tensor_853], outputs = [Tensor_855], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_855: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_856: operator = TypecastForward, inputs = [Tensor_72], outputs = [Tensor_857], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_857: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_858: operator = TypecastForward, inputs = [Tensor_73], outputs = [Tensor_859], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_859: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_860: operator = LinearForward, inputs = [Tensor_855, Tensor_857, Tensor_859], outputs = [Tensor_861], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_861: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_862: operator = TypecastForward, inputs = [Tensor_861], outputs = [Tensor_863], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_863: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_864: operator = ElemwiseUnaryForward, inputs = [Tensor_863], outputs = [Tensor_865], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_GELU]
Tensor_865: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_866: operator = TypecastForward, inputs = [Tensor_865], outputs = [Tensor_867], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_867: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_868: operator = TypecastForward, inputs = [Tensor_867], outputs = [Tensor_869], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_869: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_870: operator = LayernormForward, inputs = [Tensor_869, Tensor_76, Tensor_77], outputs = [Tensor_871, Tensor_872, Tensor_873], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_871: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_872: shape = [25216], stride = [1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_873: shape = [25216], stride = [1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_874: operator = TypecastForward, inputs = [Tensor_871], outputs = [Tensor_875], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_875: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_876: operator = TypecastForward, inputs = [Tensor_74], outputs = [Tensor_877], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_877: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_878: operator = TypecastForward, inputs = [Tensor_75], outputs = [Tensor_879], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_879: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_880: operator = LinearForward, inputs = [Tensor_875, Tensor_877, Tensor_879], outputs = [Tensor_881], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_881: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_882: operator = ReshapeForward, inputs = [Tensor_881], outputs = [Tensor_883], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 151296 stride: 768 stride: 1]
Tensor_883: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_884: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_885: operator = ElemwiseUnaryForward, inputs = [Tensor_884], outputs = [Tensor_886], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_ADD scal1_float: 0.986363649]
Tensor_886: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_887: operator = ElemwiseUnaryForward, inputs = [Tensor_886], outputs = [Tensor_888], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_FLOOR]
Tensor_888: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_889: operator = ElemwiseUnaryForward, inputs = [Tensor_883], outputs = [Tensor_890], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_DIV scal1_float: 0.986363649]
Tensor_890: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_891: operator = ReshapeForward, inputs = [Tensor_888], outputs = [Tensor_892], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 1 stride: 0 stride: 0]
Tensor_892: shape = [128, 197, 768], stride = [1, 0, 0], offset = 0, size = 128, datatype = float16, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_893: operator = ElemwiseBinaryForward, inputs = [Tensor_890, Tensor_892], outputs = [Tensor_894], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_MUL]
Tensor_894: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_895: operator = ElemwiseUnaryForward, inputs = [Tensor_847], outputs = [Tensor_896], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 1]
Tensor_896: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_897: operator = TypecastForward, inputs = [Tensor_894], outputs = [Tensor_898], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_898: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_899: operator = ElemwiseBinaryForward, inputs = [Tensor_896, Tensor_898], outputs = [Tensor_900], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_900: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_901: operator = LayernormForward, inputs = [Tensor_900, Tensor_108, Tensor_109], outputs = [Tensor_902, Tensor_903, Tensor_904], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_902: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_903: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_904: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_905: operator = TypecastForward, inputs = [Tensor_902], outputs = [Tensor_906], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_906: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_907: operator = TypecastForward, inputs = [Tensor_96], outputs = [Tensor_908], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_908: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_909: operator = TypecastForward, inputs = [Tensor_97], outputs = [Tensor_910], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_910: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_911: operator = LinearForward, inputs = [Tensor_906, Tensor_908, Tensor_910], outputs = [Tensor_912], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_912: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_913: operator = TypecastForward, inputs = [Tensor_902], outputs = [Tensor_914], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_914: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_915: operator = TypecastForward, inputs = [Tensor_88], outputs = [Tensor_916], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_916: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_917: operator = TypecastForward, inputs = [Tensor_89], outputs = [Tensor_918], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_918: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_919: operator = LinearForward, inputs = [Tensor_914, Tensor_916, Tensor_918], outputs = [Tensor_920], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_920: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_921: operator = TypecastForward, inputs = [Tensor_902], outputs = [Tensor_922], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_922: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_923: operator = TypecastForward, inputs = [Tensor_92], outputs = [Tensor_924], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_924: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_925: operator = TypecastForward, inputs = [Tensor_93], outputs = [Tensor_926], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_926: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_927: operator = LinearForward, inputs = [Tensor_922, Tensor_924, Tensor_926], outputs = [Tensor_928], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_928: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_929: operator = ElemwiseUnaryForward, inputs = [Tensor_912], outputs = [Tensor_930], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 0.125]
Tensor_930: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_931: operator = ReshapeForward, inputs = [Tensor_930], outputs = [Tensor_932], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 12 shape: 64 stride: 151296 stride: 768 stride: 64 stride: 1]
Tensor_932: shape = [128, 197, 12, 64], stride = [151296, 768, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_933: operator = ReshapeForward, inputs = [Tensor_932], outputs = [Tensor_934], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 64 stride: 151296 stride: 64 stride: 768 stride: 1]
Tensor_934: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_935: operator = ReshapeForward, inputs = [Tensor_920], outputs = [Tensor_936], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 12 shape: 64 stride: 151296 stride: 768 stride: 64 stride: 1]
Tensor_936: shape = [128, 197, 12, 64], stride = [151296, 768, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_937: operator = ReshapeForward, inputs = [Tensor_936], outputs = [Tensor_938], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 64 stride: 151296 stride: 64 stride: 768 stride: 1]
Tensor_938: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_939: operator = ReshapeForward, inputs = [Tensor_928], outputs = [Tensor_940], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 12 shape: 64 stride: 151296 stride: 768 stride: 64 stride: 1]
Tensor_940: shape = [128, 197, 12, 64], stride = [151296, 768, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_941: operator = ReshapeForward, inputs = [Tensor_940], outputs = [Tensor_942], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 64 stride: 151296 stride: 64 stride: 768 stride: 1]
Tensor_942: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_943: operator = ContiguousForward, inputs = [Tensor_934], outputs = [Tensor_944], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_944: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_945: operator = ReshapeForward, inputs = [Tensor_944], outputs = [Tensor_946], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_946: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_947: operator = ContiguousForward, inputs = [Tensor_938], outputs = [Tensor_948], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_948: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_949: operator = ReshapeForward, inputs = [Tensor_948], outputs = [Tensor_950], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_950: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_951: operator = ContiguousForward, inputs = [Tensor_942], outputs = [Tensor_952], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_952: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_953: operator = ReshapeForward, inputs = [Tensor_952], outputs = [Tensor_954], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_954: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_955: operator = ReshapeForward, inputs = [Tensor_950], outputs = [Tensor_956], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 64 shape: 197 stride: 12608 stride: 1 stride: 64]
Tensor_956: shape = [1536, 64, 197], stride = [12608, 1, 64], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_957: operator = BatchedMatmulForward, inputs = [Tensor_946, Tensor_956], outputs = [Tensor_958], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_958: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_959: operator = ReshapeForward, inputs = [Tensor_958], outputs = [Tensor_960], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 197 stride: 465708 stride: 38809 stride: 197 stride: 1]
Tensor_960: shape = [128, 12, 197, 197], stride = [465708, 38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_961: operator = ReshapeForward, inputs = [Tensor_552], outputs = [Tensor_962], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 1 shape: 197 stride: 197 stride: 197 stride: 1]
Tensor_962: shape = [128, 1, 197], stride = [197, 197, 1], offset = 0, size = 25216, datatype = boolean, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_963: operator = ReshapeForward, inputs = [Tensor_962], outputs = [Tensor_964], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 1 shape: 1 shape: 197 stride: 197 stride: 197 stride: 197 stride: 1]
Tensor_964: shape = [128, 1, 1, 197], stride = [197, 197, 197, 1], offset = 0, size = 25216, datatype = boolean, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_965: operator = ReshapeForward, inputs = [Tensor_964], outputs = [Tensor_966], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 197 stride: 197 stride: 0 stride: 0 stride: 1]
Tensor_966: shape = [128, 12, 197, 197], stride = [197, 0, 0, 1], offset = 0, size = 25216, datatype = boolean, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_967: operator = MaskedFillForward, inputs = [Tensor_960, Tensor_966], outputs = [Tensor_968], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [value_float: -inf]
Tensor_968: shape = [128, 12, 197, 197], stride = [465708, 38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_969: operator = ReshapeForward, inputs = [Tensor_968], outputs = [Tensor_970], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 197 stride: 38809 stride: 197 stride: 1]
Tensor_970: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_971: operator = TypecastForward, inputs = [Tensor_970], outputs = [Tensor_972], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_972: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_973: operator = SoftmaxForward, inputs = [Tensor_972], outputs = [Tensor_974], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dim: 2]
Tensor_974: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_975: operator = TypecastForward, inputs = [Tensor_974], outputs = [Tensor_976], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_976: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_977: operator = BatchedMatmulForward, inputs = [Tensor_976, Tensor_954], outputs = [Tensor_978], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_978: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_979: operator = ReshapeForward, inputs = [Tensor_978], outputs = [Tensor_980], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 197 shape: 1536 shape: 64 stride: 64 stride: 12608 stride: 1]
Tensor_980: shape = [197, 1536, 64], stride = [64, 12608, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_981: operator = ContiguousForward, inputs = [Tensor_980], outputs = [Tensor_982], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_982: shape = [197, 1536, 64], stride = [98304, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_983: operator = ReshapeForward, inputs = [Tensor_982], outputs = [Tensor_984], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 197 shape: 128 shape: 768 stride: 98304 stride: 768 stride: 1]
Tensor_984: shape = [197, 128, 768], stride = [98304, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_985: operator = ReshapeForward, inputs = [Tensor_984], outputs = [Tensor_986], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 768 stride: 98304 stride: 1]
Tensor_986: shape = [128, 197, 768], stride = [768, 98304, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_987: operator = TypecastForward, inputs = [Tensor_986], outputs = [Tensor_988], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_988: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_989: operator = LayernormForward, inputs = [Tensor_988, Tensor_104, Tensor_105], outputs = [Tensor_990, Tensor_991, Tensor_992], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_990: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_991: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_992: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_993: operator = TypecastForward, inputs = [Tensor_990], outputs = [Tensor_994], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_994: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_995: operator = TypecastForward, inputs = [Tensor_100], outputs = [Tensor_996], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_996: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_997: operator = TypecastForward, inputs = [Tensor_101], outputs = [Tensor_998], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_998: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_999: operator = LinearForward, inputs = [Tensor_994, Tensor_996, Tensor_998], outputs = [Tensor_1000], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_1000: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_1005: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1006: operator = ElemwiseUnaryForward, inputs = [Tensor_1005], outputs = [Tensor_1007], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_ADD scal1_float: 0.972727299]
Tensor_1007: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1008: operator = ElemwiseUnaryForward, inputs = [Tensor_1007], outputs = [Tensor_1009], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_FLOOR]
Tensor_1009: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1010: operator = ElemwiseUnaryForward, inputs = [Tensor_1000], outputs = [Tensor_1011], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_DIV scal1_float: 0.972727299]
Tensor_1011: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1012: operator = ReshapeForward, inputs = [Tensor_1009], outputs = [Tensor_1013], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 1 stride: 0 stride: 0]
Tensor_1013: shape = [128, 197, 768], stride = [1, 0, 0], offset = 0, size = 128, datatype = float16, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1014: operator = ElemwiseBinaryForward, inputs = [Tensor_1011, Tensor_1013], outputs = [Tensor_1015], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_MUL]
Tensor_1015: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1016: operator = ElemwiseUnaryForward, inputs = [Tensor_900], outputs = [Tensor_1017], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 1]
Tensor_1017: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1018: operator = TypecastForward, inputs = [Tensor_1015], outputs = [Tensor_1019], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_1019: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1020: operator = ElemwiseBinaryForward, inputs = [Tensor_1017, Tensor_1019], outputs = [Tensor_1021], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_1021: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1022: operator = LayernormForward, inputs = [Tensor_1021, Tensor_124, Tensor_125], outputs = [Tensor_1023, Tensor_1024, Tensor_1025], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_1023: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_1024: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_1025: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1026: operator = ReshapeForward, inputs = [Tensor_1023], outputs = [Tensor_1027], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 25216 shape: 768 stride: 768 stride: 1]
Tensor_1027: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1028: operator = TypecastForward, inputs = [Tensor_1027], outputs = [Tensor_1029], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_1029: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1030: operator = TypecastForward, inputs = [Tensor_112], outputs = [Tensor_1031], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_1031: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1032: operator = TypecastForward, inputs = [Tensor_113], outputs = [Tensor_1033], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_1033: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1034: operator = LinearForward, inputs = [Tensor_1029, Tensor_1031, Tensor_1033], outputs = [Tensor_1035], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_1035: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1036: operator = TypecastForward, inputs = [Tensor_1035], outputs = [Tensor_1037], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_1037: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1038: operator = ElemwiseUnaryForward, inputs = [Tensor_1037], outputs = [Tensor_1039], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_GELU]
Tensor_1039: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1040: operator = TypecastForward, inputs = [Tensor_1039], outputs = [Tensor_1041], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_1041: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1042: operator = TypecastForward, inputs = [Tensor_1041], outputs = [Tensor_1043], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_1043: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1044: operator = LayernormForward, inputs = [Tensor_1043, Tensor_116, Tensor_117], outputs = [Tensor_1045, Tensor_1046, Tensor_1047], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_1045: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_1046: shape = [25216], stride = [1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_1047: shape = [25216], stride = [1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1048: operator = TypecastForward, inputs = [Tensor_1045], outputs = [Tensor_1049], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_1049: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1050: operator = TypecastForward, inputs = [Tensor_114], outputs = [Tensor_1051], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_1051: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1052: operator = TypecastForward, inputs = [Tensor_115], outputs = [Tensor_1053], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_1053: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1054: operator = LinearForward, inputs = [Tensor_1049, Tensor_1051, Tensor_1053], outputs = [Tensor_1055], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_1055: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1056: operator = ReshapeForward, inputs = [Tensor_1055], outputs = [Tensor_1057], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 151296 stride: 768 stride: 1]
Tensor_1057: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_1058: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1059: operator = ElemwiseUnaryForward, inputs = [Tensor_1058], outputs = [Tensor_1060], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_ADD scal1_float: 0.972727299]
Tensor_1060: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1061: operator = ElemwiseUnaryForward, inputs = [Tensor_1060], outputs = [Tensor_1062], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_FLOOR]
Tensor_1062: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1063: operator = ElemwiseUnaryForward, inputs = [Tensor_1057], outputs = [Tensor_1064], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_DIV scal1_float: 0.972727299]
Tensor_1064: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1065: operator = ReshapeForward, inputs = [Tensor_1062], outputs = [Tensor_1066], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 1 stride: 0 stride: 0]
Tensor_1066: shape = [128, 197, 768], stride = [1, 0, 0], offset = 0, size = 128, datatype = float16, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1067: operator = ElemwiseBinaryForward, inputs = [Tensor_1064, Tensor_1066], outputs = [Tensor_1068], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_MUL]
Tensor_1068: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1069: operator = ElemwiseUnaryForward, inputs = [Tensor_1021], outputs = [Tensor_1070], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 1]
Tensor_1070: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1071: operator = TypecastForward, inputs = [Tensor_1068], outputs = [Tensor_1072], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_1072: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1073: operator = ElemwiseBinaryForward, inputs = [Tensor_1070, Tensor_1072], outputs = [Tensor_1074], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_1074: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1075: operator = LayernormForward, inputs = [Tensor_1074, Tensor_148, Tensor_149], outputs = [Tensor_1076, Tensor_1077, Tensor_1078], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_1076: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_1077: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_1078: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1079: operator = TypecastForward, inputs = [Tensor_1076], outputs = [Tensor_1080], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_1080: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1081: operator = TypecastForward, inputs = [Tensor_136], outputs = [Tensor_1082], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_1082: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1083: operator = TypecastForward, inputs = [Tensor_137], outputs = [Tensor_1084], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_1084: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1085: operator = LinearForward, inputs = [Tensor_1080, Tensor_1082, Tensor_1084], outputs = [Tensor_1086], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_1086: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1087: operator = TypecastForward, inputs = [Tensor_1076], outputs = [Tensor_1088], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_1088: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1089: operator = TypecastForward, inputs = [Tensor_128], outputs = [Tensor_1090], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_1090: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1091: operator = TypecastForward, inputs = [Tensor_129], outputs = [Tensor_1092], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_1092: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1093: operator = LinearForward, inputs = [Tensor_1088, Tensor_1090, Tensor_1092], outputs = [Tensor_1094], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_1094: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1095: operator = TypecastForward, inputs = [Tensor_1076], outputs = [Tensor_1096], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_1096: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1097: operator = TypecastForward, inputs = [Tensor_132], outputs = [Tensor_1098], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_1098: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1099: operator = TypecastForward, inputs = [Tensor_133], outputs = [Tensor_1100], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_1100: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1101: operator = LinearForward, inputs = [Tensor_1096, Tensor_1098, Tensor_1100], outputs = [Tensor_1102], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_1102: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1103: operator = ElemwiseUnaryForward, inputs = [Tensor_1086], outputs = [Tensor_1104], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 0.125]
Tensor_1104: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1105: operator = ReshapeForward, inputs = [Tensor_1104], outputs = [Tensor_1106], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 12 shape: 64 stride: 151296 stride: 768 stride: 64 stride: 1]
Tensor_1106: shape = [128, 197, 12, 64], stride = [151296, 768, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1107: operator = ReshapeForward, inputs = [Tensor_1106], outputs = [Tensor_1108], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 64 stride: 151296 stride: 64 stride: 768 stride: 1]
Tensor_1108: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1109: operator = ReshapeForward, inputs = [Tensor_1094], outputs = [Tensor_1110], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 12 shape: 64 stride: 151296 stride: 768 stride: 64 stride: 1]
Tensor_1110: shape = [128, 197, 12, 64], stride = [151296, 768, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1111: operator = ReshapeForward, inputs = [Tensor_1110], outputs = [Tensor_1112], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 64 stride: 151296 stride: 64 stride: 768 stride: 1]
Tensor_1112: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1113: operator = ReshapeForward, inputs = [Tensor_1102], outputs = [Tensor_1114], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 12 shape: 64 stride: 151296 stride: 768 stride: 64 stride: 1]
Tensor_1114: shape = [128, 197, 12, 64], stride = [151296, 768, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1115: operator = ReshapeForward, inputs = [Tensor_1114], outputs = [Tensor_1116], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 64 stride: 151296 stride: 64 stride: 768 stride: 1]
Tensor_1116: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1117: operator = ContiguousForward, inputs = [Tensor_1108], outputs = [Tensor_1118], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_1118: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1119: operator = ReshapeForward, inputs = [Tensor_1118], outputs = [Tensor_1120], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_1120: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1121: operator = ContiguousForward, inputs = [Tensor_1112], outputs = [Tensor_1122], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_1122: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1123: operator = ReshapeForward, inputs = [Tensor_1122], outputs = [Tensor_1124], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_1124: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1125: operator = ContiguousForward, inputs = [Tensor_1116], outputs = [Tensor_1126], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_1126: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1127: operator = ReshapeForward, inputs = [Tensor_1126], outputs = [Tensor_1128], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_1128: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1129: operator = ReshapeForward, inputs = [Tensor_1124], outputs = [Tensor_1130], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 64 shape: 197 stride: 12608 stride: 1 stride: 64]
Tensor_1130: shape = [1536, 64, 197], stride = [12608, 1, 64], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1131: operator = BatchedMatmulForward, inputs = [Tensor_1120, Tensor_1130], outputs = [Tensor_1132], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_1132: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1133: operator = ReshapeForward, inputs = [Tensor_1132], outputs = [Tensor_1134], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 197 stride: 465708 stride: 38809 stride: 197 stride: 1]
Tensor_1134: shape = [128, 12, 197, 197], stride = [465708, 38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1135: operator = ReshapeForward, inputs = [Tensor_552], outputs = [Tensor_1136], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 1 shape: 197 stride: 197 stride: 197 stride: 1]
Tensor_1136: shape = [128, 1, 197], stride = [197, 197, 1], offset = 0, size = 25216, datatype = boolean, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1137: operator = ReshapeForward, inputs = [Tensor_1136], outputs = [Tensor_1138], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 1 shape: 1 shape: 197 stride: 197 stride: 197 stride: 197 stride: 1]
Tensor_1138: shape = [128, 1, 1, 197], stride = [197, 197, 197, 1], offset = 0, size = 25216, datatype = boolean, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1139: operator = ReshapeForward, inputs = [Tensor_1138], outputs = [Tensor_1140], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 197 stride: 197 stride: 0 stride: 0 stride: 1]
Tensor_1140: shape = [128, 12, 197, 197], stride = [197, 0, 0, 1], offset = 0, size = 25216, datatype = boolean, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1141: operator = MaskedFillForward, inputs = [Tensor_1134, Tensor_1140], outputs = [Tensor_1142], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [value_float: -inf]
Tensor_1142: shape = [128, 12, 197, 197], stride = [465708, 38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1143: operator = ReshapeForward, inputs = [Tensor_1142], outputs = [Tensor_1144], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 197 stride: 38809 stride: 197 stride: 1]
Tensor_1144: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1145: operator = TypecastForward, inputs = [Tensor_1144], outputs = [Tensor_1146], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_1146: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1147: operator = SoftmaxForward, inputs = [Tensor_1146], outputs = [Tensor_1148], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dim: 2]
Tensor_1148: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1149: operator = TypecastForward, inputs = [Tensor_1148], outputs = [Tensor_1150], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_1150: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1151: operator = BatchedMatmulForward, inputs = [Tensor_1150, Tensor_1128], outputs = [Tensor_1152], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_1152: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1153: operator = ReshapeForward, inputs = [Tensor_1152], outputs = [Tensor_1154], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 197 shape: 1536 shape: 64 stride: 64 stride: 12608 stride: 1]
Tensor_1154: shape = [197, 1536, 64], stride = [64, 12608, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1155: operator = ContiguousForward, inputs = [Tensor_1154], outputs = [Tensor_1156], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_1156: shape = [197, 1536, 64], stride = [98304, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1157: operator = ReshapeForward, inputs = [Tensor_1156], outputs = [Tensor_1158], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 197 shape: 128 shape: 768 stride: 98304 stride: 768 stride: 1]
Tensor_1158: shape = [197, 128, 768], stride = [98304, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1159: operator = ReshapeForward, inputs = [Tensor_1158], outputs = [Tensor_1160], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 768 stride: 98304 stride: 1]
Tensor_1160: shape = [128, 197, 768], stride = [768, 98304, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1161: operator = TypecastForward, inputs = [Tensor_1160], outputs = [Tensor_1162], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_1162: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1163: operator = LayernormForward, inputs = [Tensor_1162, Tensor_144, Tensor_145], outputs = [Tensor_1164, Tensor_1165, Tensor_1166], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_1164: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_1165: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_1166: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1167: operator = TypecastForward, inputs = [Tensor_1164], outputs = [Tensor_1168], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_1168: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1169: operator = TypecastForward, inputs = [Tensor_140], outputs = [Tensor_1170], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_1170: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1171: operator = TypecastForward, inputs = [Tensor_141], outputs = [Tensor_1172], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_1172: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1173: operator = LinearForward, inputs = [Tensor_1168, Tensor_1170, Tensor_1172], outputs = [Tensor_1174], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_1174: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_1179: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1180: operator = ElemwiseUnaryForward, inputs = [Tensor_1179], outputs = [Tensor_1181], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_ADD scal1_float: 0.959090889]
Tensor_1181: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1182: operator = ElemwiseUnaryForward, inputs = [Tensor_1181], outputs = [Tensor_1183], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_FLOOR]
Tensor_1183: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1184: operator = ElemwiseUnaryForward, inputs = [Tensor_1174], outputs = [Tensor_1185], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_DIV scal1_float: 0.959090889]
Tensor_1185: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1186: operator = ReshapeForward, inputs = [Tensor_1183], outputs = [Tensor_1187], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 1 stride: 0 stride: 0]
Tensor_1187: shape = [128, 197, 768], stride = [1, 0, 0], offset = 0, size = 128, datatype = float16, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1188: operator = ElemwiseBinaryForward, inputs = [Tensor_1185, Tensor_1187], outputs = [Tensor_1189], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_MUL]
Tensor_1189: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1190: operator = ElemwiseUnaryForward, inputs = [Tensor_1074], outputs = [Tensor_1191], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 1]
Tensor_1191: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1192: operator = TypecastForward, inputs = [Tensor_1189], outputs = [Tensor_1193], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_1193: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1194: operator = ElemwiseBinaryForward, inputs = [Tensor_1191, Tensor_1193], outputs = [Tensor_1195], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_1195: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1196: operator = LayernormForward, inputs = [Tensor_1195, Tensor_164, Tensor_165], outputs = [Tensor_1197, Tensor_1198, Tensor_1199], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_1197: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_1198: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_1199: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1200: operator = ReshapeForward, inputs = [Tensor_1197], outputs = [Tensor_1201], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 25216 shape: 768 stride: 768 stride: 1]
Tensor_1201: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1202: operator = TypecastForward, inputs = [Tensor_1201], outputs = [Tensor_1203], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_1203: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1204: operator = TypecastForward, inputs = [Tensor_152], outputs = [Tensor_1205], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_1205: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1206: operator = TypecastForward, inputs = [Tensor_153], outputs = [Tensor_1207], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_1207: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1208: operator = LinearForward, inputs = [Tensor_1203, Tensor_1205, Tensor_1207], outputs = [Tensor_1209], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_1209: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1210: operator = TypecastForward, inputs = [Tensor_1209], outputs = [Tensor_1211], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_1211: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1212: operator = ElemwiseUnaryForward, inputs = [Tensor_1211], outputs = [Tensor_1213], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_GELU]
Tensor_1213: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1214: operator = TypecastForward, inputs = [Tensor_1213], outputs = [Tensor_1215], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_1215: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1216: operator = TypecastForward, inputs = [Tensor_1215], outputs = [Tensor_1217], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_1217: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1218: operator = LayernormForward, inputs = [Tensor_1217, Tensor_156, Tensor_157], outputs = [Tensor_1219, Tensor_1220, Tensor_1221], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_1219: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_1220: shape = [25216], stride = [1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_1221: shape = [25216], stride = [1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1222: operator = TypecastForward, inputs = [Tensor_1219], outputs = [Tensor_1223], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_1223: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1224: operator = TypecastForward, inputs = [Tensor_154], outputs = [Tensor_1225], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_1225: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1226: operator = TypecastForward, inputs = [Tensor_155], outputs = [Tensor_1227], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_1227: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1228: operator = LinearForward, inputs = [Tensor_1223, Tensor_1225, Tensor_1227], outputs = [Tensor_1229], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_1229: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1230: operator = ReshapeForward, inputs = [Tensor_1229], outputs = [Tensor_1231], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 151296 stride: 768 stride: 1]
Tensor_1231: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_1232: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1233: operator = ElemwiseUnaryForward, inputs = [Tensor_1232], outputs = [Tensor_1234], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_ADD scal1_float: 0.959090889]
Tensor_1234: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1235: operator = ElemwiseUnaryForward, inputs = [Tensor_1234], outputs = [Tensor_1236], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_FLOOR]
Tensor_1236: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1237: operator = ElemwiseUnaryForward, inputs = [Tensor_1231], outputs = [Tensor_1238], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_DIV scal1_float: 0.959090889]
Tensor_1238: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1239: operator = ReshapeForward, inputs = [Tensor_1236], outputs = [Tensor_1240], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 1 stride: 0 stride: 0]
Tensor_1240: shape = [128, 197, 768], stride = [1, 0, 0], offset = 0, size = 128, datatype = float16, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1241: operator = ElemwiseBinaryForward, inputs = [Tensor_1238, Tensor_1240], outputs = [Tensor_1242], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_MUL]
Tensor_1242: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1243: operator = ElemwiseUnaryForward, inputs = [Tensor_1195], outputs = [Tensor_1244], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 1]
Tensor_1244: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1245: operator = TypecastForward, inputs = [Tensor_1242], outputs = [Tensor_1246], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_1246: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1247: operator = ElemwiseBinaryForward, inputs = [Tensor_1244, Tensor_1246], outputs = [Tensor_1248], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_1248: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1249: operator = LayernormForward, inputs = [Tensor_1248, Tensor_188, Tensor_189], outputs = [Tensor_1250, Tensor_1251, Tensor_1252], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_1250: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_1251: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_1252: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1253: operator = TypecastForward, inputs = [Tensor_1250], outputs = [Tensor_1254], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_1254: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1255: operator = TypecastForward, inputs = [Tensor_176], outputs = [Tensor_1256], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_1256: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1257: operator = TypecastForward, inputs = [Tensor_177], outputs = [Tensor_1258], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_1258: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1259: operator = LinearForward, inputs = [Tensor_1254, Tensor_1256, Tensor_1258], outputs = [Tensor_1260], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_1260: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1261: operator = TypecastForward, inputs = [Tensor_1250], outputs = [Tensor_1262], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_1262: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1263: operator = TypecastForward, inputs = [Tensor_168], outputs = [Tensor_1264], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_1264: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1265: operator = TypecastForward, inputs = [Tensor_169], outputs = [Tensor_1266], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_1266: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1267: operator = LinearForward, inputs = [Tensor_1262, Tensor_1264, Tensor_1266], outputs = [Tensor_1268], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_1268: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1269: operator = TypecastForward, inputs = [Tensor_1250], outputs = [Tensor_1270], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_1270: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1271: operator = TypecastForward, inputs = [Tensor_172], outputs = [Tensor_1272], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_1272: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1273: operator = TypecastForward, inputs = [Tensor_173], outputs = [Tensor_1274], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_1274: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1275: operator = LinearForward, inputs = [Tensor_1270, Tensor_1272, Tensor_1274], outputs = [Tensor_1276], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_1276: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1277: operator = ElemwiseUnaryForward, inputs = [Tensor_1260], outputs = [Tensor_1278], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 0.125]
Tensor_1278: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1279: operator = ReshapeForward, inputs = [Tensor_1278], outputs = [Tensor_1280], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 12 shape: 64 stride: 151296 stride: 768 stride: 64 stride: 1]
Tensor_1280: shape = [128, 197, 12, 64], stride = [151296, 768, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1281: operator = ReshapeForward, inputs = [Tensor_1280], outputs = [Tensor_1282], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 64 stride: 151296 stride: 64 stride: 768 stride: 1]
Tensor_1282: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1283: operator = ReshapeForward, inputs = [Tensor_1268], outputs = [Tensor_1284], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 12 shape: 64 stride: 151296 stride: 768 stride: 64 stride: 1]
Tensor_1284: shape = [128, 197, 12, 64], stride = [151296, 768, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1285: operator = ReshapeForward, inputs = [Tensor_1284], outputs = [Tensor_1286], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 64 stride: 151296 stride: 64 stride: 768 stride: 1]
Tensor_1286: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1287: operator = ReshapeForward, inputs = [Tensor_1276], outputs = [Tensor_1288], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 12 shape: 64 stride: 151296 stride: 768 stride: 64 stride: 1]
Tensor_1288: shape = [128, 197, 12, 64], stride = [151296, 768, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1289: operator = ReshapeForward, inputs = [Tensor_1288], outputs = [Tensor_1290], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 64 stride: 151296 stride: 64 stride: 768 stride: 1]
Tensor_1290: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1291: operator = ContiguousForward, inputs = [Tensor_1282], outputs = [Tensor_1292], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_1292: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1293: operator = ReshapeForward, inputs = [Tensor_1292], outputs = [Tensor_1294], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_1294: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1295: operator = ContiguousForward, inputs = [Tensor_1286], outputs = [Tensor_1296], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_1296: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1297: operator = ReshapeForward, inputs = [Tensor_1296], outputs = [Tensor_1298], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_1298: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1299: operator = ContiguousForward, inputs = [Tensor_1290], outputs = [Tensor_1300], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_1300: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1301: operator = ReshapeForward, inputs = [Tensor_1300], outputs = [Tensor_1302], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_1302: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1303: operator = ReshapeForward, inputs = [Tensor_1298], outputs = [Tensor_1304], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 64 shape: 197 stride: 12608 stride: 1 stride: 64]
Tensor_1304: shape = [1536, 64, 197], stride = [12608, 1, 64], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1305: operator = BatchedMatmulForward, inputs = [Tensor_1294, Tensor_1304], outputs = [Tensor_1306], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_1306: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1307: operator = ReshapeForward, inputs = [Tensor_1306], outputs = [Tensor_1308], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 197 stride: 465708 stride: 38809 stride: 197 stride: 1]
Tensor_1308: shape = [128, 12, 197, 197], stride = [465708, 38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1309: operator = ReshapeForward, inputs = [Tensor_552], outputs = [Tensor_1310], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 1 shape: 197 stride: 197 stride: 197 stride: 1]
Tensor_1310: shape = [128, 1, 197], stride = [197, 197, 1], offset = 0, size = 25216, datatype = boolean, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1311: operator = ReshapeForward, inputs = [Tensor_1310], outputs = [Tensor_1312], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 1 shape: 1 shape: 197 stride: 197 stride: 197 stride: 197 stride: 1]
Tensor_1312: shape = [128, 1, 1, 197], stride = [197, 197, 197, 1], offset = 0, size = 25216, datatype = boolean, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1313: operator = ReshapeForward, inputs = [Tensor_1312], outputs = [Tensor_1314], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 197 stride: 197 stride: 0 stride: 0 stride: 1]
Tensor_1314: shape = [128, 12, 197, 197], stride = [197, 0, 0, 1], offset = 0, size = 25216, datatype = boolean, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1315: operator = MaskedFillForward, inputs = [Tensor_1308, Tensor_1314], outputs = [Tensor_1316], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [value_float: -inf]
Tensor_1316: shape = [128, 12, 197, 197], stride = [465708, 38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1317: operator = ReshapeForward, inputs = [Tensor_1316], outputs = [Tensor_1318], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 197 stride: 38809 stride: 197 stride: 1]
Tensor_1318: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1319: operator = TypecastForward, inputs = [Tensor_1318], outputs = [Tensor_1320], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_1320: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1321: operator = SoftmaxForward, inputs = [Tensor_1320], outputs = [Tensor_1322], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dim: 2]
Tensor_1322: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1323: operator = TypecastForward, inputs = [Tensor_1322], outputs = [Tensor_1324], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_1324: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1325: operator = BatchedMatmulForward, inputs = [Tensor_1324, Tensor_1302], outputs = [Tensor_1326], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_1326: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1327: operator = ReshapeForward, inputs = [Tensor_1326], outputs = [Tensor_1328], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 197 shape: 1536 shape: 64 stride: 64 stride: 12608 stride: 1]
Tensor_1328: shape = [197, 1536, 64], stride = [64, 12608, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1329: operator = ContiguousForward, inputs = [Tensor_1328], outputs = [Tensor_1330], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_1330: shape = [197, 1536, 64], stride = [98304, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1331: operator = ReshapeForward, inputs = [Tensor_1330], outputs = [Tensor_1332], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 197 shape: 128 shape: 768 stride: 98304 stride: 768 stride: 1]
Tensor_1332: shape = [197, 128, 768], stride = [98304, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1333: operator = ReshapeForward, inputs = [Tensor_1332], outputs = [Tensor_1334], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 768 stride: 98304 stride: 1]
Tensor_1334: shape = [128, 197, 768], stride = [768, 98304, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1335: operator = TypecastForward, inputs = [Tensor_1334], outputs = [Tensor_1336], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_1336: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1337: operator = LayernormForward, inputs = [Tensor_1336, Tensor_184, Tensor_185], outputs = [Tensor_1338, Tensor_1339, Tensor_1340], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_1338: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_1339: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_1340: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1341: operator = TypecastForward, inputs = [Tensor_1338], outputs = [Tensor_1342], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_1342: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1343: operator = TypecastForward, inputs = [Tensor_180], outputs = [Tensor_1344], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_1344: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1345: operator = TypecastForward, inputs = [Tensor_181], outputs = [Tensor_1346], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_1346: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1347: operator = LinearForward, inputs = [Tensor_1342, Tensor_1344, Tensor_1346], outputs = [Tensor_1348], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_1348: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_1353: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1354: operator = ElemwiseUnaryForward, inputs = [Tensor_1353], outputs = [Tensor_1355], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_ADD scal1_float: 0.945454538]
Tensor_1355: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1356: operator = ElemwiseUnaryForward, inputs = [Tensor_1355], outputs = [Tensor_1357], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_FLOOR]
Tensor_1357: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1358: operator = ElemwiseUnaryForward, inputs = [Tensor_1348], outputs = [Tensor_1359], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_DIV scal1_float: 0.945454538]
Tensor_1359: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1360: operator = ReshapeForward, inputs = [Tensor_1357], outputs = [Tensor_1361], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 1 stride: 0 stride: 0]
Tensor_1361: shape = [128, 197, 768], stride = [1, 0, 0], offset = 0, size = 128, datatype = float16, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1362: operator = ElemwiseBinaryForward, inputs = [Tensor_1359, Tensor_1361], outputs = [Tensor_1363], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_MUL]
Tensor_1363: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1364: operator = ElemwiseUnaryForward, inputs = [Tensor_1248], outputs = [Tensor_1365], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 1]
Tensor_1365: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1366: operator = TypecastForward, inputs = [Tensor_1363], outputs = [Tensor_1367], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_1367: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1368: operator = ElemwiseBinaryForward, inputs = [Tensor_1365, Tensor_1367], outputs = [Tensor_1369], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_1369: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1370: operator = LayernormForward, inputs = [Tensor_1369, Tensor_204, Tensor_205], outputs = [Tensor_1371, Tensor_1372, Tensor_1373], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_1371: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_1372: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_1373: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1374: operator = ReshapeForward, inputs = [Tensor_1371], outputs = [Tensor_1375], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 25216 shape: 768 stride: 768 stride: 1]
Tensor_1375: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1376: operator = TypecastForward, inputs = [Tensor_1375], outputs = [Tensor_1377], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_1377: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1378: operator = TypecastForward, inputs = [Tensor_192], outputs = [Tensor_1379], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_1379: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1380: operator = TypecastForward, inputs = [Tensor_193], outputs = [Tensor_1381], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_1381: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1382: operator = LinearForward, inputs = [Tensor_1377, Tensor_1379, Tensor_1381], outputs = [Tensor_1383], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_1383: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1384: operator = TypecastForward, inputs = [Tensor_1383], outputs = [Tensor_1385], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_1385: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1386: operator = ElemwiseUnaryForward, inputs = [Tensor_1385], outputs = [Tensor_1387], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_GELU]
Tensor_1387: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1388: operator = TypecastForward, inputs = [Tensor_1387], outputs = [Tensor_1389], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_1389: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1390: operator = TypecastForward, inputs = [Tensor_1389], outputs = [Tensor_1391], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_1391: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1392: operator = LayernormForward, inputs = [Tensor_1391, Tensor_196, Tensor_197], outputs = [Tensor_1393, Tensor_1394, Tensor_1395], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_1393: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_1394: shape = [25216], stride = [1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_1395: shape = [25216], stride = [1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1396: operator = TypecastForward, inputs = [Tensor_1393], outputs = [Tensor_1397], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_1397: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1398: operator = TypecastForward, inputs = [Tensor_194], outputs = [Tensor_1399], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_1399: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1400: operator = TypecastForward, inputs = [Tensor_195], outputs = [Tensor_1401], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_1401: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1402: operator = LinearForward, inputs = [Tensor_1397, Tensor_1399, Tensor_1401], outputs = [Tensor_1403], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_1403: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1404: operator = ReshapeForward, inputs = [Tensor_1403], outputs = [Tensor_1405], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 151296 stride: 768 stride: 1]
Tensor_1405: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_1406: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1407: operator = ElemwiseUnaryForward, inputs = [Tensor_1406], outputs = [Tensor_1408], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_ADD scal1_float: 0.945454538]
Tensor_1408: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1409: operator = ElemwiseUnaryForward, inputs = [Tensor_1408], outputs = [Tensor_1410], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_FLOOR]
Tensor_1410: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1411: operator = ElemwiseUnaryForward, inputs = [Tensor_1405], outputs = [Tensor_1412], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_DIV scal1_float: 0.945454538]
Tensor_1412: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1413: operator = ReshapeForward, inputs = [Tensor_1410], outputs = [Tensor_1414], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 1 stride: 0 stride: 0]
Tensor_1414: shape = [128, 197, 768], stride = [1, 0, 0], offset = 0, size = 128, datatype = float16, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1415: operator = ElemwiseBinaryForward, inputs = [Tensor_1412, Tensor_1414], outputs = [Tensor_1416], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_MUL]
Tensor_1416: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1417: operator = ElemwiseUnaryForward, inputs = [Tensor_1369], outputs = [Tensor_1418], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 1]
Tensor_1418: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1419: operator = TypecastForward, inputs = [Tensor_1416], outputs = [Tensor_1420], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_1420: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1421: operator = ElemwiseBinaryForward, inputs = [Tensor_1418, Tensor_1420], outputs = [Tensor_1422], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_1422: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1423: operator = LayernormForward, inputs = [Tensor_1422, Tensor_228, Tensor_229], outputs = [Tensor_1424, Tensor_1425, Tensor_1426], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_1424: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_1425: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_1426: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1427: operator = TypecastForward, inputs = [Tensor_1424], outputs = [Tensor_1428], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_1428: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1429: operator = TypecastForward, inputs = [Tensor_216], outputs = [Tensor_1430], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_1430: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1431: operator = TypecastForward, inputs = [Tensor_217], outputs = [Tensor_1432], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_1432: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1433: operator = LinearForward, inputs = [Tensor_1428, Tensor_1430, Tensor_1432], outputs = [Tensor_1434], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_1434: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1435: operator = TypecastForward, inputs = [Tensor_1424], outputs = [Tensor_1436], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_1436: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1437: operator = TypecastForward, inputs = [Tensor_208], outputs = [Tensor_1438], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_1438: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1439: operator = TypecastForward, inputs = [Tensor_209], outputs = [Tensor_1440], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_1440: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1441: operator = LinearForward, inputs = [Tensor_1436, Tensor_1438, Tensor_1440], outputs = [Tensor_1442], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_1442: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1443: operator = TypecastForward, inputs = [Tensor_1424], outputs = [Tensor_1444], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_1444: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1445: operator = TypecastForward, inputs = [Tensor_212], outputs = [Tensor_1446], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_1446: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1447: operator = TypecastForward, inputs = [Tensor_213], outputs = [Tensor_1448], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_1448: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1449: operator = LinearForward, inputs = [Tensor_1444, Tensor_1446, Tensor_1448], outputs = [Tensor_1450], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_1450: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1451: operator = ElemwiseUnaryForward, inputs = [Tensor_1434], outputs = [Tensor_1452], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 0.125]
Tensor_1452: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1453: operator = ReshapeForward, inputs = [Tensor_1452], outputs = [Tensor_1454], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 12 shape: 64 stride: 151296 stride: 768 stride: 64 stride: 1]
Tensor_1454: shape = [128, 197, 12, 64], stride = [151296, 768, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1455: operator = ReshapeForward, inputs = [Tensor_1454], outputs = [Tensor_1456], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 64 stride: 151296 stride: 64 stride: 768 stride: 1]
Tensor_1456: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1457: operator = ReshapeForward, inputs = [Tensor_1442], outputs = [Tensor_1458], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 12 shape: 64 stride: 151296 stride: 768 stride: 64 stride: 1]
Tensor_1458: shape = [128, 197, 12, 64], stride = [151296, 768, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1459: operator = ReshapeForward, inputs = [Tensor_1458], outputs = [Tensor_1460], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 64 stride: 151296 stride: 64 stride: 768 stride: 1]
Tensor_1460: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1461: operator = ReshapeForward, inputs = [Tensor_1450], outputs = [Tensor_1462], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 12 shape: 64 stride: 151296 stride: 768 stride: 64 stride: 1]
Tensor_1462: shape = [128, 197, 12, 64], stride = [151296, 768, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1463: operator = ReshapeForward, inputs = [Tensor_1462], outputs = [Tensor_1464], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 64 stride: 151296 stride: 64 stride: 768 stride: 1]
Tensor_1464: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1465: operator = ContiguousForward, inputs = [Tensor_1456], outputs = [Tensor_1466], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_1466: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1467: operator = ReshapeForward, inputs = [Tensor_1466], outputs = [Tensor_1468], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_1468: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1469: operator = ContiguousForward, inputs = [Tensor_1460], outputs = [Tensor_1470], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_1470: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1471: operator = ReshapeForward, inputs = [Tensor_1470], outputs = [Tensor_1472], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_1472: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1473: operator = ContiguousForward, inputs = [Tensor_1464], outputs = [Tensor_1474], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_1474: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1475: operator = ReshapeForward, inputs = [Tensor_1474], outputs = [Tensor_1476], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_1476: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1477: operator = ReshapeForward, inputs = [Tensor_1472], outputs = [Tensor_1478], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 64 shape: 197 stride: 12608 stride: 1 stride: 64]
Tensor_1478: shape = [1536, 64, 197], stride = [12608, 1, 64], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1479: operator = BatchedMatmulForward, inputs = [Tensor_1468, Tensor_1478], outputs = [Tensor_1480], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_1480: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1481: operator = ReshapeForward, inputs = [Tensor_1480], outputs = [Tensor_1482], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 197 stride: 465708 stride: 38809 stride: 197 stride: 1]
Tensor_1482: shape = [128, 12, 197, 197], stride = [465708, 38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1483: operator = ReshapeForward, inputs = [Tensor_552], outputs = [Tensor_1484], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 1 shape: 197 stride: 197 stride: 197 stride: 1]
Tensor_1484: shape = [128, 1, 197], stride = [197, 197, 1], offset = 0, size = 25216, datatype = boolean, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1485: operator = ReshapeForward, inputs = [Tensor_1484], outputs = [Tensor_1486], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 1 shape: 1 shape: 197 stride: 197 stride: 197 stride: 197 stride: 1]
Tensor_1486: shape = [128, 1, 1, 197], stride = [197, 197, 197, 1], offset = 0, size = 25216, datatype = boolean, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1487: operator = ReshapeForward, inputs = [Tensor_1486], outputs = [Tensor_1488], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 197 stride: 197 stride: 0 stride: 0 stride: 1]
Tensor_1488: shape = [128, 12, 197, 197], stride = [197, 0, 0, 1], offset = 0, size = 25216, datatype = boolean, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1489: operator = MaskedFillForward, inputs = [Tensor_1482, Tensor_1488], outputs = [Tensor_1490], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [value_float: -inf]
Tensor_1490: shape = [128, 12, 197, 197], stride = [465708, 38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1491: operator = ReshapeForward, inputs = [Tensor_1490], outputs = [Tensor_1492], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 197 stride: 38809 stride: 197 stride: 1]
Tensor_1492: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1493: operator = TypecastForward, inputs = [Tensor_1492], outputs = [Tensor_1494], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_1494: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1495: operator = SoftmaxForward, inputs = [Tensor_1494], outputs = [Tensor_1496], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dim: 2]
Tensor_1496: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1497: operator = TypecastForward, inputs = [Tensor_1496], outputs = [Tensor_1498], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_1498: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1499: operator = BatchedMatmulForward, inputs = [Tensor_1498, Tensor_1476], outputs = [Tensor_1500], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_1500: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1501: operator = ReshapeForward, inputs = [Tensor_1500], outputs = [Tensor_1502], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 197 shape: 1536 shape: 64 stride: 64 stride: 12608 stride: 1]
Tensor_1502: shape = [197, 1536, 64], stride = [64, 12608, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1503: operator = ContiguousForward, inputs = [Tensor_1502], outputs = [Tensor_1504], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_1504: shape = [197, 1536, 64], stride = [98304, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1505: operator = ReshapeForward, inputs = [Tensor_1504], outputs = [Tensor_1506], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 197 shape: 128 shape: 768 stride: 98304 stride: 768 stride: 1]
Tensor_1506: shape = [197, 128, 768], stride = [98304, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1507: operator = ReshapeForward, inputs = [Tensor_1506], outputs = [Tensor_1508], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 768 stride: 98304 stride: 1]
Tensor_1508: shape = [128, 197, 768], stride = [768, 98304, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1509: operator = TypecastForward, inputs = [Tensor_1508], outputs = [Tensor_1510], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_1510: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1511: operator = LayernormForward, inputs = [Tensor_1510, Tensor_224, Tensor_225], outputs = [Tensor_1512, Tensor_1513, Tensor_1514], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_1512: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_1513: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_1514: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1515: operator = TypecastForward, inputs = [Tensor_1512], outputs = [Tensor_1516], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_1516: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1517: operator = TypecastForward, inputs = [Tensor_220], outputs = [Tensor_1518], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_1518: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1519: operator = TypecastForward, inputs = [Tensor_221], outputs = [Tensor_1520], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_1520: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1521: operator = LinearForward, inputs = [Tensor_1516, Tensor_1518, Tensor_1520], outputs = [Tensor_1522], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_1522: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_1527: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1528: operator = ElemwiseUnaryForward, inputs = [Tensor_1527], outputs = [Tensor_1529], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_ADD scal1_float: 0.931818187]
Tensor_1529: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1530: operator = ElemwiseUnaryForward, inputs = [Tensor_1529], outputs = [Tensor_1531], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_FLOOR]
Tensor_1531: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1532: operator = ElemwiseUnaryForward, inputs = [Tensor_1522], outputs = [Tensor_1533], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_DIV scal1_float: 0.931818187]
Tensor_1533: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1534: operator = ReshapeForward, inputs = [Tensor_1531], outputs = [Tensor_1535], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 1 stride: 0 stride: 0]
Tensor_1535: shape = [128, 197, 768], stride = [1, 0, 0], offset = 0, size = 128, datatype = float16, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1536: operator = ElemwiseBinaryForward, inputs = [Tensor_1533, Tensor_1535], outputs = [Tensor_1537], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_MUL]
Tensor_1537: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1538: operator = ElemwiseUnaryForward, inputs = [Tensor_1422], outputs = [Tensor_1539], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 1]
Tensor_1539: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1540: operator = TypecastForward, inputs = [Tensor_1537], outputs = [Tensor_1541], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_1541: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1542: operator = ElemwiseBinaryForward, inputs = [Tensor_1539, Tensor_1541], outputs = [Tensor_1543], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_1543: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1544: operator = LayernormForward, inputs = [Tensor_1543, Tensor_244, Tensor_245], outputs = [Tensor_1545, Tensor_1546, Tensor_1547], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_1545: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_1546: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_1547: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1548: operator = ReshapeForward, inputs = [Tensor_1545], outputs = [Tensor_1549], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 25216 shape: 768 stride: 768 stride: 1]
Tensor_1549: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1550: operator = TypecastForward, inputs = [Tensor_1549], outputs = [Tensor_1551], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_1551: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1552: operator = TypecastForward, inputs = [Tensor_232], outputs = [Tensor_1553], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_1553: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1554: operator = TypecastForward, inputs = [Tensor_233], outputs = [Tensor_1555], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_1555: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1556: operator = LinearForward, inputs = [Tensor_1551, Tensor_1553, Tensor_1555], outputs = [Tensor_1557], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_1557: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1558: operator = TypecastForward, inputs = [Tensor_1557], outputs = [Tensor_1559], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_1559: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1560: operator = ElemwiseUnaryForward, inputs = [Tensor_1559], outputs = [Tensor_1561], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_GELU]
Tensor_1561: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1562: operator = TypecastForward, inputs = [Tensor_1561], outputs = [Tensor_1563], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_1563: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1564: operator = TypecastForward, inputs = [Tensor_1563], outputs = [Tensor_1565], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_1565: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1566: operator = LayernormForward, inputs = [Tensor_1565, Tensor_236, Tensor_237], outputs = [Tensor_1567, Tensor_1568, Tensor_1569], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_1567: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_1568: shape = [25216], stride = [1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_1569: shape = [25216], stride = [1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1570: operator = TypecastForward, inputs = [Tensor_1567], outputs = [Tensor_1571], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_1571: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1572: operator = TypecastForward, inputs = [Tensor_234], outputs = [Tensor_1573], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_1573: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1574: operator = TypecastForward, inputs = [Tensor_235], outputs = [Tensor_1575], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_1575: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1576: operator = LinearForward, inputs = [Tensor_1571, Tensor_1573, Tensor_1575], outputs = [Tensor_1577], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_1577: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1578: operator = ReshapeForward, inputs = [Tensor_1577], outputs = [Tensor_1579], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 151296 stride: 768 stride: 1]
Tensor_1579: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_1580: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1581: operator = ElemwiseUnaryForward, inputs = [Tensor_1580], outputs = [Tensor_1582], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_ADD scal1_float: 0.931818187]
Tensor_1582: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1583: operator = ElemwiseUnaryForward, inputs = [Tensor_1582], outputs = [Tensor_1584], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_FLOOR]
Tensor_1584: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1585: operator = ElemwiseUnaryForward, inputs = [Tensor_1579], outputs = [Tensor_1586], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_DIV scal1_float: 0.931818187]
Tensor_1586: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1587: operator = ReshapeForward, inputs = [Tensor_1584], outputs = [Tensor_1588], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 1 stride: 0 stride: 0]
Tensor_1588: shape = [128, 197, 768], stride = [1, 0, 0], offset = 0, size = 128, datatype = float16, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1589: operator = ElemwiseBinaryForward, inputs = [Tensor_1586, Tensor_1588], outputs = [Tensor_1590], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_MUL]
Tensor_1590: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1591: operator = ElemwiseUnaryForward, inputs = [Tensor_1543], outputs = [Tensor_1592], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 1]
Tensor_1592: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1593: operator = TypecastForward, inputs = [Tensor_1590], outputs = [Tensor_1594], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_1594: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1595: operator = ElemwiseBinaryForward, inputs = [Tensor_1592, Tensor_1594], outputs = [Tensor_1596], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_1596: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1597: operator = LayernormForward, inputs = [Tensor_1596, Tensor_268, Tensor_269], outputs = [Tensor_1598, Tensor_1599, Tensor_1600], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_1598: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_1599: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_1600: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1601: operator = TypecastForward, inputs = [Tensor_1598], outputs = [Tensor_1602], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_1602: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1603: operator = TypecastForward, inputs = [Tensor_256], outputs = [Tensor_1604], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_1604: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1605: operator = TypecastForward, inputs = [Tensor_257], outputs = [Tensor_1606], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_1606: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1607: operator = LinearForward, inputs = [Tensor_1602, Tensor_1604, Tensor_1606], outputs = [Tensor_1608], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_1608: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1609: operator = TypecastForward, inputs = [Tensor_1598], outputs = [Tensor_1610], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_1610: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1611: operator = TypecastForward, inputs = [Tensor_248], outputs = [Tensor_1612], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_1612: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1613: operator = TypecastForward, inputs = [Tensor_249], outputs = [Tensor_1614], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_1614: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1615: operator = LinearForward, inputs = [Tensor_1610, Tensor_1612, Tensor_1614], outputs = [Tensor_1616], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_1616: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1617: operator = TypecastForward, inputs = [Tensor_1598], outputs = [Tensor_1618], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_1618: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1619: operator = TypecastForward, inputs = [Tensor_252], outputs = [Tensor_1620], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_1620: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1621: operator = TypecastForward, inputs = [Tensor_253], outputs = [Tensor_1622], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_1622: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1623: operator = LinearForward, inputs = [Tensor_1618, Tensor_1620, Tensor_1622], outputs = [Tensor_1624], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_1624: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1625: operator = ElemwiseUnaryForward, inputs = [Tensor_1608], outputs = [Tensor_1626], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 0.125]
Tensor_1626: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1627: operator = ReshapeForward, inputs = [Tensor_1626], outputs = [Tensor_1628], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 12 shape: 64 stride: 151296 stride: 768 stride: 64 stride: 1]
Tensor_1628: shape = [128, 197, 12, 64], stride = [151296, 768, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1629: operator = ReshapeForward, inputs = [Tensor_1628], outputs = [Tensor_1630], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 64 stride: 151296 stride: 64 stride: 768 stride: 1]
Tensor_1630: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1631: operator = ReshapeForward, inputs = [Tensor_1616], outputs = [Tensor_1632], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 12 shape: 64 stride: 151296 stride: 768 stride: 64 stride: 1]
Tensor_1632: shape = [128, 197, 12, 64], stride = [151296, 768, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1633: operator = ReshapeForward, inputs = [Tensor_1632], outputs = [Tensor_1634], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 64 stride: 151296 stride: 64 stride: 768 stride: 1]
Tensor_1634: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1635: operator = ReshapeForward, inputs = [Tensor_1624], outputs = [Tensor_1636], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 12 shape: 64 stride: 151296 stride: 768 stride: 64 stride: 1]
Tensor_1636: shape = [128, 197, 12, 64], stride = [151296, 768, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1637: operator = ReshapeForward, inputs = [Tensor_1636], outputs = [Tensor_1638], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 64 stride: 151296 stride: 64 stride: 768 stride: 1]
Tensor_1638: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1639: operator = ContiguousForward, inputs = [Tensor_1630], outputs = [Tensor_1640], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_1640: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1641: operator = ReshapeForward, inputs = [Tensor_1640], outputs = [Tensor_1642], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_1642: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1643: operator = ContiguousForward, inputs = [Tensor_1634], outputs = [Tensor_1644], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_1644: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1645: operator = ReshapeForward, inputs = [Tensor_1644], outputs = [Tensor_1646], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_1646: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1647: operator = ContiguousForward, inputs = [Tensor_1638], outputs = [Tensor_1648], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_1648: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1649: operator = ReshapeForward, inputs = [Tensor_1648], outputs = [Tensor_1650], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_1650: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1651: operator = ReshapeForward, inputs = [Tensor_1646], outputs = [Tensor_1652], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 64 shape: 197 stride: 12608 stride: 1 stride: 64]
Tensor_1652: shape = [1536, 64, 197], stride = [12608, 1, 64], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1653: operator = BatchedMatmulForward, inputs = [Tensor_1642, Tensor_1652], outputs = [Tensor_1654], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_1654: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1655: operator = ReshapeForward, inputs = [Tensor_1654], outputs = [Tensor_1656], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 197 stride: 465708 stride: 38809 stride: 197 stride: 1]
Tensor_1656: shape = [128, 12, 197, 197], stride = [465708, 38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1657: operator = ReshapeForward, inputs = [Tensor_552], outputs = [Tensor_1658], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 1 shape: 197 stride: 197 stride: 197 stride: 1]
Tensor_1658: shape = [128, 1, 197], stride = [197, 197, 1], offset = 0, size = 25216, datatype = boolean, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1659: operator = ReshapeForward, inputs = [Tensor_1658], outputs = [Tensor_1660], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 1 shape: 1 shape: 197 stride: 197 stride: 197 stride: 197 stride: 1]
Tensor_1660: shape = [128, 1, 1, 197], stride = [197, 197, 197, 1], offset = 0, size = 25216, datatype = boolean, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1661: operator = ReshapeForward, inputs = [Tensor_1660], outputs = [Tensor_1662], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 197 stride: 197 stride: 0 stride: 0 stride: 1]
Tensor_1662: shape = [128, 12, 197, 197], stride = [197, 0, 0, 1], offset = 0, size = 25216, datatype = boolean, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1663: operator = MaskedFillForward, inputs = [Tensor_1656, Tensor_1662], outputs = [Tensor_1664], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [value_float: -inf]
Tensor_1664: shape = [128, 12, 197, 197], stride = [465708, 38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1665: operator = ReshapeForward, inputs = [Tensor_1664], outputs = [Tensor_1666], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 197 stride: 38809 stride: 197 stride: 1]
Tensor_1666: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1667: operator = TypecastForward, inputs = [Tensor_1666], outputs = [Tensor_1668], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_1668: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1669: operator = SoftmaxForward, inputs = [Tensor_1668], outputs = [Tensor_1670], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dim: 2]
Tensor_1670: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1671: operator = TypecastForward, inputs = [Tensor_1670], outputs = [Tensor_1672], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_1672: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1673: operator = BatchedMatmulForward, inputs = [Tensor_1672, Tensor_1650], outputs = [Tensor_1674], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_1674: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1675: operator = ReshapeForward, inputs = [Tensor_1674], outputs = [Tensor_1676], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 197 shape: 1536 shape: 64 stride: 64 stride: 12608 stride: 1]
Tensor_1676: shape = [197, 1536, 64], stride = [64, 12608, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1677: operator = ContiguousForward, inputs = [Tensor_1676], outputs = [Tensor_1678], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_1678: shape = [197, 1536, 64], stride = [98304, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1679: operator = ReshapeForward, inputs = [Tensor_1678], outputs = [Tensor_1680], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 197 shape: 128 shape: 768 stride: 98304 stride: 768 stride: 1]
Tensor_1680: shape = [197, 128, 768], stride = [98304, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1681: operator = ReshapeForward, inputs = [Tensor_1680], outputs = [Tensor_1682], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 768 stride: 98304 stride: 1]
Tensor_1682: shape = [128, 197, 768], stride = [768, 98304, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1683: operator = TypecastForward, inputs = [Tensor_1682], outputs = [Tensor_1684], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_1684: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1685: operator = LayernormForward, inputs = [Tensor_1684, Tensor_264, Tensor_265], outputs = [Tensor_1686, Tensor_1687, Tensor_1688], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_1686: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_1687: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_1688: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1689: operator = TypecastForward, inputs = [Tensor_1686], outputs = [Tensor_1690], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_1690: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1691: operator = TypecastForward, inputs = [Tensor_260], outputs = [Tensor_1692], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_1692: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1693: operator = TypecastForward, inputs = [Tensor_261], outputs = [Tensor_1694], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_1694: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1695: operator = LinearForward, inputs = [Tensor_1690, Tensor_1692, Tensor_1694], outputs = [Tensor_1696], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_1696: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_1701: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1702: operator = ElemwiseUnaryForward, inputs = [Tensor_1701], outputs = [Tensor_1703], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_ADD scal1_float: 0.918181837]
Tensor_1703: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1704: operator = ElemwiseUnaryForward, inputs = [Tensor_1703], outputs = [Tensor_1705], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_FLOOR]
Tensor_1705: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1706: operator = ElemwiseUnaryForward, inputs = [Tensor_1696], outputs = [Tensor_1707], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_DIV scal1_float: 0.918181837]
Tensor_1707: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1708: operator = ReshapeForward, inputs = [Tensor_1705], outputs = [Tensor_1709], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 1 stride: 0 stride: 0]
Tensor_1709: shape = [128, 197, 768], stride = [1, 0, 0], offset = 0, size = 128, datatype = float16, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1710: operator = ElemwiseBinaryForward, inputs = [Tensor_1707, Tensor_1709], outputs = [Tensor_1711], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_MUL]
Tensor_1711: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1712: operator = ElemwiseUnaryForward, inputs = [Tensor_1596], outputs = [Tensor_1713], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 1]
Tensor_1713: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1714: operator = TypecastForward, inputs = [Tensor_1711], outputs = [Tensor_1715], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_1715: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1716: operator = ElemwiseBinaryForward, inputs = [Tensor_1713, Tensor_1715], outputs = [Tensor_1717], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_1717: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1718: operator = LayernormForward, inputs = [Tensor_1717, Tensor_284, Tensor_285], outputs = [Tensor_1719, Tensor_1720, Tensor_1721], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_1719: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_1720: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_1721: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1722: operator = ReshapeForward, inputs = [Tensor_1719], outputs = [Tensor_1723], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 25216 shape: 768 stride: 768 stride: 1]
Tensor_1723: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1724: operator = TypecastForward, inputs = [Tensor_1723], outputs = [Tensor_1725], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_1725: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1726: operator = TypecastForward, inputs = [Tensor_272], outputs = [Tensor_1727], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_1727: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1728: operator = TypecastForward, inputs = [Tensor_273], outputs = [Tensor_1729], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_1729: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1730: operator = LinearForward, inputs = [Tensor_1725, Tensor_1727, Tensor_1729], outputs = [Tensor_1731], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_1731: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1732: operator = TypecastForward, inputs = [Tensor_1731], outputs = [Tensor_1733], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_1733: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1734: operator = ElemwiseUnaryForward, inputs = [Tensor_1733], outputs = [Tensor_1735], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_GELU]
Tensor_1735: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1736: operator = TypecastForward, inputs = [Tensor_1735], outputs = [Tensor_1737], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_1737: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1738: operator = TypecastForward, inputs = [Tensor_1737], outputs = [Tensor_1739], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_1739: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1740: operator = LayernormForward, inputs = [Tensor_1739, Tensor_276, Tensor_277], outputs = [Tensor_1741, Tensor_1742, Tensor_1743], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_1741: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_1742: shape = [25216], stride = [1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_1743: shape = [25216], stride = [1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1744: operator = TypecastForward, inputs = [Tensor_1741], outputs = [Tensor_1745], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_1745: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1746: operator = TypecastForward, inputs = [Tensor_274], outputs = [Tensor_1747], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_1747: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1748: operator = TypecastForward, inputs = [Tensor_275], outputs = [Tensor_1749], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_1749: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1750: operator = LinearForward, inputs = [Tensor_1745, Tensor_1747, Tensor_1749], outputs = [Tensor_1751], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_1751: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1752: operator = ReshapeForward, inputs = [Tensor_1751], outputs = [Tensor_1753], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 151296 stride: 768 stride: 1]
Tensor_1753: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_1754: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1755: operator = ElemwiseUnaryForward, inputs = [Tensor_1754], outputs = [Tensor_1756], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_ADD scal1_float: 0.918181837]
Tensor_1756: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1757: operator = ElemwiseUnaryForward, inputs = [Tensor_1756], outputs = [Tensor_1758], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_FLOOR]
Tensor_1758: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1759: operator = ElemwiseUnaryForward, inputs = [Tensor_1753], outputs = [Tensor_1760], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_DIV scal1_float: 0.918181837]
Tensor_1760: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1761: operator = ReshapeForward, inputs = [Tensor_1758], outputs = [Tensor_1762], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 1 stride: 0 stride: 0]
Tensor_1762: shape = [128, 197, 768], stride = [1, 0, 0], offset = 0, size = 128, datatype = float16, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1763: operator = ElemwiseBinaryForward, inputs = [Tensor_1760, Tensor_1762], outputs = [Tensor_1764], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_MUL]
Tensor_1764: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1765: operator = ElemwiseUnaryForward, inputs = [Tensor_1717], outputs = [Tensor_1766], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 1]
Tensor_1766: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1767: operator = TypecastForward, inputs = [Tensor_1764], outputs = [Tensor_1768], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_1768: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1769: operator = ElemwiseBinaryForward, inputs = [Tensor_1766, Tensor_1768], outputs = [Tensor_1770], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_1770: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1771: operator = LayernormForward, inputs = [Tensor_1770, Tensor_308, Tensor_309], outputs = [Tensor_1772, Tensor_1773, Tensor_1774], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_1772: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_1773: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_1774: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1775: operator = TypecastForward, inputs = [Tensor_1772], outputs = [Tensor_1776], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_1776: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1777: operator = TypecastForward, inputs = [Tensor_296], outputs = [Tensor_1778], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_1778: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1779: operator = TypecastForward, inputs = [Tensor_297], outputs = [Tensor_1780], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_1780: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1781: operator = LinearForward, inputs = [Tensor_1776, Tensor_1778, Tensor_1780], outputs = [Tensor_1782], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_1782: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1783: operator = TypecastForward, inputs = [Tensor_1772], outputs = [Tensor_1784], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_1784: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1785: operator = TypecastForward, inputs = [Tensor_288], outputs = [Tensor_1786], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_1786: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1787: operator = TypecastForward, inputs = [Tensor_289], outputs = [Tensor_1788], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_1788: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1789: operator = LinearForward, inputs = [Tensor_1784, Tensor_1786, Tensor_1788], outputs = [Tensor_1790], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_1790: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1791: operator = TypecastForward, inputs = [Tensor_1772], outputs = [Tensor_1792], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_1792: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1793: operator = TypecastForward, inputs = [Tensor_292], outputs = [Tensor_1794], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_1794: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1795: operator = TypecastForward, inputs = [Tensor_293], outputs = [Tensor_1796], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_1796: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1797: operator = LinearForward, inputs = [Tensor_1792, Tensor_1794, Tensor_1796], outputs = [Tensor_1798], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_1798: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1799: operator = ElemwiseUnaryForward, inputs = [Tensor_1782], outputs = [Tensor_1800], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 0.125]
Tensor_1800: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1801: operator = ReshapeForward, inputs = [Tensor_1800], outputs = [Tensor_1802], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 12 shape: 64 stride: 151296 stride: 768 stride: 64 stride: 1]
Tensor_1802: shape = [128, 197, 12, 64], stride = [151296, 768, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1803: operator = ReshapeForward, inputs = [Tensor_1802], outputs = [Tensor_1804], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 64 stride: 151296 stride: 64 stride: 768 stride: 1]
Tensor_1804: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1805: operator = ReshapeForward, inputs = [Tensor_1790], outputs = [Tensor_1806], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 12 shape: 64 stride: 151296 stride: 768 stride: 64 stride: 1]
Tensor_1806: shape = [128, 197, 12, 64], stride = [151296, 768, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1807: operator = ReshapeForward, inputs = [Tensor_1806], outputs = [Tensor_1808], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 64 stride: 151296 stride: 64 stride: 768 stride: 1]
Tensor_1808: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1809: operator = ReshapeForward, inputs = [Tensor_1798], outputs = [Tensor_1810], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 12 shape: 64 stride: 151296 stride: 768 stride: 64 stride: 1]
Tensor_1810: shape = [128, 197, 12, 64], stride = [151296, 768, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1811: operator = ReshapeForward, inputs = [Tensor_1810], outputs = [Tensor_1812], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 64 stride: 151296 stride: 64 stride: 768 stride: 1]
Tensor_1812: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1813: operator = ContiguousForward, inputs = [Tensor_1804], outputs = [Tensor_1814], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_1814: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1815: operator = ReshapeForward, inputs = [Tensor_1814], outputs = [Tensor_1816], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_1816: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1817: operator = ContiguousForward, inputs = [Tensor_1808], outputs = [Tensor_1818], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_1818: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1819: operator = ReshapeForward, inputs = [Tensor_1818], outputs = [Tensor_1820], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_1820: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1821: operator = ContiguousForward, inputs = [Tensor_1812], outputs = [Tensor_1822], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_1822: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1823: operator = ReshapeForward, inputs = [Tensor_1822], outputs = [Tensor_1824], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_1824: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1825: operator = ReshapeForward, inputs = [Tensor_1820], outputs = [Tensor_1826], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 64 shape: 197 stride: 12608 stride: 1 stride: 64]
Tensor_1826: shape = [1536, 64, 197], stride = [12608, 1, 64], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1827: operator = BatchedMatmulForward, inputs = [Tensor_1816, Tensor_1826], outputs = [Tensor_1828], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_1828: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1829: operator = ReshapeForward, inputs = [Tensor_1828], outputs = [Tensor_1830], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 197 stride: 465708 stride: 38809 stride: 197 stride: 1]
Tensor_1830: shape = [128, 12, 197, 197], stride = [465708, 38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1831: operator = ReshapeForward, inputs = [Tensor_552], outputs = [Tensor_1832], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 1 shape: 197 stride: 197 stride: 197 stride: 1]
Tensor_1832: shape = [128, 1, 197], stride = [197, 197, 1], offset = 0, size = 25216, datatype = boolean, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1833: operator = ReshapeForward, inputs = [Tensor_1832], outputs = [Tensor_1834], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 1 shape: 1 shape: 197 stride: 197 stride: 197 stride: 197 stride: 1]
Tensor_1834: shape = [128, 1, 1, 197], stride = [197, 197, 197, 1], offset = 0, size = 25216, datatype = boolean, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1835: operator = ReshapeForward, inputs = [Tensor_1834], outputs = [Tensor_1836], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 197 stride: 197 stride: 0 stride: 0 stride: 1]
Tensor_1836: shape = [128, 12, 197, 197], stride = [197, 0, 0, 1], offset = 0, size = 25216, datatype = boolean, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1837: operator = MaskedFillForward, inputs = [Tensor_1830, Tensor_1836], outputs = [Tensor_1838], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [value_float: -inf]
Tensor_1838: shape = [128, 12, 197, 197], stride = [465708, 38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1839: operator = ReshapeForward, inputs = [Tensor_1838], outputs = [Tensor_1840], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 197 stride: 38809 stride: 197 stride: 1]
Tensor_1840: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1841: operator = TypecastForward, inputs = [Tensor_1840], outputs = [Tensor_1842], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_1842: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1843: operator = SoftmaxForward, inputs = [Tensor_1842], outputs = [Tensor_1844], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dim: 2]
Tensor_1844: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1845: operator = TypecastForward, inputs = [Tensor_1844], outputs = [Tensor_1846], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_1846: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1847: operator = BatchedMatmulForward, inputs = [Tensor_1846, Tensor_1824], outputs = [Tensor_1848], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_1848: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1849: operator = ReshapeForward, inputs = [Tensor_1848], outputs = [Tensor_1850], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 197 shape: 1536 shape: 64 stride: 64 stride: 12608 stride: 1]
Tensor_1850: shape = [197, 1536, 64], stride = [64, 12608, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1851: operator = ContiguousForward, inputs = [Tensor_1850], outputs = [Tensor_1852], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_1852: shape = [197, 1536, 64], stride = [98304, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1853: operator = ReshapeForward, inputs = [Tensor_1852], outputs = [Tensor_1854], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 197 shape: 128 shape: 768 stride: 98304 stride: 768 stride: 1]
Tensor_1854: shape = [197, 128, 768], stride = [98304, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1855: operator = ReshapeForward, inputs = [Tensor_1854], outputs = [Tensor_1856], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 768 stride: 98304 stride: 1]
Tensor_1856: shape = [128, 197, 768], stride = [768, 98304, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1857: operator = TypecastForward, inputs = [Tensor_1856], outputs = [Tensor_1858], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_1858: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1859: operator = LayernormForward, inputs = [Tensor_1858, Tensor_304, Tensor_305], outputs = [Tensor_1860, Tensor_1861, Tensor_1862], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_1860: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_1861: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_1862: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1863: operator = TypecastForward, inputs = [Tensor_1860], outputs = [Tensor_1864], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_1864: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1865: operator = TypecastForward, inputs = [Tensor_300], outputs = [Tensor_1866], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_1866: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1867: operator = TypecastForward, inputs = [Tensor_301], outputs = [Tensor_1868], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_1868: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1869: operator = LinearForward, inputs = [Tensor_1864, Tensor_1866, Tensor_1868], outputs = [Tensor_1870], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_1870: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_1875: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1876: operator = ElemwiseUnaryForward, inputs = [Tensor_1875], outputs = [Tensor_1877], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_ADD scal1_float: 0.904545426]
Tensor_1877: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1878: operator = ElemwiseUnaryForward, inputs = [Tensor_1877], outputs = [Tensor_1879], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_FLOOR]
Tensor_1879: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1880: operator = ElemwiseUnaryForward, inputs = [Tensor_1870], outputs = [Tensor_1881], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_DIV scal1_float: 0.904545426]
Tensor_1881: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1882: operator = ReshapeForward, inputs = [Tensor_1879], outputs = [Tensor_1883], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 1 stride: 0 stride: 0]
Tensor_1883: shape = [128, 197, 768], stride = [1, 0, 0], offset = 0, size = 128, datatype = float16, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1884: operator = ElemwiseBinaryForward, inputs = [Tensor_1881, Tensor_1883], outputs = [Tensor_1885], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_MUL]
Tensor_1885: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1886: operator = ElemwiseUnaryForward, inputs = [Tensor_1770], outputs = [Tensor_1887], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 1]
Tensor_1887: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1888: operator = TypecastForward, inputs = [Tensor_1885], outputs = [Tensor_1889], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_1889: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1890: operator = ElemwiseBinaryForward, inputs = [Tensor_1887, Tensor_1889], outputs = [Tensor_1891], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_1891: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1892: operator = LayernormForward, inputs = [Tensor_1891, Tensor_324, Tensor_325], outputs = [Tensor_1893, Tensor_1894, Tensor_1895], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_1893: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_1894: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_1895: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1896: operator = ReshapeForward, inputs = [Tensor_1893], outputs = [Tensor_1897], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 25216 shape: 768 stride: 768 stride: 1]
Tensor_1897: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1898: operator = TypecastForward, inputs = [Tensor_1897], outputs = [Tensor_1899], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_1899: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1900: operator = TypecastForward, inputs = [Tensor_312], outputs = [Tensor_1901], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_1901: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1902: operator = TypecastForward, inputs = [Tensor_313], outputs = [Tensor_1903], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_1903: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1904: operator = LinearForward, inputs = [Tensor_1899, Tensor_1901, Tensor_1903], outputs = [Tensor_1905], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_1905: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1906: operator = TypecastForward, inputs = [Tensor_1905], outputs = [Tensor_1907], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_1907: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1908: operator = ElemwiseUnaryForward, inputs = [Tensor_1907], outputs = [Tensor_1909], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_GELU]
Tensor_1909: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1910: operator = TypecastForward, inputs = [Tensor_1909], outputs = [Tensor_1911], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_1911: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1912: operator = TypecastForward, inputs = [Tensor_1911], outputs = [Tensor_1913], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_1913: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1914: operator = LayernormForward, inputs = [Tensor_1913, Tensor_316, Tensor_317], outputs = [Tensor_1915, Tensor_1916, Tensor_1917], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_1915: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_1916: shape = [25216], stride = [1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_1917: shape = [25216], stride = [1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1918: operator = TypecastForward, inputs = [Tensor_1915], outputs = [Tensor_1919], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_1919: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1920: operator = TypecastForward, inputs = [Tensor_314], outputs = [Tensor_1921], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_1921: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1922: operator = TypecastForward, inputs = [Tensor_315], outputs = [Tensor_1923], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_1923: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1924: operator = LinearForward, inputs = [Tensor_1919, Tensor_1921, Tensor_1923], outputs = [Tensor_1925], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_1925: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1926: operator = ReshapeForward, inputs = [Tensor_1925], outputs = [Tensor_1927], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 151296 stride: 768 stride: 1]
Tensor_1927: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_1928: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1929: operator = ElemwiseUnaryForward, inputs = [Tensor_1928], outputs = [Tensor_1930], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_ADD scal1_float: 0.904545426]
Tensor_1930: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1931: operator = ElemwiseUnaryForward, inputs = [Tensor_1930], outputs = [Tensor_1932], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_FLOOR]
Tensor_1932: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1933: operator = ElemwiseUnaryForward, inputs = [Tensor_1927], outputs = [Tensor_1934], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_DIV scal1_float: 0.904545426]
Tensor_1934: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1935: operator = ReshapeForward, inputs = [Tensor_1932], outputs = [Tensor_1936], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 1 stride: 0 stride: 0]
Tensor_1936: shape = [128, 197, 768], stride = [1, 0, 0], offset = 0, size = 128, datatype = float16, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1937: operator = ElemwiseBinaryForward, inputs = [Tensor_1934, Tensor_1936], outputs = [Tensor_1938], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_MUL]
Tensor_1938: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1939: operator = ElemwiseUnaryForward, inputs = [Tensor_1891], outputs = [Tensor_1940], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 1]
Tensor_1940: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1941: operator = TypecastForward, inputs = [Tensor_1938], outputs = [Tensor_1942], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_1942: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1943: operator = ElemwiseBinaryForward, inputs = [Tensor_1940, Tensor_1942], outputs = [Tensor_1944], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_1944: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1945: operator = LayernormForward, inputs = [Tensor_1944, Tensor_348, Tensor_349], outputs = [Tensor_1946, Tensor_1947, Tensor_1948], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_1946: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_1947: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_1948: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1949: operator = TypecastForward, inputs = [Tensor_1946], outputs = [Tensor_1950], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_1950: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1951: operator = TypecastForward, inputs = [Tensor_336], outputs = [Tensor_1952], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_1952: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1953: operator = TypecastForward, inputs = [Tensor_337], outputs = [Tensor_1954], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_1954: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1955: operator = LinearForward, inputs = [Tensor_1950, Tensor_1952, Tensor_1954], outputs = [Tensor_1956], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_1956: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1957: operator = TypecastForward, inputs = [Tensor_1946], outputs = [Tensor_1958], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_1958: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1959: operator = TypecastForward, inputs = [Tensor_328], outputs = [Tensor_1960], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_1960: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1961: operator = TypecastForward, inputs = [Tensor_329], outputs = [Tensor_1962], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_1962: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1963: operator = LinearForward, inputs = [Tensor_1958, Tensor_1960, Tensor_1962], outputs = [Tensor_1964], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_1964: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1965: operator = TypecastForward, inputs = [Tensor_1946], outputs = [Tensor_1966], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_1966: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1967: operator = TypecastForward, inputs = [Tensor_332], outputs = [Tensor_1968], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_1968: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1969: operator = TypecastForward, inputs = [Tensor_333], outputs = [Tensor_1970], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_1970: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1971: operator = LinearForward, inputs = [Tensor_1966, Tensor_1968, Tensor_1970], outputs = [Tensor_1972], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_1972: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1973: operator = ElemwiseUnaryForward, inputs = [Tensor_1956], outputs = [Tensor_1974], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 0.125]
Tensor_1974: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1975: operator = ReshapeForward, inputs = [Tensor_1974], outputs = [Tensor_1976], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 12 shape: 64 stride: 151296 stride: 768 stride: 64 stride: 1]
Tensor_1976: shape = [128, 197, 12, 64], stride = [151296, 768, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1977: operator = ReshapeForward, inputs = [Tensor_1976], outputs = [Tensor_1978], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 64 stride: 151296 stride: 64 stride: 768 stride: 1]
Tensor_1978: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1979: operator = ReshapeForward, inputs = [Tensor_1964], outputs = [Tensor_1980], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 12 shape: 64 stride: 151296 stride: 768 stride: 64 stride: 1]
Tensor_1980: shape = [128, 197, 12, 64], stride = [151296, 768, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1981: operator = ReshapeForward, inputs = [Tensor_1980], outputs = [Tensor_1982], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 64 stride: 151296 stride: 64 stride: 768 stride: 1]
Tensor_1982: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1983: operator = ReshapeForward, inputs = [Tensor_1972], outputs = [Tensor_1984], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 12 shape: 64 stride: 151296 stride: 768 stride: 64 stride: 1]
Tensor_1984: shape = [128, 197, 12, 64], stride = [151296, 768, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1985: operator = ReshapeForward, inputs = [Tensor_1984], outputs = [Tensor_1986], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 64 stride: 151296 stride: 64 stride: 768 stride: 1]
Tensor_1986: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1987: operator = ContiguousForward, inputs = [Tensor_1978], outputs = [Tensor_1988], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_1988: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1989: operator = ReshapeForward, inputs = [Tensor_1988], outputs = [Tensor_1990], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_1990: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1991: operator = ContiguousForward, inputs = [Tensor_1982], outputs = [Tensor_1992], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_1992: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1993: operator = ReshapeForward, inputs = [Tensor_1992], outputs = [Tensor_1994], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_1994: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1995: operator = ContiguousForward, inputs = [Tensor_1986], outputs = [Tensor_1996], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_1996: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1997: operator = ReshapeForward, inputs = [Tensor_1996], outputs = [Tensor_1998], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_1998: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_1999: operator = ReshapeForward, inputs = [Tensor_1994], outputs = [Tensor_2000], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 64 shape: 197 stride: 12608 stride: 1 stride: 64]
Tensor_2000: shape = [1536, 64, 197], stride = [12608, 1, 64], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2001: operator = BatchedMatmulForward, inputs = [Tensor_1990, Tensor_2000], outputs = [Tensor_2002], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_2002: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2003: operator = ReshapeForward, inputs = [Tensor_2002], outputs = [Tensor_2004], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 197 stride: 465708 stride: 38809 stride: 197 stride: 1]
Tensor_2004: shape = [128, 12, 197, 197], stride = [465708, 38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2005: operator = ReshapeForward, inputs = [Tensor_552], outputs = [Tensor_2006], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 1 shape: 197 stride: 197 stride: 197 stride: 1]
Tensor_2006: shape = [128, 1, 197], stride = [197, 197, 1], offset = 0, size = 25216, datatype = boolean, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2007: operator = ReshapeForward, inputs = [Tensor_2006], outputs = [Tensor_2008], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 1 shape: 1 shape: 197 stride: 197 stride: 197 stride: 197 stride: 1]
Tensor_2008: shape = [128, 1, 1, 197], stride = [197, 197, 197, 1], offset = 0, size = 25216, datatype = boolean, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2009: operator = ReshapeForward, inputs = [Tensor_2008], outputs = [Tensor_2010], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 197 stride: 197 stride: 0 stride: 0 stride: 1]
Tensor_2010: shape = [128, 12, 197, 197], stride = [197, 0, 0, 1], offset = 0, size = 25216, datatype = boolean, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2011: operator = MaskedFillForward, inputs = [Tensor_2004, Tensor_2010], outputs = [Tensor_2012], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [value_float: -inf]
Tensor_2012: shape = [128, 12, 197, 197], stride = [465708, 38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2013: operator = ReshapeForward, inputs = [Tensor_2012], outputs = [Tensor_2014], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 197 stride: 38809 stride: 197 stride: 1]
Tensor_2014: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2015: operator = TypecastForward, inputs = [Tensor_2014], outputs = [Tensor_2016], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_2016: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2017: operator = SoftmaxForward, inputs = [Tensor_2016], outputs = [Tensor_2018], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dim: 2]
Tensor_2018: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2019: operator = TypecastForward, inputs = [Tensor_2018], outputs = [Tensor_2020], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_2020: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2021: operator = BatchedMatmulForward, inputs = [Tensor_2020, Tensor_1998], outputs = [Tensor_2022], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_2022: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2023: operator = ReshapeForward, inputs = [Tensor_2022], outputs = [Tensor_2024], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 197 shape: 1536 shape: 64 stride: 64 stride: 12608 stride: 1]
Tensor_2024: shape = [197, 1536, 64], stride = [64, 12608, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2025: operator = ContiguousForward, inputs = [Tensor_2024], outputs = [Tensor_2026], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_2026: shape = [197, 1536, 64], stride = [98304, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2027: operator = ReshapeForward, inputs = [Tensor_2026], outputs = [Tensor_2028], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 197 shape: 128 shape: 768 stride: 98304 stride: 768 stride: 1]
Tensor_2028: shape = [197, 128, 768], stride = [98304, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2029: operator = ReshapeForward, inputs = [Tensor_2028], outputs = [Tensor_2030], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 768 stride: 98304 stride: 1]
Tensor_2030: shape = [128, 197, 768], stride = [768, 98304, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2031: operator = TypecastForward, inputs = [Tensor_2030], outputs = [Tensor_2032], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_2032: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2033: operator = LayernormForward, inputs = [Tensor_2032, Tensor_344, Tensor_345], outputs = [Tensor_2034, Tensor_2035, Tensor_2036], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_2034: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_2035: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_2036: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2037: operator = TypecastForward, inputs = [Tensor_2034], outputs = [Tensor_2038], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_2038: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2039: operator = TypecastForward, inputs = [Tensor_340], outputs = [Tensor_2040], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_2040: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2041: operator = TypecastForward, inputs = [Tensor_341], outputs = [Tensor_2042], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_2042: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2043: operator = LinearForward, inputs = [Tensor_2038, Tensor_2040, Tensor_2042], outputs = [Tensor_2044], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_2044: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_2049: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2050: operator = ElemwiseUnaryForward, inputs = [Tensor_2049], outputs = [Tensor_2051], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_ADD scal1_float: 0.890909076]
Tensor_2051: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2052: operator = ElemwiseUnaryForward, inputs = [Tensor_2051], outputs = [Tensor_2053], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_FLOOR]
Tensor_2053: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2054: operator = ElemwiseUnaryForward, inputs = [Tensor_2044], outputs = [Tensor_2055], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_DIV scal1_float: 0.890909076]
Tensor_2055: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2056: operator = ReshapeForward, inputs = [Tensor_2053], outputs = [Tensor_2057], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 1 stride: 0 stride: 0]
Tensor_2057: shape = [128, 197, 768], stride = [1, 0, 0], offset = 0, size = 128, datatype = float16, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2058: operator = ElemwiseBinaryForward, inputs = [Tensor_2055, Tensor_2057], outputs = [Tensor_2059], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_MUL]
Tensor_2059: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2060: operator = ElemwiseUnaryForward, inputs = [Tensor_1944], outputs = [Tensor_2061], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 1]
Tensor_2061: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2062: operator = TypecastForward, inputs = [Tensor_2059], outputs = [Tensor_2063], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_2063: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2064: operator = ElemwiseBinaryForward, inputs = [Tensor_2061, Tensor_2063], outputs = [Tensor_2065], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_2065: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2066: operator = LayernormForward, inputs = [Tensor_2065, Tensor_364, Tensor_365], outputs = [Tensor_2067, Tensor_2068, Tensor_2069], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_2067: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_2068: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_2069: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2070: operator = ReshapeForward, inputs = [Tensor_2067], outputs = [Tensor_2071], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 25216 shape: 768 stride: 768 stride: 1]
Tensor_2071: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2072: operator = TypecastForward, inputs = [Tensor_2071], outputs = [Tensor_2073], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_2073: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2074: operator = TypecastForward, inputs = [Tensor_352], outputs = [Tensor_2075], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_2075: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2076: operator = TypecastForward, inputs = [Tensor_353], outputs = [Tensor_2077], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_2077: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2078: operator = LinearForward, inputs = [Tensor_2073, Tensor_2075, Tensor_2077], outputs = [Tensor_2079], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_2079: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2080: operator = TypecastForward, inputs = [Tensor_2079], outputs = [Tensor_2081], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_2081: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2082: operator = ElemwiseUnaryForward, inputs = [Tensor_2081], outputs = [Tensor_2083], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_GELU]
Tensor_2083: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2084: operator = TypecastForward, inputs = [Tensor_2083], outputs = [Tensor_2085], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_2085: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2086: operator = TypecastForward, inputs = [Tensor_2085], outputs = [Tensor_2087], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_2087: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2088: operator = LayernormForward, inputs = [Tensor_2087, Tensor_356, Tensor_357], outputs = [Tensor_2089, Tensor_2090, Tensor_2091], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_2089: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_2090: shape = [25216], stride = [1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_2091: shape = [25216], stride = [1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2092: operator = TypecastForward, inputs = [Tensor_2089], outputs = [Tensor_2093], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_2093: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2094: operator = TypecastForward, inputs = [Tensor_354], outputs = [Tensor_2095], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_2095: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2096: operator = TypecastForward, inputs = [Tensor_355], outputs = [Tensor_2097], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_2097: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2098: operator = LinearForward, inputs = [Tensor_2093, Tensor_2095, Tensor_2097], outputs = [Tensor_2099], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_2099: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2100: operator = ReshapeForward, inputs = [Tensor_2099], outputs = [Tensor_2101], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 151296 stride: 768 stride: 1]
Tensor_2101: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_2102: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2103: operator = ElemwiseUnaryForward, inputs = [Tensor_2102], outputs = [Tensor_2104], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_ADD scal1_float: 0.890909076]
Tensor_2104: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2105: operator = ElemwiseUnaryForward, inputs = [Tensor_2104], outputs = [Tensor_2106], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_FLOOR]
Tensor_2106: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2107: operator = ElemwiseUnaryForward, inputs = [Tensor_2101], outputs = [Tensor_2108], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_DIV scal1_float: 0.890909076]
Tensor_2108: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2109: operator = ReshapeForward, inputs = [Tensor_2106], outputs = [Tensor_2110], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 1 stride: 0 stride: 0]
Tensor_2110: shape = [128, 197, 768], stride = [1, 0, 0], offset = 0, size = 128, datatype = float16, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2111: operator = ElemwiseBinaryForward, inputs = [Tensor_2108, Tensor_2110], outputs = [Tensor_2112], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_MUL]
Tensor_2112: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2113: operator = ElemwiseUnaryForward, inputs = [Tensor_2065], outputs = [Tensor_2114], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 1]
Tensor_2114: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2115: operator = TypecastForward, inputs = [Tensor_2112], outputs = [Tensor_2116], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_2116: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2117: operator = ElemwiseBinaryForward, inputs = [Tensor_2114, Tensor_2116], outputs = [Tensor_2118], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_2118: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2119: operator = LayernormForward, inputs = [Tensor_2118, Tensor_388, Tensor_389], outputs = [Tensor_2120, Tensor_2121, Tensor_2122], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_2120: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_2121: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_2122: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2123: operator = TypecastForward, inputs = [Tensor_2120], outputs = [Tensor_2124], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_2124: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2125: operator = TypecastForward, inputs = [Tensor_376], outputs = [Tensor_2126], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_2126: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2127: operator = TypecastForward, inputs = [Tensor_377], outputs = [Tensor_2128], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_2128: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2129: operator = LinearForward, inputs = [Tensor_2124, Tensor_2126, Tensor_2128], outputs = [Tensor_2130], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_2130: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2131: operator = TypecastForward, inputs = [Tensor_2120], outputs = [Tensor_2132], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_2132: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2133: operator = TypecastForward, inputs = [Tensor_368], outputs = [Tensor_2134], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_2134: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2135: operator = TypecastForward, inputs = [Tensor_369], outputs = [Tensor_2136], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_2136: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2137: operator = LinearForward, inputs = [Tensor_2132, Tensor_2134, Tensor_2136], outputs = [Tensor_2138], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_2138: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2139: operator = TypecastForward, inputs = [Tensor_2120], outputs = [Tensor_2140], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_2140: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2141: operator = TypecastForward, inputs = [Tensor_372], outputs = [Tensor_2142], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_2142: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2143: operator = TypecastForward, inputs = [Tensor_373], outputs = [Tensor_2144], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_2144: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2145: operator = LinearForward, inputs = [Tensor_2140, Tensor_2142, Tensor_2144], outputs = [Tensor_2146], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_2146: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2147: operator = ElemwiseUnaryForward, inputs = [Tensor_2130], outputs = [Tensor_2148], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 0.125]
Tensor_2148: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2149: operator = ReshapeForward, inputs = [Tensor_2148], outputs = [Tensor_2150], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 12 shape: 64 stride: 151296 stride: 768 stride: 64 stride: 1]
Tensor_2150: shape = [128, 197, 12, 64], stride = [151296, 768, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2151: operator = ReshapeForward, inputs = [Tensor_2150], outputs = [Tensor_2152], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 64 stride: 151296 stride: 64 stride: 768 stride: 1]
Tensor_2152: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2153: operator = ReshapeForward, inputs = [Tensor_2138], outputs = [Tensor_2154], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 12 shape: 64 stride: 151296 stride: 768 stride: 64 stride: 1]
Tensor_2154: shape = [128, 197, 12, 64], stride = [151296, 768, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2155: operator = ReshapeForward, inputs = [Tensor_2154], outputs = [Tensor_2156], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 64 stride: 151296 stride: 64 stride: 768 stride: 1]
Tensor_2156: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2157: operator = ReshapeForward, inputs = [Tensor_2146], outputs = [Tensor_2158], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 12 shape: 64 stride: 151296 stride: 768 stride: 64 stride: 1]
Tensor_2158: shape = [128, 197, 12, 64], stride = [151296, 768, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2159: operator = ReshapeForward, inputs = [Tensor_2158], outputs = [Tensor_2160], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 64 stride: 151296 stride: 64 stride: 768 stride: 1]
Tensor_2160: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2161: operator = ContiguousForward, inputs = [Tensor_2152], outputs = [Tensor_2162], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_2162: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2163: operator = ReshapeForward, inputs = [Tensor_2162], outputs = [Tensor_2164], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_2164: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2165: operator = ContiguousForward, inputs = [Tensor_2156], outputs = [Tensor_2166], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_2166: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2167: operator = ReshapeForward, inputs = [Tensor_2166], outputs = [Tensor_2168], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_2168: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2169: operator = ContiguousForward, inputs = [Tensor_2160], outputs = [Tensor_2170], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_2170: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2171: operator = ReshapeForward, inputs = [Tensor_2170], outputs = [Tensor_2172], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_2172: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2173: operator = ReshapeForward, inputs = [Tensor_2168], outputs = [Tensor_2174], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 64 shape: 197 stride: 12608 stride: 1 stride: 64]
Tensor_2174: shape = [1536, 64, 197], stride = [12608, 1, 64], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2175: operator = BatchedMatmulForward, inputs = [Tensor_2164, Tensor_2174], outputs = [Tensor_2176], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_2176: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2177: operator = ReshapeForward, inputs = [Tensor_2176], outputs = [Tensor_2178], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 197 stride: 465708 stride: 38809 stride: 197 stride: 1]
Tensor_2178: shape = [128, 12, 197, 197], stride = [465708, 38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2179: operator = ReshapeForward, inputs = [Tensor_552], outputs = [Tensor_2180], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 1 shape: 197 stride: 197 stride: 197 stride: 1]
Tensor_2180: shape = [128, 1, 197], stride = [197, 197, 1], offset = 0, size = 25216, datatype = boolean, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2181: operator = ReshapeForward, inputs = [Tensor_2180], outputs = [Tensor_2182], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 1 shape: 1 shape: 197 stride: 197 stride: 197 stride: 197 stride: 1]
Tensor_2182: shape = [128, 1, 1, 197], stride = [197, 197, 197, 1], offset = 0, size = 25216, datatype = boolean, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2183: operator = ReshapeForward, inputs = [Tensor_2182], outputs = [Tensor_2184], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 197 stride: 197 stride: 0 stride: 0 stride: 1]
Tensor_2184: shape = [128, 12, 197, 197], stride = [197, 0, 0, 1], offset = 0, size = 25216, datatype = boolean, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2185: operator = MaskedFillForward, inputs = [Tensor_2178, Tensor_2184], outputs = [Tensor_2186], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [value_float: -inf]
Tensor_2186: shape = [128, 12, 197, 197], stride = [465708, 38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2187: operator = ReshapeForward, inputs = [Tensor_2186], outputs = [Tensor_2188], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 197 stride: 38809 stride: 197 stride: 1]
Tensor_2188: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2189: operator = TypecastForward, inputs = [Tensor_2188], outputs = [Tensor_2190], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_2190: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2191: operator = SoftmaxForward, inputs = [Tensor_2190], outputs = [Tensor_2192], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dim: 2]
Tensor_2192: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2193: operator = TypecastForward, inputs = [Tensor_2192], outputs = [Tensor_2194], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_2194: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2195: operator = BatchedMatmulForward, inputs = [Tensor_2194, Tensor_2172], outputs = [Tensor_2196], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_2196: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2197: operator = ReshapeForward, inputs = [Tensor_2196], outputs = [Tensor_2198], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 197 shape: 1536 shape: 64 stride: 64 stride: 12608 stride: 1]
Tensor_2198: shape = [197, 1536, 64], stride = [64, 12608, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2199: operator = ContiguousForward, inputs = [Tensor_2198], outputs = [Tensor_2200], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_2200: shape = [197, 1536, 64], stride = [98304, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2201: operator = ReshapeForward, inputs = [Tensor_2200], outputs = [Tensor_2202], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 197 shape: 128 shape: 768 stride: 98304 stride: 768 stride: 1]
Tensor_2202: shape = [197, 128, 768], stride = [98304, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2203: operator = ReshapeForward, inputs = [Tensor_2202], outputs = [Tensor_2204], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 768 stride: 98304 stride: 1]
Tensor_2204: shape = [128, 197, 768], stride = [768, 98304, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2205: operator = TypecastForward, inputs = [Tensor_2204], outputs = [Tensor_2206], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_2206: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2207: operator = LayernormForward, inputs = [Tensor_2206, Tensor_384, Tensor_385], outputs = [Tensor_2208, Tensor_2209, Tensor_2210], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_2208: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_2209: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_2210: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2211: operator = TypecastForward, inputs = [Tensor_2208], outputs = [Tensor_2212], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_2212: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2213: operator = TypecastForward, inputs = [Tensor_380], outputs = [Tensor_2214], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_2214: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2215: operator = TypecastForward, inputs = [Tensor_381], outputs = [Tensor_2216], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_2216: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2217: operator = LinearForward, inputs = [Tensor_2212, Tensor_2214, Tensor_2216], outputs = [Tensor_2218], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_2218: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_2223: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2224: operator = ElemwiseUnaryForward, inputs = [Tensor_2223], outputs = [Tensor_2225], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_ADD scal1_float: 0.877272725]
Tensor_2225: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2226: operator = ElemwiseUnaryForward, inputs = [Tensor_2225], outputs = [Tensor_2227], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_FLOOR]
Tensor_2227: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2228: operator = ElemwiseUnaryForward, inputs = [Tensor_2218], outputs = [Tensor_2229], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_DIV scal1_float: 0.877272725]
Tensor_2229: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2230: operator = ReshapeForward, inputs = [Tensor_2227], outputs = [Tensor_2231], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 1 stride: 0 stride: 0]
Tensor_2231: shape = [128, 197, 768], stride = [1, 0, 0], offset = 0, size = 128, datatype = float16, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2232: operator = ElemwiseBinaryForward, inputs = [Tensor_2229, Tensor_2231], outputs = [Tensor_2233], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_MUL]
Tensor_2233: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2234: operator = ElemwiseUnaryForward, inputs = [Tensor_2118], outputs = [Tensor_2235], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 1]
Tensor_2235: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2236: operator = TypecastForward, inputs = [Tensor_2233], outputs = [Tensor_2237], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_2237: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2238: operator = ElemwiseBinaryForward, inputs = [Tensor_2235, Tensor_2237], outputs = [Tensor_2239], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_2239: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2240: operator = LayernormForward, inputs = [Tensor_2239, Tensor_404, Tensor_405], outputs = [Tensor_2241, Tensor_2242, Tensor_2243], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_2241: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_2242: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_2243: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2244: operator = ReshapeForward, inputs = [Tensor_2241], outputs = [Tensor_2245], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 25216 shape: 768 stride: 768 stride: 1]
Tensor_2245: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2246: operator = TypecastForward, inputs = [Tensor_2245], outputs = [Tensor_2247], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_2247: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2248: operator = TypecastForward, inputs = [Tensor_392], outputs = [Tensor_2249], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_2249: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2250: operator = TypecastForward, inputs = [Tensor_393], outputs = [Tensor_2251], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_2251: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2252: operator = LinearForward, inputs = [Tensor_2247, Tensor_2249, Tensor_2251], outputs = [Tensor_2253], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_2253: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2254: operator = TypecastForward, inputs = [Tensor_2253], outputs = [Tensor_2255], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_2255: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2256: operator = ElemwiseUnaryForward, inputs = [Tensor_2255], outputs = [Tensor_2257], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_GELU]
Tensor_2257: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2258: operator = TypecastForward, inputs = [Tensor_2257], outputs = [Tensor_2259], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_2259: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2260: operator = TypecastForward, inputs = [Tensor_2259], outputs = [Tensor_2261], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_2261: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2262: operator = LayernormForward, inputs = [Tensor_2261, Tensor_396, Tensor_397], outputs = [Tensor_2263, Tensor_2264, Tensor_2265], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_2263: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_2264: shape = [25216], stride = [1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_2265: shape = [25216], stride = [1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2266: operator = TypecastForward, inputs = [Tensor_2263], outputs = [Tensor_2267], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_2267: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2268: operator = TypecastForward, inputs = [Tensor_394], outputs = [Tensor_2269], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_2269: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2270: operator = TypecastForward, inputs = [Tensor_395], outputs = [Tensor_2271], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_2271: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2272: operator = LinearForward, inputs = [Tensor_2267, Tensor_2269, Tensor_2271], outputs = [Tensor_2273], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_2273: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2274: operator = ReshapeForward, inputs = [Tensor_2273], outputs = [Tensor_2275], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 151296 stride: 768 stride: 1]
Tensor_2275: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_2276: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2277: operator = ElemwiseUnaryForward, inputs = [Tensor_2276], outputs = [Tensor_2278], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_ADD scal1_float: 0.877272725]
Tensor_2278: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2279: operator = ElemwiseUnaryForward, inputs = [Tensor_2278], outputs = [Tensor_2280], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_FLOOR]
Tensor_2280: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2281: operator = ElemwiseUnaryForward, inputs = [Tensor_2275], outputs = [Tensor_2282], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_DIV scal1_float: 0.877272725]
Tensor_2282: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2283: operator = ReshapeForward, inputs = [Tensor_2280], outputs = [Tensor_2284], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 1 stride: 0 stride: 0]
Tensor_2284: shape = [128, 197, 768], stride = [1, 0, 0], offset = 0, size = 128, datatype = float16, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2285: operator = ElemwiseBinaryForward, inputs = [Tensor_2282, Tensor_2284], outputs = [Tensor_2286], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_MUL]
Tensor_2286: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2287: operator = ElemwiseUnaryForward, inputs = [Tensor_2239], outputs = [Tensor_2288], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 1]
Tensor_2288: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2289: operator = TypecastForward, inputs = [Tensor_2286], outputs = [Tensor_2290], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_2290: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2291: operator = ElemwiseBinaryForward, inputs = [Tensor_2288, Tensor_2290], outputs = [Tensor_2292], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_2292: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2293: operator = LayernormForward, inputs = [Tensor_2292, Tensor_428, Tensor_429], outputs = [Tensor_2294, Tensor_2295, Tensor_2296], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_2294: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_2295: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_2296: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2297: operator = TypecastForward, inputs = [Tensor_2294], outputs = [Tensor_2298], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_2298: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2299: operator = TypecastForward, inputs = [Tensor_416], outputs = [Tensor_2300], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_2300: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2301: operator = TypecastForward, inputs = [Tensor_417], outputs = [Tensor_2302], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_2302: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2303: operator = LinearForward, inputs = [Tensor_2298, Tensor_2300, Tensor_2302], outputs = [Tensor_2304], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_2304: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2305: operator = TypecastForward, inputs = [Tensor_2294], outputs = [Tensor_2306], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_2306: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2307: operator = TypecastForward, inputs = [Tensor_408], outputs = [Tensor_2308], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_2308: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2309: operator = TypecastForward, inputs = [Tensor_409], outputs = [Tensor_2310], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_2310: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2311: operator = LinearForward, inputs = [Tensor_2306, Tensor_2308, Tensor_2310], outputs = [Tensor_2312], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_2312: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2313: operator = TypecastForward, inputs = [Tensor_2294], outputs = [Tensor_2314], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_2314: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2315: operator = TypecastForward, inputs = [Tensor_412], outputs = [Tensor_2316], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_2316: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2317: operator = TypecastForward, inputs = [Tensor_413], outputs = [Tensor_2318], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_2318: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2319: operator = LinearForward, inputs = [Tensor_2314, Tensor_2316, Tensor_2318], outputs = [Tensor_2320], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_2320: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2321: operator = ElemwiseUnaryForward, inputs = [Tensor_2304], outputs = [Tensor_2322], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 0.125]
Tensor_2322: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2323: operator = ReshapeForward, inputs = [Tensor_2322], outputs = [Tensor_2324], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 12 shape: 64 stride: 151296 stride: 768 stride: 64 stride: 1]
Tensor_2324: shape = [128, 197, 12, 64], stride = [151296, 768, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2325: operator = ReshapeForward, inputs = [Tensor_2324], outputs = [Tensor_2326], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 64 stride: 151296 stride: 64 stride: 768 stride: 1]
Tensor_2326: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2327: operator = ReshapeForward, inputs = [Tensor_2312], outputs = [Tensor_2328], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 12 shape: 64 stride: 151296 stride: 768 stride: 64 stride: 1]
Tensor_2328: shape = [128, 197, 12, 64], stride = [151296, 768, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2329: operator = ReshapeForward, inputs = [Tensor_2328], outputs = [Tensor_2330], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 64 stride: 151296 stride: 64 stride: 768 stride: 1]
Tensor_2330: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2331: operator = ReshapeForward, inputs = [Tensor_2320], outputs = [Tensor_2332], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 12 shape: 64 stride: 151296 stride: 768 stride: 64 stride: 1]
Tensor_2332: shape = [128, 197, 12, 64], stride = [151296, 768, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2333: operator = ReshapeForward, inputs = [Tensor_2332], outputs = [Tensor_2334], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 64 stride: 151296 stride: 64 stride: 768 stride: 1]
Tensor_2334: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2335: operator = ContiguousForward, inputs = [Tensor_2326], outputs = [Tensor_2336], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_2336: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2337: operator = ReshapeForward, inputs = [Tensor_2336], outputs = [Tensor_2338], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_2338: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2339: operator = ContiguousForward, inputs = [Tensor_2330], outputs = [Tensor_2340], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_2340: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2341: operator = ReshapeForward, inputs = [Tensor_2340], outputs = [Tensor_2342], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_2342: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2343: operator = ContiguousForward, inputs = [Tensor_2334], outputs = [Tensor_2344], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_2344: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2345: operator = ReshapeForward, inputs = [Tensor_2344], outputs = [Tensor_2346], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_2346: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2347: operator = ReshapeForward, inputs = [Tensor_2342], outputs = [Tensor_2348], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 64 shape: 197 stride: 12608 stride: 1 stride: 64]
Tensor_2348: shape = [1536, 64, 197], stride = [12608, 1, 64], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2349: operator = BatchedMatmulForward, inputs = [Tensor_2338, Tensor_2348], outputs = [Tensor_2350], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_2350: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2351: operator = ReshapeForward, inputs = [Tensor_2350], outputs = [Tensor_2352], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 197 stride: 465708 stride: 38809 stride: 197 stride: 1]
Tensor_2352: shape = [128, 12, 197, 197], stride = [465708, 38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2353: operator = ReshapeForward, inputs = [Tensor_552], outputs = [Tensor_2354], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 1 shape: 197 stride: 197 stride: 197 stride: 1]
Tensor_2354: shape = [128, 1, 197], stride = [197, 197, 1], offset = 0, size = 25216, datatype = boolean, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2355: operator = ReshapeForward, inputs = [Tensor_2354], outputs = [Tensor_2356], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 1 shape: 1 shape: 197 stride: 197 stride: 197 stride: 197 stride: 1]
Tensor_2356: shape = [128, 1, 1, 197], stride = [197, 197, 197, 1], offset = 0, size = 25216, datatype = boolean, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2357: operator = ReshapeForward, inputs = [Tensor_2356], outputs = [Tensor_2358], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 197 stride: 197 stride: 0 stride: 0 stride: 1]
Tensor_2358: shape = [128, 12, 197, 197], stride = [197, 0, 0, 1], offset = 0, size = 25216, datatype = boolean, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2359: operator = MaskedFillForward, inputs = [Tensor_2352, Tensor_2358], outputs = [Tensor_2360], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [value_float: -inf]
Tensor_2360: shape = [128, 12, 197, 197], stride = [465708, 38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2361: operator = ReshapeForward, inputs = [Tensor_2360], outputs = [Tensor_2362], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 197 stride: 38809 stride: 197 stride: 1]
Tensor_2362: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2363: operator = TypecastForward, inputs = [Tensor_2362], outputs = [Tensor_2364], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_2364: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2365: operator = SoftmaxForward, inputs = [Tensor_2364], outputs = [Tensor_2366], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dim: 2]
Tensor_2366: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2367: operator = TypecastForward, inputs = [Tensor_2366], outputs = [Tensor_2368], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_2368: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2369: operator = BatchedMatmulForward, inputs = [Tensor_2368, Tensor_2346], outputs = [Tensor_2370], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_2370: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2371: operator = ReshapeForward, inputs = [Tensor_2370], outputs = [Tensor_2372], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 197 shape: 1536 shape: 64 stride: 64 stride: 12608 stride: 1]
Tensor_2372: shape = [197, 1536, 64], stride = [64, 12608, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2373: operator = ContiguousForward, inputs = [Tensor_2372], outputs = [Tensor_2374], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_2374: shape = [197, 1536, 64], stride = [98304, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2375: operator = ReshapeForward, inputs = [Tensor_2374], outputs = [Tensor_2376], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 197 shape: 128 shape: 768 stride: 98304 stride: 768 stride: 1]
Tensor_2376: shape = [197, 128, 768], stride = [98304, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2377: operator = ReshapeForward, inputs = [Tensor_2376], outputs = [Tensor_2378], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 768 stride: 98304 stride: 1]
Tensor_2378: shape = [128, 197, 768], stride = [768, 98304, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2379: operator = TypecastForward, inputs = [Tensor_2378], outputs = [Tensor_2380], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_2380: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2381: operator = LayernormForward, inputs = [Tensor_2380, Tensor_424, Tensor_425], outputs = [Tensor_2382, Tensor_2383, Tensor_2384], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_2382: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_2383: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_2384: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2385: operator = TypecastForward, inputs = [Tensor_2382], outputs = [Tensor_2386], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_2386: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2387: operator = TypecastForward, inputs = [Tensor_420], outputs = [Tensor_2388], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_2388: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2389: operator = TypecastForward, inputs = [Tensor_421], outputs = [Tensor_2390], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_2390: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2391: operator = LinearForward, inputs = [Tensor_2386, Tensor_2388, Tensor_2390], outputs = [Tensor_2392], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_2392: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_2397: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2398: operator = ElemwiseUnaryForward, inputs = [Tensor_2397], outputs = [Tensor_2399], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_ADD scal1_float: 0.863636374]
Tensor_2399: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2400: operator = ElemwiseUnaryForward, inputs = [Tensor_2399], outputs = [Tensor_2401], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_FLOOR]
Tensor_2401: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2402: operator = ElemwiseUnaryForward, inputs = [Tensor_2392], outputs = [Tensor_2403], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_DIV scal1_float: 0.863636374]
Tensor_2403: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2404: operator = ReshapeForward, inputs = [Tensor_2401], outputs = [Tensor_2405], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 1 stride: 0 stride: 0]
Tensor_2405: shape = [128, 197, 768], stride = [1, 0, 0], offset = 0, size = 128, datatype = float16, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2406: operator = ElemwiseBinaryForward, inputs = [Tensor_2403, Tensor_2405], outputs = [Tensor_2407], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_MUL]
Tensor_2407: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2408: operator = ElemwiseUnaryForward, inputs = [Tensor_2292], outputs = [Tensor_2409], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 1]
Tensor_2409: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2410: operator = TypecastForward, inputs = [Tensor_2407], outputs = [Tensor_2411], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_2411: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2412: operator = ElemwiseBinaryForward, inputs = [Tensor_2409, Tensor_2411], outputs = [Tensor_2413], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_2413: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2414: operator = LayernormForward, inputs = [Tensor_2413, Tensor_444, Tensor_445], outputs = [Tensor_2415, Tensor_2416, Tensor_2417], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_2415: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_2416: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_2417: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2418: operator = ReshapeForward, inputs = [Tensor_2415], outputs = [Tensor_2419], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 25216 shape: 768 stride: 768 stride: 1]
Tensor_2419: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2420: operator = TypecastForward, inputs = [Tensor_2419], outputs = [Tensor_2421], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_2421: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2422: operator = TypecastForward, inputs = [Tensor_432], outputs = [Tensor_2423], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_2423: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2424: operator = TypecastForward, inputs = [Tensor_433], outputs = [Tensor_2425], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_2425: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2426: operator = LinearForward, inputs = [Tensor_2421, Tensor_2423, Tensor_2425], outputs = [Tensor_2427], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_2427: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2428: operator = TypecastForward, inputs = [Tensor_2427], outputs = [Tensor_2429], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_2429: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2430: operator = ElemwiseUnaryForward, inputs = [Tensor_2429], outputs = [Tensor_2431], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_GELU]
Tensor_2431: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2432: operator = TypecastForward, inputs = [Tensor_2431], outputs = [Tensor_2433], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_2433: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2434: operator = TypecastForward, inputs = [Tensor_2433], outputs = [Tensor_2435], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_2435: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2436: operator = LayernormForward, inputs = [Tensor_2435, Tensor_436, Tensor_437], outputs = [Tensor_2437, Tensor_2438, Tensor_2439], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_2437: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_2438: shape = [25216], stride = [1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_2439: shape = [25216], stride = [1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2440: operator = TypecastForward, inputs = [Tensor_2437], outputs = [Tensor_2441], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_2441: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2442: operator = TypecastForward, inputs = [Tensor_434], outputs = [Tensor_2443], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_2443: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2444: operator = TypecastForward, inputs = [Tensor_435], outputs = [Tensor_2445], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_2445: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2446: operator = LinearForward, inputs = [Tensor_2441, Tensor_2443, Tensor_2445], outputs = [Tensor_2447], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_2447: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2448: operator = ReshapeForward, inputs = [Tensor_2447], outputs = [Tensor_2449], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 151296 stride: 768 stride: 1]
Tensor_2449: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_2450: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2451: operator = ElemwiseUnaryForward, inputs = [Tensor_2450], outputs = [Tensor_2452], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_ADD scal1_float: 0.863636374]
Tensor_2452: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2453: operator = ElemwiseUnaryForward, inputs = [Tensor_2452], outputs = [Tensor_2454], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_FLOOR]
Tensor_2454: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2455: operator = ElemwiseUnaryForward, inputs = [Tensor_2449], outputs = [Tensor_2456], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_DIV scal1_float: 0.863636374]
Tensor_2456: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2457: operator = ReshapeForward, inputs = [Tensor_2454], outputs = [Tensor_2458], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 1 stride: 0 stride: 0]
Tensor_2458: shape = [128, 197, 768], stride = [1, 0, 0], offset = 0, size = 128, datatype = float16, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2459: operator = ElemwiseBinaryForward, inputs = [Tensor_2456, Tensor_2458], outputs = [Tensor_2460], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_MUL]
Tensor_2460: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2461: operator = ElemwiseUnaryForward, inputs = [Tensor_2413], outputs = [Tensor_2462], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 1]
Tensor_2462: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2463: operator = TypecastForward, inputs = [Tensor_2460], outputs = [Tensor_2464], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_2464: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2465: operator = ElemwiseBinaryForward, inputs = [Tensor_2462, Tensor_2464], outputs = [Tensor_2466], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_2466: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2467: operator = LayernormForward, inputs = [Tensor_2466, Tensor_468, Tensor_469], outputs = [Tensor_2468, Tensor_2469, Tensor_2470], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_2468: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_2469: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_2470: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2471: operator = TypecastForward, inputs = [Tensor_2468], outputs = [Tensor_2472], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_2472: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2473: operator = TypecastForward, inputs = [Tensor_456], outputs = [Tensor_2474], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_2474: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2475: operator = TypecastForward, inputs = [Tensor_457], outputs = [Tensor_2476], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_2476: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2477: operator = LinearForward, inputs = [Tensor_2472, Tensor_2474, Tensor_2476], outputs = [Tensor_2478], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_2478: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2479: operator = TypecastForward, inputs = [Tensor_2468], outputs = [Tensor_2480], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_2480: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2481: operator = TypecastForward, inputs = [Tensor_448], outputs = [Tensor_2482], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_2482: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2483: operator = TypecastForward, inputs = [Tensor_449], outputs = [Tensor_2484], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_2484: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2485: operator = LinearForward, inputs = [Tensor_2480, Tensor_2482, Tensor_2484], outputs = [Tensor_2486], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_2486: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2487: operator = TypecastForward, inputs = [Tensor_2468], outputs = [Tensor_2488], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_2488: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2489: operator = TypecastForward, inputs = [Tensor_452], outputs = [Tensor_2490], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_2490: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2491: operator = TypecastForward, inputs = [Tensor_453], outputs = [Tensor_2492], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_2492: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2493: operator = LinearForward, inputs = [Tensor_2488, Tensor_2490, Tensor_2492], outputs = [Tensor_2494], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_2494: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2495: operator = ElemwiseUnaryForward, inputs = [Tensor_2478], outputs = [Tensor_2496], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 0.125]
Tensor_2496: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2497: operator = ReshapeForward, inputs = [Tensor_2496], outputs = [Tensor_2498], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 12 shape: 64 stride: 151296 stride: 768 stride: 64 stride: 1]
Tensor_2498: shape = [128, 197, 12, 64], stride = [151296, 768, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2499: operator = ReshapeForward, inputs = [Tensor_2498], outputs = [Tensor_2500], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 64 stride: 151296 stride: 64 stride: 768 stride: 1]
Tensor_2500: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2501: operator = ReshapeForward, inputs = [Tensor_2486], outputs = [Tensor_2502], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 12 shape: 64 stride: 151296 stride: 768 stride: 64 stride: 1]
Tensor_2502: shape = [128, 197, 12, 64], stride = [151296, 768, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2503: operator = ReshapeForward, inputs = [Tensor_2502], outputs = [Tensor_2504], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 64 stride: 151296 stride: 64 stride: 768 stride: 1]
Tensor_2504: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2505: operator = ReshapeForward, inputs = [Tensor_2494], outputs = [Tensor_2506], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 12 shape: 64 stride: 151296 stride: 768 stride: 64 stride: 1]
Tensor_2506: shape = [128, 197, 12, 64], stride = [151296, 768, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2507: operator = ReshapeForward, inputs = [Tensor_2506], outputs = [Tensor_2508], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 64 stride: 151296 stride: 64 stride: 768 stride: 1]
Tensor_2508: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2509: operator = ContiguousForward, inputs = [Tensor_2500], outputs = [Tensor_2510], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_2510: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2511: operator = ReshapeForward, inputs = [Tensor_2510], outputs = [Tensor_2512], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_2512: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2513: operator = ContiguousForward, inputs = [Tensor_2504], outputs = [Tensor_2514], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_2514: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2515: operator = ReshapeForward, inputs = [Tensor_2514], outputs = [Tensor_2516], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_2516: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2517: operator = ContiguousForward, inputs = [Tensor_2508], outputs = [Tensor_2518], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_2518: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2519: operator = ReshapeForward, inputs = [Tensor_2518], outputs = [Tensor_2520], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_2520: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2521: operator = ReshapeForward, inputs = [Tensor_2516], outputs = [Tensor_2522], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 64 shape: 197 stride: 12608 stride: 1 stride: 64]
Tensor_2522: shape = [1536, 64, 197], stride = [12608, 1, 64], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2523: operator = BatchedMatmulForward, inputs = [Tensor_2512, Tensor_2522], outputs = [Tensor_2524], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_2524: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2525: operator = ReshapeForward, inputs = [Tensor_2524], outputs = [Tensor_2526], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 197 stride: 465708 stride: 38809 stride: 197 stride: 1]
Tensor_2526: shape = [128, 12, 197, 197], stride = [465708, 38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2527: operator = ReshapeForward, inputs = [Tensor_552], outputs = [Tensor_2528], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 1 shape: 197 stride: 197 stride: 197 stride: 1]
Tensor_2528: shape = [128, 1, 197], stride = [197, 197, 1], offset = 0, size = 25216, datatype = boolean, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2529: operator = ReshapeForward, inputs = [Tensor_2528], outputs = [Tensor_2530], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 1 shape: 1 shape: 197 stride: 197 stride: 197 stride: 197 stride: 1]
Tensor_2530: shape = [128, 1, 1, 197], stride = [197, 197, 197, 1], offset = 0, size = 25216, datatype = boolean, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2531: operator = ReshapeForward, inputs = [Tensor_2530], outputs = [Tensor_2532], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 197 stride: 197 stride: 0 stride: 0 stride: 1]
Tensor_2532: shape = [128, 12, 197, 197], stride = [197, 0, 0, 1], offset = 0, size = 25216, datatype = boolean, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2533: operator = MaskedFillForward, inputs = [Tensor_2526, Tensor_2532], outputs = [Tensor_2534], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [value_float: -inf]
Tensor_2534: shape = [128, 12, 197, 197], stride = [465708, 38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2535: operator = ReshapeForward, inputs = [Tensor_2534], outputs = [Tensor_2536], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 197 stride: 38809 stride: 197 stride: 1]
Tensor_2536: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2537: operator = TypecastForward, inputs = [Tensor_2536], outputs = [Tensor_2538], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_2538: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2539: operator = SoftmaxForward, inputs = [Tensor_2538], outputs = [Tensor_2540], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dim: 2]
Tensor_2540: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2541: operator = TypecastForward, inputs = [Tensor_2540], outputs = [Tensor_2542], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_2542: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2543: operator = BatchedMatmulForward, inputs = [Tensor_2542, Tensor_2520], outputs = [Tensor_2544], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_2544: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2545: operator = ReshapeForward, inputs = [Tensor_2544], outputs = [Tensor_2546], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 197 shape: 1536 shape: 64 stride: 64 stride: 12608 stride: 1]
Tensor_2546: shape = [197, 1536, 64], stride = [64, 12608, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2547: operator = ContiguousForward, inputs = [Tensor_2546], outputs = [Tensor_2548], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_2548: shape = [197, 1536, 64], stride = [98304, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2549: operator = ReshapeForward, inputs = [Tensor_2548], outputs = [Tensor_2550], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 197 shape: 128 shape: 768 stride: 98304 stride: 768 stride: 1]
Tensor_2550: shape = [197, 128, 768], stride = [98304, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2551: operator = ReshapeForward, inputs = [Tensor_2550], outputs = [Tensor_2552], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 768 stride: 98304 stride: 1]
Tensor_2552: shape = [128, 197, 768], stride = [768, 98304, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2553: operator = TypecastForward, inputs = [Tensor_2552], outputs = [Tensor_2554], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_2554: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2555: operator = LayernormForward, inputs = [Tensor_2554, Tensor_464, Tensor_465], outputs = [Tensor_2556, Tensor_2557, Tensor_2558], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_2556: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_2557: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_2558: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2559: operator = TypecastForward, inputs = [Tensor_2556], outputs = [Tensor_2560], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_2560: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2561: operator = TypecastForward, inputs = [Tensor_460], outputs = [Tensor_2562], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_2562: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2563: operator = TypecastForward, inputs = [Tensor_461], outputs = [Tensor_2564], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_2564: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2565: operator = LinearForward, inputs = [Tensor_2560, Tensor_2562, Tensor_2564], outputs = [Tensor_2566], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_2566: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_2571: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2572: operator = ElemwiseUnaryForward, inputs = [Tensor_2571], outputs = [Tensor_2573], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_ADD scal1_float: 0.85]
Tensor_2573: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2574: operator = ElemwiseUnaryForward, inputs = [Tensor_2573], outputs = [Tensor_2575], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_FLOOR]
Tensor_2575: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2576: operator = ElemwiseUnaryForward, inputs = [Tensor_2566], outputs = [Tensor_2577], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_DIV scal1_float: 0.85]
Tensor_2577: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2578: operator = ReshapeForward, inputs = [Tensor_2575], outputs = [Tensor_2579], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 1 stride: 0 stride: 0]
Tensor_2579: shape = [128, 197, 768], stride = [1, 0, 0], offset = 0, size = 128, datatype = float16, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2580: operator = ElemwiseBinaryForward, inputs = [Tensor_2577, Tensor_2579], outputs = [Tensor_2581], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_MUL]
Tensor_2581: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2582: operator = ElemwiseUnaryForward, inputs = [Tensor_2466], outputs = [Tensor_2583], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 1]
Tensor_2583: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2584: operator = TypecastForward, inputs = [Tensor_2581], outputs = [Tensor_2585], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_2585: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2586: operator = ElemwiseBinaryForward, inputs = [Tensor_2583, Tensor_2585], outputs = [Tensor_2587], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_2587: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2588: operator = LayernormForward, inputs = [Tensor_2587, Tensor_484, Tensor_485], outputs = [Tensor_2589, Tensor_2590, Tensor_2591], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_2589: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_2590: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_2591: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2592: operator = ReshapeForward, inputs = [Tensor_2589], outputs = [Tensor_2593], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 25216 shape: 768 stride: 768 stride: 1]
Tensor_2593: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2594: operator = TypecastForward, inputs = [Tensor_2593], outputs = [Tensor_2595], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_2595: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2596: operator = TypecastForward, inputs = [Tensor_472], outputs = [Tensor_2597], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_2597: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2598: operator = TypecastForward, inputs = [Tensor_473], outputs = [Tensor_2599], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_2599: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2600: operator = LinearForward, inputs = [Tensor_2595, Tensor_2597, Tensor_2599], outputs = [Tensor_2601], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_2601: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2602: operator = TypecastForward, inputs = [Tensor_2601], outputs = [Tensor_2603], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_2603: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2604: operator = ElemwiseUnaryForward, inputs = [Tensor_2603], outputs = [Tensor_2605], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_GELU]
Tensor_2605: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2606: operator = TypecastForward, inputs = [Tensor_2605], outputs = [Tensor_2607], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_2607: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2608: operator = TypecastForward, inputs = [Tensor_2607], outputs = [Tensor_2609], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_2609: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2610: operator = LayernormForward, inputs = [Tensor_2609, Tensor_476, Tensor_477], outputs = [Tensor_2611, Tensor_2612, Tensor_2613], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_2611: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_2612: shape = [25216], stride = [1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_2613: shape = [25216], stride = [1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2614: operator = TypecastForward, inputs = [Tensor_2611], outputs = [Tensor_2615], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_2615: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2616: operator = TypecastForward, inputs = [Tensor_474], outputs = [Tensor_2617], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_2617: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2618: operator = TypecastForward, inputs = [Tensor_475], outputs = [Tensor_2619], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_2619: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2620: operator = LinearForward, inputs = [Tensor_2615, Tensor_2617, Tensor_2619], outputs = [Tensor_2621], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_2621: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2622: operator = ReshapeForward, inputs = [Tensor_2621], outputs = [Tensor_2623], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 151296 stride: 768 stride: 1]
Tensor_2623: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_2624: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2625: operator = ElemwiseUnaryForward, inputs = [Tensor_2624], outputs = [Tensor_2626], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_ADD scal1_float: 0.85]
Tensor_2626: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2627: operator = ElemwiseUnaryForward, inputs = [Tensor_2626], outputs = [Tensor_2628], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_FLOOR]
Tensor_2628: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2629: operator = ElemwiseUnaryForward, inputs = [Tensor_2623], outputs = [Tensor_2630], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_DIV scal1_float: 0.85]
Tensor_2630: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2631: operator = ReshapeForward, inputs = [Tensor_2628], outputs = [Tensor_2632], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 1 stride: 0 stride: 0]
Tensor_2632: shape = [128, 197, 768], stride = [1, 0, 0], offset = 0, size = 128, datatype = float16, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2633: operator = ElemwiseBinaryForward, inputs = [Tensor_2630, Tensor_2632], outputs = [Tensor_2634], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_MUL]
Tensor_2634: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2635: operator = ElemwiseUnaryForward, inputs = [Tensor_2587], outputs = [Tensor_2636], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 1]
Tensor_2636: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2637: operator = TypecastForward, inputs = [Tensor_2634], outputs = [Tensor_2638], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_2638: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2639: operator = ElemwiseBinaryForward, inputs = [Tensor_2636, Tensor_2638], outputs = [Tensor_2640], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_2640: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2641: operator = ReshapeForward, inputs = [Tensor_2640], outputs = [Tensor_2642], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 196 shape: 768 stride: 151296 stride: 768 stride: 1 offset: 768]
Tensor_2642: shape = [128, 196, 768], stride = [151296, 768, 1], offset = 768, size = 19365888, datatype = float32, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2643: operator = MeanForward, inputs = [Tensor_2642], outputs = [Tensor_2644], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dims: 1 divisor: 196]
Tensor_2644: shape = [128, 768], stride = [768, 1], offset = 0, size = 98304, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2645: operator = LayernormForward, inputs = [Tensor_2644, Tensor_488, Tensor_489], outputs = [Tensor_2646, Tensor_2647, Tensor_2648], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_2646: shape = [128, 768], stride = [768, 1], offset = 0, size = 98304, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_2647: shape = [128], stride = [1], offset = 0, size = 128, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_2648: shape = [128], stride = [1], offset = 0, size = 128, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2649: operator = TypecastForward, inputs = [Tensor_2646], outputs = [Tensor_2650], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_2650: shape = [128, 768], stride = [768, 1], offset = 0, size = 98304, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2651: operator = TypecastForward, inputs = [Tensor_490], outputs = [Tensor_2652], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_2652: shape = [1000, 768], stride = [768, 1], offset = 0, size = 768000, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2653: operator = TypecastForward, inputs = [Tensor_491], outputs = [Tensor_2654], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_2654: shape = [1000], stride = [1], offset = 0, size = 1000, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2655: operator = LinearForward, inputs = [Tensor_2650, Tensor_2652, Tensor_2654], outputs = [Tensor_2656], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_2656: shape = [128, 1000], stride = [1000, 1], offset = 0, size = 128000, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2657: operator = ElemwiseUnaryForward, inputs = [Tensor_529], outputs = [Tensor_2658], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_NEG]
Tensor_2658: shape = [128, 1000], stride = [1000, 1], offset = 0, size = 128000, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2659: operator = TypecastForward, inputs = [Tensor_2656], outputs = [Tensor_2660], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_2660: shape = [128, 1000], stride = [1000, 1], offset = 0, size = 128000, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2661: operator = SoftmaxForward, inputs = [Tensor_2660], outputs = [Tensor_2662], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [method: SOFTMAX_LOG dim: 1]
Tensor_2662: shape = [128, 1000], stride = [1000, 1], offset = 0, size = 128000, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2663: operator = ElemwiseBinaryForward, inputs = [Tensor_2658, Tensor_2662], outputs = [Tensor_2664], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_MUL]
Tensor_2664: shape = [128, 1000], stride = [1000, 1], offset = 0, size = 128000, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2665: operator = SumForward, inputs = [Tensor_2664], outputs = [Tensor_2666], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dims: 1]
Tensor_2666: shape = [128], stride = [1], offset = 0, size = 128, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_2667: operator = MeanForward, inputs = [Tensor_2666], outputs = [Tensor_2668], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dims: 0 divisor: 128]
Tensor_2668: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)

Target (result) graph:
Tensor_11530: shape = [768, 3, 16, 16], stride = [768, 256, 16, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11530: 2359296 Ban)
Tensor_11531: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11531: 3072 Ban)
Tensor_11532: shape = [1, 1, 768], stride = [768, 768, 1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11827: 3072 Ban)
Tensor_11533: shape = [199, 768], stride = [768, 1], offset = 0, size = 152832, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11533: 611328 Ban)
Tensor_11534: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11534: 2359296 Ban)
Tensor_11535: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11535: 3072 Ban)
Tensor_11536: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11536: 2359296 Ban)
Tensor_11537: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11537: 3072 Ban)
Tensor_11538: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11538: 2359296 Ban)
Tensor_11539: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11539: 3072 Ban)
Tensor_11540: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11540: 2359296 Ban)
Tensor_11541: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11541: 3072 Ban)
Tensor_11542: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11542: 3072 Ban)
Tensor_11543: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11543: 3072 Ban)
Tensor_11544: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11544: 3072 Ban)
Tensor_11545: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11545: 3072 Ban)
Tensor_11546: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11546: 9437184 Ban)
Tensor_11547: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11547: 12288 Ban)
Tensor_11548: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11548: 9437184 Ban)
Tensor_11549: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11549: 3072 Ban)
Tensor_11550: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11550: 12288 Ban)
Tensor_11551: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11551: 12288 Ban)
Tensor_11552: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11552: 3072 Ban)
Tensor_11553: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11553: 3072 Ban)
Tensor_11554: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11554: 2359296 Ban)
Tensor_11555: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11555: 3072 Ban)
Tensor_11556: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11556: 2359296 Ban)
Tensor_11557: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11557: 3072 Ban)
Tensor_11558: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11558: 2359296 Ban)
Tensor_11559: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11559: 3072 Ban)
Tensor_11560: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11560: 2359296 Ban)
Tensor_11561: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11561: 3072 Ban)
Tensor_11562: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11562: 3072 Ban)
Tensor_11563: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11563: 3072 Ban)
Tensor_11564: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11564: 3072 Ban)
Tensor_11565: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11565: 3072 Ban)
Tensor_11566: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11566: 9437184 Ban)
Tensor_11567: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11567: 12288 Ban)
Tensor_11568: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11568: 9437184 Ban)
Tensor_11569: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11569: 3072 Ban)
Tensor_11570: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11570: 12288 Ban)
Tensor_11571: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11571: 12288 Ban)
Tensor_11572: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11572: 3072 Ban)
Tensor_11573: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11573: 3072 Ban)
Tensor_11574: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11574: 2359296 Ban)
Tensor_11575: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11575: 3072 Ban)
Tensor_11576: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11576: 2359296 Ban)
Tensor_11577: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11577: 3072 Ban)
Tensor_11578: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11578: 2359296 Ban)
Tensor_11579: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11579: 3072 Ban)
Tensor_11580: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11580: 2359296 Ban)
Tensor_11581: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11581: 3072 Ban)
Tensor_11582: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11582: 3072 Ban)
Tensor_11583: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11583: 3072 Ban)
Tensor_11584: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11584: 3072 Ban)
Tensor_11585: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11585: 3072 Ban)
Tensor_11586: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11586: 9437184 Ban)
Tensor_11587: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11587: 12288 Ban)
Tensor_11588: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11588: 9437184 Ban)
Tensor_11589: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11589: 3072 Ban)
Tensor_11590: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11590: 12288 Ban)
Tensor_11591: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11591: 12288 Ban)
Tensor_11592: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11592: 3072 Ban)
Tensor_11593: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11593: 3072 Ban)
Tensor_11594: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11594: 2359296 Ban)
Tensor_11595: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11595: 3072 Ban)
Tensor_11596: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11596: 2359296 Ban)
Tensor_11597: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11597: 3072 Ban)
Tensor_11598: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11598: 2359296 Ban)
Tensor_11599: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11599: 3072 Ban)
Tensor_11600: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11600: 2359296 Ban)
Tensor_11601: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11601: 3072 Ban)
Tensor_11602: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11602: 3072 Ban)
Tensor_11603: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11603: 3072 Ban)
Tensor_11604: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11604: 3072 Ban)
Tensor_11605: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11605: 3072 Ban)
Tensor_11606: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11606: 9437184 Ban)
Tensor_11607: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11607: 12288 Ban)
Tensor_11608: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11608: 9437184 Ban)
Tensor_11609: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11609: 3072 Ban)
Tensor_11610: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11610: 12288 Ban)
Tensor_11611: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11611: 12288 Ban)
Tensor_11612: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11612: 3072 Ban)
Tensor_11613: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11613: 3072 Ban)
Tensor_11614: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11614: 2359296 Ban)
Tensor_11615: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11615: 3072 Ban)
Tensor_11616: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11616: 2359296 Ban)
Tensor_11617: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11617: 3072 Ban)
Tensor_11618: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11618: 2359296 Ban)
Tensor_11619: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11619: 3072 Ban)
Tensor_11620: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11620: 2359296 Ban)
Tensor_11621: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11621: 3072 Ban)
Tensor_11622: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11622: 3072 Ban)
Tensor_11623: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11623: 3072 Ban)
Tensor_11624: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11624: 3072 Ban)
Tensor_11625: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11625: 3072 Ban)
Tensor_11626: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11626: 9437184 Ban)
Tensor_11627: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11627: 12288 Ban)
Tensor_11628: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11628: 9437184 Ban)
Tensor_11629: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11629: 3072 Ban)
Tensor_11630: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11630: 12288 Ban)
Tensor_11631: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11631: 12288 Ban)
Tensor_11632: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11632: 3072 Ban)
Tensor_11633: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11633: 3072 Ban)
Tensor_11634: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11634: 2359296 Ban)
Tensor_11635: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11635: 3072 Ban)
Tensor_11636: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11636: 2359296 Ban)
Tensor_11637: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11637: 3072 Ban)
Tensor_11638: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11638: 2359296 Ban)
Tensor_11639: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11639: 3072 Ban)
Tensor_11640: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11640: 2359296 Ban)
Tensor_11641: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11641: 3072 Ban)
Tensor_11642: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11642: 3072 Ban)
Tensor_11643: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11643: 3072 Ban)
Tensor_11644: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11644: 3072 Ban)
Tensor_11645: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11645: 3072 Ban)
Tensor_11646: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11646: 9437184 Ban)
Tensor_11647: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11647: 12288 Ban)
Tensor_11648: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11648: 9437184 Ban)
Tensor_11649: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11649: 3072 Ban)
Tensor_11650: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11650: 12288 Ban)
Tensor_11651: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11651: 12288 Ban)
Tensor_11652: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11652: 3072 Ban)
Tensor_11653: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11653: 3072 Ban)
Tensor_11654: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11654: 2359296 Ban)
Tensor_11655: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11655: 3072 Ban)
Tensor_11656: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11656: 2359296 Ban)
Tensor_11657: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11657: 3072 Ban)
Tensor_11658: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11658: 2359296 Ban)
Tensor_11659: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11659: 3072 Ban)
Tensor_11660: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11660: 2359296 Ban)
Tensor_11661: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11661: 3072 Ban)
Tensor_11662: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11662: 3072 Ban)
Tensor_11663: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11663: 3072 Ban)
Tensor_11664: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11664: 3072 Ban)
Tensor_11665: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11665: 3072 Ban)
Tensor_11666: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11666: 9437184 Ban)
Tensor_11667: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11667: 12288 Ban)
Tensor_11668: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11668: 9437184 Ban)
Tensor_11669: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11669: 3072 Ban)
Tensor_11670: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11670: 12288 Ban)
Tensor_11671: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11671: 12288 Ban)
Tensor_11672: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11672: 3072 Ban)
Tensor_11673: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11673: 3072 Ban)
Tensor_11674: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11674: 2359296 Ban)
Tensor_11675: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11675: 3072 Ban)
Tensor_11676: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11676: 2359296 Ban)
Tensor_11677: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11677: 3072 Ban)
Tensor_11678: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11678: 2359296 Ban)
Tensor_11679: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11679: 3072 Ban)
Tensor_11680: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11680: 2359296 Ban)
Tensor_11681: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11681: 3072 Ban)
Tensor_11682: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11682: 3072 Ban)
Tensor_11683: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11683: 3072 Ban)
Tensor_11684: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11684: 3072 Ban)
Tensor_11685: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11685: 3072 Ban)
Tensor_11686: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11686: 9437184 Ban)
Tensor_11687: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11687: 12288 Ban)
Tensor_11688: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11688: 9437184 Ban)
Tensor_11689: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11689: 3072 Ban)
Tensor_11690: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11690: 12288 Ban)
Tensor_11691: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11691: 12288 Ban)
Tensor_11692: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11692: 3072 Ban)
Tensor_11693: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11693: 3072 Ban)
Tensor_11694: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11694: 2359296 Ban)
Tensor_11695: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11695: 3072 Ban)
Tensor_11696: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11696: 2359296 Ban)
Tensor_11697: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11697: 3072 Ban)
Tensor_11698: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11698: 2359296 Ban)
Tensor_11699: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11699: 3072 Ban)
Tensor_11700: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11700: 2359296 Ban)
Tensor_11701: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11701: 3072 Ban)
Tensor_11702: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11702: 3072 Ban)
Tensor_11703: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11703: 3072 Ban)
Tensor_11704: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11704: 3072 Ban)
Tensor_11705: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11705: 3072 Ban)
Tensor_11706: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11706: 9437184 Ban)
Tensor_11707: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11707: 12288 Ban)
Tensor_11708: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11708: 9437184 Ban)
Tensor_11709: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11709: 3072 Ban)
Tensor_11710: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11710: 12288 Ban)
Tensor_11711: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11711: 12288 Ban)
Tensor_11712: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11712: 3072 Ban)
Tensor_11713: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11713: 3072 Ban)
Tensor_11714: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11714: 2359296 Ban)
Tensor_11715: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11715: 3072 Ban)
Tensor_11716: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11716: 2359296 Ban)
Tensor_11717: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11717: 3072 Ban)
Tensor_11718: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11718: 2359296 Ban)
Tensor_11719: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11719: 3072 Ban)
Tensor_11720: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11720: 2359296 Ban)
Tensor_11721: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11721: 3072 Ban)
Tensor_11722: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11722: 3072 Ban)
Tensor_11723: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11723: 3072 Ban)
Tensor_11724: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11724: 3072 Ban)
Tensor_11725: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11725: 3072 Ban)
Tensor_11726: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11726: 9437184 Ban)
Tensor_11727: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11727: 12288 Ban)
Tensor_11728: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11728: 9437184 Ban)
Tensor_11729: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11729: 3072 Ban)
Tensor_11730: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11730: 12288 Ban)
Tensor_11731: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11731: 12288 Ban)
Tensor_11732: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11732: 3072 Ban)
Tensor_11733: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11733: 3072 Ban)
Tensor_11734: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11734: 2359296 Ban)
Tensor_11735: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11735: 3072 Ban)
Tensor_11736: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11736: 2359296 Ban)
Tensor_11737: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11737: 3072 Ban)
Tensor_11738: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11738: 2359296 Ban)
Tensor_11739: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11739: 3072 Ban)
Tensor_11740: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11740: 2359296 Ban)
Tensor_11741: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11741: 3072 Ban)
Tensor_11742: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11742: 3072 Ban)
Tensor_11743: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11743: 3072 Ban)
Tensor_11744: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11744: 3072 Ban)
Tensor_11745: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11745: 3072 Ban)
Tensor_11746: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11746: 9437184 Ban)
Tensor_11747: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11747: 12288 Ban)
Tensor_11748: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11748: 9437184 Ban)
Tensor_11749: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11749: 3072 Ban)
Tensor_11750: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11750: 12288 Ban)
Tensor_11751: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11751: 12288 Ban)
Tensor_11752: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11752: 3072 Ban)
Tensor_11753: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11753: 3072 Ban)
Tensor_11754: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11754: 2359296 Ban)
Tensor_11755: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11755: 3072 Ban)
Tensor_11756: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11756: 2359296 Ban)
Tensor_11757: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11757: 3072 Ban)
Tensor_11758: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11758: 2359296 Ban)
Tensor_11759: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11759: 3072 Ban)
Tensor_11760: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11760: 2359296 Ban)
Tensor_11761: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11761: 3072 Ban)
Tensor_11762: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11762: 3072 Ban)
Tensor_11763: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11763: 3072 Ban)
Tensor_11764: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11764: 3072 Ban)
Tensor_11765: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11765: 3072 Ban)
Tensor_11766: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11766: 9437184 Ban)
Tensor_11767: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11767: 12288 Ban)
Tensor_11768: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11768: 9437184 Ban)
Tensor_11769: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11769: 3072 Ban)
Tensor_11770: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11770: 12288 Ban)
Tensor_11771: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11771: 12288 Ban)
Tensor_11772: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11772: 3072 Ban)
Tensor_11773: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11773: 3072 Ban)
Tensor_11774: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11774: 3072 Ban)
Tensor_11775: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11775: 3072 Ban)
Tensor_11776: shape = [1000, 768], stride = [768, 1], offset = 0, size = 768000, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11776: 3072000 Ban)
Tensor_11777: shape = [1000], stride = [1], offset = 0, size = 1000, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11777: 4000 Ban)
Tensor_11778: shape = [128, 3, 224, 224], stride = [150528, 50176, 224, 1], offset = 0, size = 19267584, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11785: 77070336 Ban)
Tensor_11779: shape = [128], stride = [1], offset = 0, size = 128, datatype = int64, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11793: 1024 Bar)
Operation_494: operator = FlipForward, inputs = [Tensor_11778], outputs = [Tensor_11781], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dims: 0]
Tensor_11781: shape = [128, 3, 224, 224], stride = [150528, 50176, 224, 1], offset = 0, size = 19267584, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11783: 77070336 Ban)
Operation_496: operator = ElemwiseUnaryForward, inputs = [Tensor_11781], outputs = [Tensor_11783], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 0.49995169]
Tensor_11783: shape = [128, 3, 224, 224], stride = [150528, 50176, 224, 1], offset = 0, size = 19267584, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11783: 77070336 Bnr)
Operation_498: operator = ElemwiseUnaryForward, inputs = [Tensor_11778], outputs = [Tensor_11785], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 0.500048339]
Tensor_11785: shape = [128, 3, 224, 224], stride = [150528, 50176, 224, 1], offset = 0, size = 19267584, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11785: 77070336 Bnn)
Operation_500: operator = ElemwiseBinaryForward, inputs = [Tensor_11785, Tensor_11783], outputs = [Tensor_11787], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_11787: shape = [128, 3, 224, 224], stride = [150528, 50176, 224, 1], offset = 0, size = 19267584, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11785: 77070336 Bnn)
Tensor_11788: shape = [128, 1000], stride = [1000, 1], offset = 0, size = 128000, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11790: 512000 Ban)
Operation_505: operator = ElemwiseUnaryForward, inputs = [Tensor_11788], outputs = [Tensor_11790], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_FILL scal1_float: 0.0001]
Tensor_11790: shape = [128, 1000], stride = [1000, 1], offset = 0, size = 128000, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11790: 512000 Bnn)
Tensor_11791: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11795: 4 Ban)
Operation_502: operator = ReshapeForward, inputs = [Tensor_11779], outputs = [Tensor_11793], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 1 stride: 1 stride: 1]
Tensor_11793: shape = [128, 1], stride = [1, 1], offset = 0, size = 128, datatype = int64, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11793: 1024 Bnn)
Operation_508: operator = ReshapeForward, inputs = [Tensor_11791], outputs = [Tensor_11795], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 1 stride: 0 stride: 0]
Tensor_11795: shape = [128, 1], stride = [0, 0], offset = 0, size = 1, datatype = float32, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11795: 4 Bnn)
Operation_510: operator = ScatterForward, inputs = [Tensor_11790, Tensor_11793, Tensor_11795], outputs = [Tensor_11797], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dim: 1]
Tensor_11797: shape = [128, 1000], stride = [1000, 1], offset = 0, size = 128000, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11790: 512000 Bnn)
Operation_512: operator = FlipForward, inputs = [Tensor_11779], outputs = [Tensor_11799], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dims: 0]
Tensor_11799: shape = [128], stride = [1], offset = 0, size = 128, datatype = int64, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11805: 1024 Ban)
Tensor_11800: shape = [128, 1000], stride = [1000, 1], offset = 0, size = 128000, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11802: 512000 Ban)
Operation_517: operator = ElemwiseUnaryForward, inputs = [Tensor_11800], outputs = [Tensor_11802], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_FILL scal1_float: 0.0001]
Tensor_11802: shape = [128, 1000], stride = [1000, 1], offset = 0, size = 128000, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11802: 512000 Bnn)
Tensor_11803: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11795: 4 Bnn)
Operation_514: operator = ReshapeForward, inputs = [Tensor_11799], outputs = [Tensor_11805], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 1 stride: 1 stride: 1]
Tensor_11805: shape = [128, 1], stride = [1, 1], offset = 0, size = 128, datatype = int64, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11805: 1024 Bnr)
Operation_520: operator = ReshapeForward, inputs = [Tensor_11803], outputs = [Tensor_11807], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 1 stride: 0 stride: 0]
Tensor_11807: shape = [128, 1], stride = [0, 0], offset = 0, size = 1, datatype = float32, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11795: 4 Bnn)
Operation_522: operator = ScatterForward, inputs = [Tensor_11802, Tensor_11805, Tensor_11807], outputs = [Tensor_11809], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dim: 1]
Tensor_11809: shape = [128, 1000], stride = [1000, 1], offset = 0, size = 128000, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11802: 512000 Bnn)
Operation_524: operator = ElemwiseUnaryForward, inputs = [Tensor_11797], outputs = [Tensor_11811], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 0.500048339]
Tensor_11811: shape = [128, 1000], stride = [1000, 1], offset = 0, size = 128000, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11790: 512000 Bnn)
Operation_526: operator = ElemwiseUnaryForward, inputs = [Tensor_11809], outputs = [Tensor_11813], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 0.49995169]
Tensor_11813: shape = [128, 1000], stride = [1000, 1], offset = 0, size = 128000, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11802: 512000 Bnn)
Operation_528: operator = ElemwiseBinaryForward, inputs = [Tensor_11811, Tensor_11813], outputs = [Tensor_11815], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_11815: shape = [128, 1000], stride = [1000, 1], offset = 0, size = 128000, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11790: 512000 Bnn)
Operation_530: operator = TypecastForward, inputs = [Tensor_11787], outputs = [Tensor_11817], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_11817: shape = [128, 3, 224, 224], stride = [150528, 50176, 224, 1], offset = 0, size = 19267584, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11817: 38535168 Ban)
Operation_532: operator = TypecastForward, inputs = [Tensor_11530], outputs = [Tensor_11819], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_11819: shape = [768, 3, 16, 16], stride = [768, 256, 16, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11819: 1179648 Ban)
Operation_534: operator = TypecastForward, inputs = [Tensor_11531], outputs = [Tensor_11821], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_11821: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11821: 1536 Ban)
Operation_7331: operator = DistributeWithMemAllocForward, inputs = [Tensor_11817], outputs = [Tensor_11823], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_11823: shape = [128, 3, 224, 224], stride = [150528, 50176, 224, 1], offset = 0, size = 19267584, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11823: 19267584 Bar)
Operation_536: operator = Convolution2dForward, inputs = [Tensor_11823, Tensor_11819, Tensor_11821], outputs = [Tensor_11825], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: [stride_h: 16 stride_w: 16 dilation_h: 1 dilation_w: 1 groups: 1]
Tensor_11825: shape = [128, 768, 14, 14], stride = [150528, 196, 14, 1], offset = 0, size = 19267584, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11829: 19267584 Ban)
Operation_542: operator = ReshapeForward, inputs = [Tensor_11532], outputs = [Tensor_11827], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 1 shape: 768 stride: 0 stride: 768 stride: 1]
Tensor_11827: shape = [128, 1, 768], stride = [0, 768, 1], offset = 0, size = 768, datatype = float32, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11827: 3072 Bnn)
Operation_540: operator = ReshapeForward, inputs = [Tensor_11825], outputs = [Tensor_11829], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 196 shape: 768 stride: 150528 stride: 1 stride: 196]
Tensor_11829: shape = [128, 196, 768], stride = [150528, 1, 196], offset = 0, size = 19267584, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11829: 19267584 Bnr)
Operation_544: operator = TypecastForward, inputs = [Tensor_11829], outputs = [Tensor_11831], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_11831: shape = [128, 196, 768], stride = [150528, 768, 1], offset = 0, size = 19267584, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11831: 38535168 Ban)
Operation_546: operator = CatForward, inputs = [Tensor_11827, Tensor_11831], outputs = [Tensor_11833], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [dim: 1]
Tensor_11833: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11840: 38731776 Ban)
Tensor_11834: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11836: 50432 Ban)
Operation_549: operator = ElemwiseUnaryForward, inputs = [Tensor_11834], outputs = [Tensor_11836], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_FILL scal1_float: 0]
Tensor_11836: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11836: 50432 Bnn)
Operation_551: operator = TypecastForward, inputs = [Tensor_11836], outputs = [Tensor_11838], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_BOOLEAN]
Tensor_11838: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = boolean, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11852: 12608 Ban)
Operation_553: operator = ElemwiseUnaryForward, inputs = [Tensor_11833], outputs = [Tensor_11840], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 1]
Tensor_11840: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11840: 38731776 Bnn)
Operation_555: operator = ArangeForward, inputs = [], outputs = [Tensor_11842], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [start: 2 end: 199 step: 1 output_dtype: INT64]
Tensor_11842: shape = [197], stride = [1], offset = 0, size = 197, datatype = int64, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11844: 1576 Ban)
Operation_557: operator = ReshapeForward, inputs = [Tensor_11842], outputs = [Tensor_11844], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1 shape: 197 stride: 197 stride: 1]
Tensor_11844: shape = [1, 197], stride = [197, 1], offset = 0, size = 197, datatype = int64, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11844: 1576 Bnn)
Operation_559: operator = EmbeddingForward, inputs = [Tensor_11533, Tensor_11844], outputs = [Tensor_11846, nullptr], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [norm_type: 2 padding_idx: -1]
Tensor_11846: shape = [1, 197, 768], stride = [151296, 768, 1], offset = 0, size = 151296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11848: 605184 Ban)
Operation_561: operator = ReshapeForward, inputs = [Tensor_11846], outputs = [Tensor_11848], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 0 stride: 768 stride: 1]
Tensor_11848: shape = [128, 197, 768], stride = [0, 768, 1], offset = 0, size = 151296, datatype = float32, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11848: 605184 Bnr)
Operation_563: operator = ElemwiseBinaryForward, inputs = [Tensor_11840, Tensor_11848], outputs = [Tensor_11850], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_11850: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11840: 38731776 Bnn)
Operation_565: operator = ReshapeForward, inputs = [Tensor_11838], outputs = [Tensor_11852], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 1 stride: 197 stride: 1 stride: 1]
Tensor_11852: shape = [128, 197, 1], stride = [197, 1, 1], offset = 0, size = 25216, datatype = boolean, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11852: 12608 Bnn)
Operation_567: operator = TypecastForward, inputs = [Tensor_11852], outputs = [Tensor_11854], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_11854: shape = [128, 197, 1], stride = [197, 1, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11836: 50432 Bnn)
Operation_569: operator = ElemwiseUnaryForward, inputs = [Tensor_11854], outputs = [Tensor_11856], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_RSUB scal1_float: 1]
Tensor_11856: shape = [128, 197, 1], stride = [197, 1, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11836: 50432 Bnn)
Operation_571: operator = ReshapeForward, inputs = [Tensor_11856], outputs = [Tensor_11858], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 197 stride: 1 stride: 0]
Tensor_11858: shape = [128, 197, 768], stride = [197, 1, 0], offset = 0, size = 25216, datatype = float32, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11836: 50432 Bnn)
Operation_573: operator = ElemwiseBinaryForward, inputs = [Tensor_11850, Tensor_11858], outputs = [Tensor_11860], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_MUL]
Tensor_11860: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11860: 38731776 Ban)
Operation_575: operator = LayernormForward, inputs = [Tensor_11860, Tensor_11544, Tensor_11545], outputs = [Tensor_11862, Tensor_11863, Tensor_11864], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_11862: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11862: 38731776 Ban)
Tensor_11863: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11863: 50432 Ban)
Tensor_11864: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11864: 50432 Ban)
Operation_579: operator = TypecastForward, inputs = [Tensor_11862], outputs = [Tensor_11866], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_11866: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11866: 19365888 Ban)
Operation_581: operator = TypecastForward, inputs = [Tensor_11538], outputs = [Tensor_11868], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_11868: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11868: 1179648 Ban)
Operation_583: operator = TypecastForward, inputs = [Tensor_11539], outputs = [Tensor_11870], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_11870: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11821: 1536 Bnn)
Operation_585: operator = LinearForward, inputs = [Tensor_11866, Tensor_11868, Tensor_11870], outputs = [Tensor_11872], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_11872: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11890: 19365888 Ban)
Operation_587: operator = TypecastForward, inputs = [Tensor_11862], outputs = [Tensor_11874], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_11874: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11874: 19365888 Ban)
Operation_589: operator = TypecastForward, inputs = [Tensor_11534], outputs = [Tensor_11876], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_11876: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11876: 1179648 Ban)
Operation_591: operator = TypecastForward, inputs = [Tensor_11535], outputs = [Tensor_11878], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_11878: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11821: 1536 Bnn)
Operation_593: operator = LinearForward, inputs = [Tensor_11874, Tensor_11876, Tensor_11878], outputs = [Tensor_11880], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_11880: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11898: 19365888 Ban)
Operation_595: operator = TypecastForward, inputs = [Tensor_11862], outputs = [Tensor_11882], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_11882: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11882: 19365888 Ban)
Operation_597: operator = TypecastForward, inputs = [Tensor_11536], outputs = [Tensor_11884], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_11884: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11884: 1179648 Ban)
Operation_599: operator = TypecastForward, inputs = [Tensor_11537], outputs = [Tensor_11886], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_11886: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11821: 1536 Bnn)
Operation_601: operator = LinearForward, inputs = [Tensor_11882, Tensor_11884, Tensor_11886], outputs = [Tensor_11888], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_11888: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11902: 19365888 Ban)
Operation_603: operator = ElemwiseUnaryForward, inputs = [Tensor_11872], outputs = [Tensor_11890], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 0.125]
Tensor_11890: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11890: 19365888 Bnn)
Operation_607: operator = ReshapeForward, inputs = [Tensor_11890], outputs = [Tensor_11892], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 64 stride: 151296 stride: 64 stride: 768 stride: 1]
Tensor_11892: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11890: 19365888 Bnn)
Operation_617: operator = ContiguousForward, inputs = [Tensor_11892], outputs = [Tensor_11894], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_11894: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11896: 19365888 Ban)
Operation_619: operator = ReshapeForward, inputs = [Tensor_11894], outputs = [Tensor_11896], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_11896: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11896: 19365888 Bnn)
Operation_611: operator = ReshapeForward, inputs = [Tensor_11880], outputs = [Tensor_11898], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 64 stride: 151296 stride: 64 stride: 768 stride: 1]
Tensor_11898: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11898: 19365888 Bnn)
Operation_621: operator = ContiguousForward, inputs = [Tensor_11898], outputs = [Tensor_11900], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_11900: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11890: 19365888 Bnn)
Operation_615: operator = ReshapeForward, inputs = [Tensor_11888], outputs = [Tensor_11902], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 64 stride: 151296 stride: 64 stride: 768 stride: 1]
Tensor_11902: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11902: 19365888 Bnn)
Operation_625: operator = ContiguousForward, inputs = [Tensor_11902], outputs = [Tensor_11904], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_11904: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11898: 19365888 Bnn)
Operation_627: operator = ReshapeForward, inputs = [Tensor_11904], outputs = [Tensor_11906], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_11906: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11898: 19365888 Bnn)
Operation_629: operator = ReshapeForward, inputs = [Tensor_11900], outputs = [Tensor_11908], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 64 shape: 197 stride: 12608 stride: 1 stride: 64]
Tensor_11908: shape = [1536, 64, 197], stride = [12608, 1, 64], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11890: 19365888 Bnn)
Operation_631: operator = BatchedMatmulForward, inputs = [Tensor_11896, Tensor_11908], outputs = [Tensor_11910], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_11910: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11914: 59610624 Ban)
Operation_639: operator = ReshapeForward, inputs = [Tensor_11838], outputs = [Tensor_11912], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 197 stride: 197 stride: 0 stride: 0 stride: 1]
Tensor_11912: shape = [128, 12, 197, 197], stride = [197, 0, 0, 1], offset = 0, size = 25216, datatype = boolean, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11852: 12608 Bnn)
Operation_633: operator = ReshapeForward, inputs = [Tensor_11910], outputs = [Tensor_11914], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 197 stride: 465708 stride: 38809 stride: 197 stride: 1]
Tensor_11914: shape = [128, 12, 197, 197], stride = [465708, 38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11914: 59610624 Bnn)
Operation_641: operator = MaskedFillForward, inputs = [Tensor_11914, Tensor_11912], outputs = [Tensor_11916], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [value_float: -inf]
Tensor_11916: shape = [128, 12, 197, 197], stride = [465708, 38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11914: 59610624 Bnn)
Operation_643: operator = ReshapeForward, inputs = [Tensor_11916], outputs = [Tensor_11918], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 197 stride: 38809 stride: 197 stride: 1]
Tensor_11918: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11914: 59610624 Bnn)
Operation_645: operator = TypecastForward, inputs = [Tensor_11918], outputs = [Tensor_11920], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_11920: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11920: 119221248 Ban)
Operation_647: operator = SoftmaxForward, inputs = [Tensor_11920], outputs = [Tensor_11922], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dim: 2]
Tensor_11922: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11922: 119221248 Ban)
Operation_649: operator = TypecastForward, inputs = [Tensor_11922], outputs = [Tensor_11924], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_11924: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11914: 59610624 Bnn)
Operation_651: operator = BatchedMatmulForward, inputs = [Tensor_11924, Tensor_11906], outputs = [Tensor_11926], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_11926: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11902: 19365888 Bnn)
Operation_7437: operator = AllGatherDuplicated, inputs = [Tensor_11926], outputs = [Tensor_11929], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = all->this->653
  - Operand: [tag: 7436]
Operation_11928: operator = Consume, inputs = [Tensor_11926], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_11929: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11936: 38731776 Ban)
Operation_669: operator = TypecastForward, inputs = [Tensor_11540], outputs = [Tensor_11951], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Operation_671: operator = TypecastForward, inputs = [Tensor_11541], outputs = [Tensor_11952], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Operation_679: operator = ElemwiseUnaryForward, inputs = [Tensor_11860], outputs = [Tensor_11955], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 1]
Operation_693: operator = TypecastForward, inputs = [Tensor_11546], outputs = [Tensor_11968], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Operation_695: operator = TypecastForward, inputs = [Tensor_11547], outputs = [Tensor_11969], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Operation_653: operator = ReshapeForward, inputs = [Tensor_11929], outputs = [Tensor_11936], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = 7437->this
  - Operand: [shape: 197 shape: 1536 shape: 64 stride: 64 stride: 12608 stride: 1]
Tensor_11936: shape = [197, 1536, 64], stride = [64, 12608, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11936: 38731776 Bnn)
Operation_655: operator = ContiguousForward, inputs = [Tensor_11936], outputs = [Tensor_11938], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_11938: shape = [197, 1536, 64], stride = [98304, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11940: 38731776 Ban)
Operation_659: operator = ReshapeForward, inputs = [Tensor_11938], outputs = [Tensor_11940], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 768 stride: 98304 stride: 1]
Tensor_11940: shape = [128, 197, 768], stride = [768, 98304, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11940: 38731776 Bnn)
Operation_661: operator = TypecastForward, inputs = [Tensor_11940], outputs = [Tensor_11942], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_11942: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11942: 77463552 Ban)
Operation_7447: operator = DistributeWithMemAllocForward, inputs = [Tensor_11942], outputs = [Tensor_11944], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_11944: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11936: 38731776 Bnn)
Operation_663: operator = LayernormForward, inputs = [Tensor_11944, Tensor_11542, Tensor_11543], outputs = [Tensor_11946, Tensor_11947, Tensor_11948], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_11946: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11940: 38731776 Bnn)
Tensor_11947: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11947: 50432 Ban)
Tensor_11948: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11948: 50432 Ban)
Operation_667: operator = TypecastForward, inputs = [Tensor_11946], outputs = [Tensor_11950], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_11950: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11902: 19365888 Bnn)
Tensor_11951: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11951: 1179648 Ban)
Tensor_11952: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11821: 1536 Bnn)
Operation_673: operator = LinearForward, inputs = [Tensor_11950, Tensor_11951, Tensor_11952], outputs = [Tensor_11954], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_11954: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11954: 19365888 Ban)
Tensor_11955: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11959: 38731776 Ban)
Operation_681: operator = TypecastForward, inputs = [Tensor_11954], outputs = [Tensor_11957], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_11957: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11936: 38731776 Bnn)
Operation_683: operator = ElemwiseBinaryForward, inputs = [Tensor_11955, Tensor_11957], outputs = [Tensor_11959], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_11959: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11959: 38731776 Bnn)
Operation_685: operator = LayernormForward, inputs = [Tensor_11959, Tensor_11552, Tensor_11553], outputs = [Tensor_11961, Tensor_11962, Tensor_11963], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_11961: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11936: 38731776 Bnn)
Tensor_11962: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11962: 50432 Ban)
Tensor_11963: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11963: 50432 Ban)
Operation_689: operator = ReshapeForward, inputs = [Tensor_11961], outputs = [Tensor_11965], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 25216 shape: 768 stride: 768 stride: 1]
Tensor_11965: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11936: 38731776 Bnn)
Operation_691: operator = TypecastForward, inputs = [Tensor_11965], outputs = [Tensor_11967], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_11967: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11954: 19365888 Bnn)
Tensor_11968: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11968: 4718592 Ban)
Tensor_11969: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11969: 6144 Ban)
Operation_697: operator = LinearForward, inputs = [Tensor_11967, Tensor_11968, Tensor_11969], outputs = [Tensor_11971], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_11971: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11971: 77463552 Ban)
Operation_699: operator = TypecastForward, inputs = [Tensor_11971], outputs = [Tensor_11973], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_11973: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11973: 154927104 Ban)
Operation_701: operator = ElemwiseUnaryForward, inputs = [Tensor_11973], outputs = [Tensor_11975], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_GELU]
Tensor_11975: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11975: 154927104 Ban)
Operation_703: operator = TypecastForward, inputs = [Tensor_11975], outputs = [Tensor_11977], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_11977: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11971: 77463552 Bnn)
Operation_705: operator = TypecastForward, inputs = [Tensor_11977], outputs = [Tensor_11979], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_11979: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11979: 154927104 Ban)
Operation_707: operator = LayernormForward, inputs = [Tensor_11979, Tensor_11550, Tensor_11551], outputs = [Tensor_11981, Tensor_11982, Tensor_11983], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_11981: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11981: 154927104 Ban)
Tensor_11982: shape = [25216], stride = [1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11982: 50432 Ban)
Tensor_11983: shape = [25216], stride = [1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11983: 50432 Ban)
Operation_711: operator = TypecastForward, inputs = [Tensor_11981], outputs = [Tensor_11985], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_11985: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11971: 77463552 Bnn)
Operation_713: operator = TypecastForward, inputs = [Tensor_11548], outputs = [Tensor_11987], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_11987: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11987: 4718592 Ban)
Operation_715: operator = TypecastForward, inputs = [Tensor_11549], outputs = [Tensor_11989], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_11989: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11821: 1536 Bnn)
Operation_717: operator = LinearForward, inputs = [Tensor_11985, Tensor_11987, Tensor_11989], outputs = [Tensor_11991], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_11991: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11995: 19365888 Ban)
Operation_721: operator = ElemwiseUnaryForward, inputs = [Tensor_11959], outputs = [Tensor_11993], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 1]
Tensor_11993: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11999: 38731776 Ban)
Operation_719: operator = ReshapeForward, inputs = [Tensor_11991], outputs = [Tensor_11995], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 151296 stride: 768 stride: 1]
Tensor_11995: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11995: 19365888 Bnn)
Operation_723: operator = TypecastForward, inputs = [Tensor_11995], outputs = [Tensor_11997], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_11997: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11997: 38731776 Ban)
Operation_725: operator = ElemwiseBinaryForward, inputs = [Tensor_11993, Tensor_11997], outputs = [Tensor_11999], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_11999: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11999: 38731776 Bnn)
Operation_727: operator = LayernormForward, inputs = [Tensor_11999, Tensor_11564, Tensor_11565], outputs = [Tensor_12001, Tensor_12002, Tensor_12003], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_12001: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11997: 38731776 Bnn)
Tensor_12002: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12002: 50432 Ban)
Tensor_12003: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12003: 50432 Ban)
Operation_731: operator = TypecastForward, inputs = [Tensor_12001], outputs = [Tensor_12005], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_12005: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11995: 19365888 Bnn)
Operation_733: operator = TypecastForward, inputs = [Tensor_11558], outputs = [Tensor_12007], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_12007: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12007: 1179648 Ban)
Operation_735: operator = TypecastForward, inputs = [Tensor_11559], outputs = [Tensor_12009], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_12009: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11821: 1536 Bnn)
Operation_737: operator = LinearForward, inputs = [Tensor_12005, Tensor_12007, Tensor_12009], outputs = [Tensor_12011], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_12011: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12029: 19365888 Ban)
Operation_739: operator = TypecastForward, inputs = [Tensor_12001], outputs = [Tensor_12013], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_12013: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12013: 19365888 Ban)
Operation_741: operator = TypecastForward, inputs = [Tensor_11554], outputs = [Tensor_12015], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_12015: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12015: 1179648 Ban)
Operation_743: operator = TypecastForward, inputs = [Tensor_11555], outputs = [Tensor_12017], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_12017: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11821: 1536 Bnn)
Operation_745: operator = LinearForward, inputs = [Tensor_12013, Tensor_12015, Tensor_12017], outputs = [Tensor_12019], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_12019: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12037: 19365888 Ban)
Operation_747: operator = TypecastForward, inputs = [Tensor_12001], outputs = [Tensor_12021], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_12021: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12021: 19365888 Ban)
Operation_749: operator = TypecastForward, inputs = [Tensor_11556], outputs = [Tensor_12023], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_12023: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12023: 1179648 Ban)
Operation_751: operator = TypecastForward, inputs = [Tensor_11557], outputs = [Tensor_12025], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_12025: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11821: 1536 Bnn)
Operation_753: operator = LinearForward, inputs = [Tensor_12021, Tensor_12023, Tensor_12025], outputs = [Tensor_12027], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_12027: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12041: 19365888 Ban)
Operation_755: operator = ElemwiseUnaryForward, inputs = [Tensor_12011], outputs = [Tensor_12029], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 0.125]
Tensor_12029: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12029: 19365888 Bnn)
Operation_759: operator = ReshapeForward, inputs = [Tensor_12029], outputs = [Tensor_12031], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 64 stride: 151296 stride: 64 stride: 768 stride: 1]
Tensor_12031: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12029: 19365888 Bnn)
Operation_769: operator = ContiguousForward, inputs = [Tensor_12031], outputs = [Tensor_12033], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_12033: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12035: 19365888 Ban)
Operation_771: operator = ReshapeForward, inputs = [Tensor_12033], outputs = [Tensor_12035], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_12035: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12035: 19365888 Bnn)
Operation_763: operator = ReshapeForward, inputs = [Tensor_12019], outputs = [Tensor_12037], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 64 stride: 151296 stride: 64 stride: 768 stride: 1]
Tensor_12037: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12037: 19365888 Bnn)
Operation_773: operator = ContiguousForward, inputs = [Tensor_12037], outputs = [Tensor_12039], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_12039: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12029: 19365888 Bnn)
Operation_767: operator = ReshapeForward, inputs = [Tensor_12027], outputs = [Tensor_12041], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 64 stride: 151296 stride: 64 stride: 768 stride: 1]
Tensor_12041: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12041: 19365888 Bnn)
Operation_777: operator = ContiguousForward, inputs = [Tensor_12041], outputs = [Tensor_12043], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_12043: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12037: 19365888 Bnn)
Operation_779: operator = ReshapeForward, inputs = [Tensor_12043], outputs = [Tensor_12045], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_12045: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12037: 19365888 Bnn)
Operation_781: operator = ReshapeForward, inputs = [Tensor_12039], outputs = [Tensor_12047], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 64 shape: 197 stride: 12608 stride: 1 stride: 64]
Tensor_12047: shape = [1536, 64, 197], stride = [12608, 1, 64], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12029: 19365888 Bnn)
Operation_783: operator = BatchedMatmulForward, inputs = [Tensor_12035, Tensor_12047], outputs = [Tensor_12049], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_12049: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12051: 59610624 Ban)
Operation_785: operator = ReshapeForward, inputs = [Tensor_12049], outputs = [Tensor_12051], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 197 stride: 465708 stride: 38809 stride: 197 stride: 1]
Tensor_12051: shape = [128, 12, 197, 197], stride = [465708, 38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12051: 59610624 Bnn)
Operation_793: operator = MaskedFillForward, inputs = [Tensor_12051, Tensor_11912], outputs = [Tensor_12053], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [value_float: -inf]
Tensor_12053: shape = [128, 12, 197, 197], stride = [465708, 38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12051: 59610624 Bnn)
Operation_795: operator = ReshapeForward, inputs = [Tensor_12053], outputs = [Tensor_12055], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 197 stride: 38809 stride: 197 stride: 1]
Tensor_12055: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12051: 59610624 Bnn)
Operation_797: operator = TypecastForward, inputs = [Tensor_12055], outputs = [Tensor_12057], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_12057: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11920: 119221248 Bnn)
Operation_799: operator = SoftmaxForward, inputs = [Tensor_12057], outputs = [Tensor_12059], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dim: 2]
Tensor_12059: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12059: 119221248 Ban)
Operation_801: operator = TypecastForward, inputs = [Tensor_12059], outputs = [Tensor_12061], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_12061: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12051: 59610624 Bnn)
Operation_803: operator = BatchedMatmulForward, inputs = [Tensor_12061, Tensor_12045], outputs = [Tensor_12063], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_12063: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12041: 19365888 Bnn)
Operation_7574: operator = AllGatherDuplicated, inputs = [Tensor_12063], outputs = [Tensor_12066], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = all->this->805
  - Operand: [tag: 7573]
Operation_12065: operator = Consume, inputs = [Tensor_12063], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_12066: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12073: 38731776 Ban)
Operation_821: operator = TypecastForward, inputs = [Tensor_11560], outputs = [Tensor_12088], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Operation_823: operator = TypecastForward, inputs = [Tensor_11561], outputs = [Tensor_12089], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Operation_842: operator = ElemwiseUnaryForward, inputs = [Tensor_11999], outputs = [Tensor_12103], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 1]
Operation_856: operator = TypecastForward, inputs = [Tensor_11566], outputs = [Tensor_12116], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Operation_858: operator = TypecastForward, inputs = [Tensor_11567], outputs = [Tensor_12117], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Operation_805: operator = ReshapeForward, inputs = [Tensor_12066], outputs = [Tensor_12073], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = 7574->this
  - Operand: [shape: 197 shape: 1536 shape: 64 stride: 64 stride: 12608 stride: 1]
Tensor_12073: shape = [197, 1536, 64], stride = [64, 12608, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12073: 38731776 Bnn)
Operation_807: operator = ContiguousForward, inputs = [Tensor_12073], outputs = [Tensor_12075], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_12075: shape = [197, 1536, 64], stride = [98304, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12077: 38731776 Ban)
Operation_811: operator = ReshapeForward, inputs = [Tensor_12075], outputs = [Tensor_12077], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 768 stride: 98304 stride: 1]
Tensor_12077: shape = [128, 197, 768], stride = [768, 98304, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12077: 38731776 Bnn)
Operation_813: operator = TypecastForward, inputs = [Tensor_12077], outputs = [Tensor_12079], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_12079: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12079: 77463552 Ban)
Operation_7584: operator = DistributeWithMemAllocForward, inputs = [Tensor_12079], outputs = [Tensor_12081], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_12081: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12073: 38731776 Bnn)
Operation_815: operator = LayernormForward, inputs = [Tensor_12081, Tensor_11562, Tensor_11563], outputs = [Tensor_12083, Tensor_12084, Tensor_12085], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_12083: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12077: 38731776 Bnn)
Tensor_12084: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12084: 50432 Ban)
Tensor_12085: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12085: 50432 Ban)
Operation_819: operator = TypecastForward, inputs = [Tensor_12083], outputs = [Tensor_12087], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_12087: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12041: 19365888 Bnn)
Tensor_12088: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12088: 1179648 Ban)
Tensor_12089: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11821: 1536 Bnn)
Operation_825: operator = LinearForward, inputs = [Tensor_12087, Tensor_12088, Tensor_12089], outputs = [Tensor_12091], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_12091: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12098: 19365888 Ban)
Tensor_12092: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12094: 128 Ban)
Operation_832: operator = ElemwiseUnaryForward, inputs = [Tensor_12092], outputs = [Tensor_12094], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_ADD scal1_float: 0.986363649]
Tensor_12094: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12094: 128 Bnn)
Operation_834: operator = ElemwiseUnaryForward, inputs = [Tensor_12094], outputs = [Tensor_12096], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_FLOOR]
Tensor_12096: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12094: 128 Bnn)
Operation_836: operator = ElemwiseUnaryForward, inputs = [Tensor_12091], outputs = [Tensor_12098], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_DIV scal1_float: 0.986363649]
Tensor_12098: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12098: 19365888 Bnn)
Operation_838: operator = ReshapeForward, inputs = [Tensor_12096], outputs = [Tensor_12100], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 1 stride: 0 stride: 0]
Tensor_12100: shape = [128, 197, 768], stride = [1, 0, 0], offset = 0, size = 128, datatype = float16, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12094: 128 Bnn)
Operation_840: operator = ElemwiseBinaryForward, inputs = [Tensor_12098, Tensor_12100], outputs = [Tensor_12102], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_MUL]
Tensor_12102: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12102: 19365888 Ban)
Tensor_12103: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12107: 38731776 Ban)
Operation_844: operator = TypecastForward, inputs = [Tensor_12102], outputs = [Tensor_12105], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_12105: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12073: 38731776 Bnn)
Operation_846: operator = ElemwiseBinaryForward, inputs = [Tensor_12103, Tensor_12105], outputs = [Tensor_12107], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_12107: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12107: 38731776 Bnn)
Operation_848: operator = LayernormForward, inputs = [Tensor_12107, Tensor_11572, Tensor_11573], outputs = [Tensor_12109, Tensor_12110, Tensor_12111], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_12109: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12073: 38731776 Bnn)
Tensor_12110: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12110: 50432 Ban)
Tensor_12111: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12111: 50432 Ban)
Operation_852: operator = ReshapeForward, inputs = [Tensor_12109], outputs = [Tensor_12113], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 25216 shape: 768 stride: 768 stride: 1]
Tensor_12113: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12073: 38731776 Bnn)
Operation_854: operator = TypecastForward, inputs = [Tensor_12113], outputs = [Tensor_12115], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_12115: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12102: 19365888 Bnn)
Tensor_12116: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12116: 4718592 Ban)
Tensor_12117: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11969: 6144 Bnn)
Operation_860: operator = LinearForward, inputs = [Tensor_12115, Tensor_12116, Tensor_12117], outputs = [Tensor_12119], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_12119: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12119: 77463552 Ban)
Operation_862: operator = TypecastForward, inputs = [Tensor_12119], outputs = [Tensor_12121], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_12121: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12121: 154927104 Ban)
Operation_864: operator = ElemwiseUnaryForward, inputs = [Tensor_12121], outputs = [Tensor_12123], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_GELU]
Tensor_12123: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12123: 154927104 Ban)
Operation_866: operator = TypecastForward, inputs = [Tensor_12123], outputs = [Tensor_12125], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_12125: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12119: 77463552 Bnn)
Operation_868: operator = TypecastForward, inputs = [Tensor_12125], outputs = [Tensor_12127], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_12127: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12127: 154927104 Ban)
Operation_870: operator = LayernormForward, inputs = [Tensor_12127, Tensor_11570, Tensor_11571], outputs = [Tensor_12129, Tensor_12130, Tensor_12131], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_12129: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12129: 154927104 Ban)
Tensor_12130: shape = [25216], stride = [1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12130: 50432 Ban)
Tensor_12131: shape = [25216], stride = [1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12131: 50432 Ban)
Operation_874: operator = TypecastForward, inputs = [Tensor_12129], outputs = [Tensor_12133], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_12133: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12119: 77463552 Bnn)
Operation_876: operator = TypecastForward, inputs = [Tensor_11568], outputs = [Tensor_12135], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_12135: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12135: 4718592 Ban)
Operation_878: operator = TypecastForward, inputs = [Tensor_11569], outputs = [Tensor_12137], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_12137: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11821: 1536 Bnn)
Operation_880: operator = LinearForward, inputs = [Tensor_12133, Tensor_12135, Tensor_12137], outputs = [Tensor_12139], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_12139: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12146: 19365888 Ban)
Tensor_12140: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12142: 128 Ban)
Operation_885: operator = ElemwiseUnaryForward, inputs = [Tensor_12140], outputs = [Tensor_12142], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_ADD scal1_float: 0.986363649]
Tensor_12142: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12142: 128 Bnn)
Operation_887: operator = ElemwiseUnaryForward, inputs = [Tensor_12142], outputs = [Tensor_12144], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_FLOOR]
Tensor_12144: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12142: 128 Bnn)
Operation_882: operator = ReshapeForward, inputs = [Tensor_12139], outputs = [Tensor_12146], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 151296 stride: 768 stride: 1]
Tensor_12146: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12146: 19365888 Bnn)
Operation_889: operator = ElemwiseUnaryForward, inputs = [Tensor_12146], outputs = [Tensor_12148], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_DIV scal1_float: 0.986363649]
Tensor_12148: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12146: 19365888 Bnn)
Operation_891: operator = ReshapeForward, inputs = [Tensor_12144], outputs = [Tensor_12150], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 1 stride: 0 stride: 0]
Tensor_12150: shape = [128, 197, 768], stride = [1, 0, 0], offset = 0, size = 128, datatype = float16, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12142: 128 Bnn)
Operation_893: operator = ElemwiseBinaryForward, inputs = [Tensor_12148, Tensor_12150], outputs = [Tensor_12152], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_MUL]
Tensor_12152: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12152: 19365888 Ban)
Operation_895: operator = ElemwiseUnaryForward, inputs = [Tensor_12107], outputs = [Tensor_12154], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 1]
Tensor_12154: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12158: 38731776 Ban)
Operation_897: operator = TypecastForward, inputs = [Tensor_12152], outputs = [Tensor_12156], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_12156: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12156: 38731776 Ban)
Operation_899: operator = ElemwiseBinaryForward, inputs = [Tensor_12154, Tensor_12156], outputs = [Tensor_12158], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_12158: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12158: 38731776 Bnn)
Operation_901: operator = LayernormForward, inputs = [Tensor_12158, Tensor_11584, Tensor_11585], outputs = [Tensor_12160, Tensor_12161, Tensor_12162], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_12160: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12156: 38731776 Bnn)
Tensor_12161: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12161: 50432 Ban)
Tensor_12162: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12162: 50432 Ban)
Operation_905: operator = TypecastForward, inputs = [Tensor_12160], outputs = [Tensor_12164], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_12164: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12152: 19365888 Bnn)
Operation_907: operator = TypecastForward, inputs = [Tensor_11578], outputs = [Tensor_12166], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_12166: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12166: 1179648 Ban)
Operation_909: operator = TypecastForward, inputs = [Tensor_11579], outputs = [Tensor_12168], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_12168: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11821: 1536 Bnn)
Operation_911: operator = LinearForward, inputs = [Tensor_12164, Tensor_12166, Tensor_12168], outputs = [Tensor_12170], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_12170: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12188: 19365888 Ban)
Operation_913: operator = TypecastForward, inputs = [Tensor_12160], outputs = [Tensor_12172], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_12172: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12172: 19365888 Ban)
Operation_915: operator = TypecastForward, inputs = [Tensor_11574], outputs = [Tensor_12174], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_12174: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12174: 1179648 Ban)
Operation_917: operator = TypecastForward, inputs = [Tensor_11575], outputs = [Tensor_12176], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_12176: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11821: 1536 Bnn)
Operation_919: operator = LinearForward, inputs = [Tensor_12172, Tensor_12174, Tensor_12176], outputs = [Tensor_12178], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_12178: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12196: 19365888 Ban)
Operation_921: operator = TypecastForward, inputs = [Tensor_12160], outputs = [Tensor_12180], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_12180: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12180: 19365888 Ban)
Operation_923: operator = TypecastForward, inputs = [Tensor_11576], outputs = [Tensor_12182], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_12182: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12182: 1179648 Ban)
Operation_925: operator = TypecastForward, inputs = [Tensor_11577], outputs = [Tensor_12184], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_12184: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11821: 1536 Bnn)
Operation_927: operator = LinearForward, inputs = [Tensor_12180, Tensor_12182, Tensor_12184], outputs = [Tensor_12186], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_12186: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12200: 19365888 Ban)
Operation_929: operator = ElemwiseUnaryForward, inputs = [Tensor_12170], outputs = [Tensor_12188], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 0.125]
Tensor_12188: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12188: 19365888 Bnn)
Operation_933: operator = ReshapeForward, inputs = [Tensor_12188], outputs = [Tensor_12190], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 64 stride: 151296 stride: 64 stride: 768 stride: 1]
Tensor_12190: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12188: 19365888 Bnn)
Operation_943: operator = ContiguousForward, inputs = [Tensor_12190], outputs = [Tensor_12192], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_12192: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12194: 19365888 Ban)
Operation_945: operator = ReshapeForward, inputs = [Tensor_12192], outputs = [Tensor_12194], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_12194: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12194: 19365888 Bnn)
Operation_937: operator = ReshapeForward, inputs = [Tensor_12178], outputs = [Tensor_12196], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 64 stride: 151296 stride: 64 stride: 768 stride: 1]
Tensor_12196: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12196: 19365888 Bnn)
Operation_947: operator = ContiguousForward, inputs = [Tensor_12196], outputs = [Tensor_12198], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_12198: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12188: 19365888 Bnn)
Operation_941: operator = ReshapeForward, inputs = [Tensor_12186], outputs = [Tensor_12200], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 64 stride: 151296 stride: 64 stride: 768 stride: 1]
Tensor_12200: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12200: 19365888 Bnn)
Operation_951: operator = ContiguousForward, inputs = [Tensor_12200], outputs = [Tensor_12202], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_12202: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12196: 19365888 Bnn)
Operation_953: operator = ReshapeForward, inputs = [Tensor_12202], outputs = [Tensor_12204], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_12204: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12196: 19365888 Bnn)
Operation_955: operator = ReshapeForward, inputs = [Tensor_12198], outputs = [Tensor_12206], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 64 shape: 197 stride: 12608 stride: 1 stride: 64]
Tensor_12206: shape = [1536, 64, 197], stride = [12608, 1, 64], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12188: 19365888 Bnn)
Operation_957: operator = BatchedMatmulForward, inputs = [Tensor_12194, Tensor_12206], outputs = [Tensor_12208], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_12208: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12210: 59610624 Ban)
Operation_959: operator = ReshapeForward, inputs = [Tensor_12208], outputs = [Tensor_12210], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 197 stride: 465708 stride: 38809 stride: 197 stride: 1]
Tensor_12210: shape = [128, 12, 197, 197], stride = [465708, 38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12210: 59610624 Bnn)
Operation_967: operator = MaskedFillForward, inputs = [Tensor_12210, Tensor_11912], outputs = [Tensor_12212], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [value_float: -inf]
Tensor_12212: shape = [128, 12, 197, 197], stride = [465708, 38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12210: 59610624 Bnn)
Operation_969: operator = ReshapeForward, inputs = [Tensor_12212], outputs = [Tensor_12214], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 197 stride: 38809 stride: 197 stride: 1]
Tensor_12214: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12210: 59610624 Bnn)
Operation_971: operator = TypecastForward, inputs = [Tensor_12214], outputs = [Tensor_12216], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_12216: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11920: 119221248 Bnn)
Operation_973: operator = SoftmaxForward, inputs = [Tensor_12216], outputs = [Tensor_12218], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dim: 2]
Tensor_12218: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12218: 119221248 Ban)
Operation_975: operator = TypecastForward, inputs = [Tensor_12218], outputs = [Tensor_12220], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_12220: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12210: 59610624 Bnn)
Operation_977: operator = BatchedMatmulForward, inputs = [Tensor_12220, Tensor_12204], outputs = [Tensor_12222], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_12222: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12200: 19365888 Bnn)
Operation_7733: operator = AllGatherDuplicated, inputs = [Tensor_12222], outputs = [Tensor_12225], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = all->this->979
  - Operand: [tag: 7732]
Operation_12224: operator = Consume, inputs = [Tensor_12222], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_12225: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12232: 38731776 Ban)
Operation_995: operator = TypecastForward, inputs = [Tensor_11580], outputs = [Tensor_12247], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Operation_997: operator = TypecastForward, inputs = [Tensor_11581], outputs = [Tensor_12248], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Operation_1016: operator = ElemwiseUnaryForward, inputs = [Tensor_12158], outputs = [Tensor_12262], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 1]
Operation_1030: operator = TypecastForward, inputs = [Tensor_11586], outputs = [Tensor_12275], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Operation_1032: operator = TypecastForward, inputs = [Tensor_11587], outputs = [Tensor_12276], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Operation_979: operator = ReshapeForward, inputs = [Tensor_12225], outputs = [Tensor_12232], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = 7733->this
  - Operand: [shape: 197 shape: 1536 shape: 64 stride: 64 stride: 12608 stride: 1]
Tensor_12232: shape = [197, 1536, 64], stride = [64, 12608, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12232: 38731776 Bnn)
Operation_981: operator = ContiguousForward, inputs = [Tensor_12232], outputs = [Tensor_12234], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_12234: shape = [197, 1536, 64], stride = [98304, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12236: 38731776 Ban)
Operation_985: operator = ReshapeForward, inputs = [Tensor_12234], outputs = [Tensor_12236], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 768 stride: 98304 stride: 1]
Tensor_12236: shape = [128, 197, 768], stride = [768, 98304, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12236: 38731776 Bnn)
Operation_987: operator = TypecastForward, inputs = [Tensor_12236], outputs = [Tensor_12238], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_12238: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12238: 77463552 Ban)
Operation_7743: operator = DistributeWithMemAllocForward, inputs = [Tensor_12238], outputs = [Tensor_12240], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_12240: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12232: 38731776 Bnn)
Operation_989: operator = LayernormForward, inputs = [Tensor_12240, Tensor_11582, Tensor_11583], outputs = [Tensor_12242, Tensor_12243, Tensor_12244], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_12242: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12236: 38731776 Bnn)
Tensor_12243: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12243: 50432 Ban)
Tensor_12244: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12244: 50432 Ban)
Operation_993: operator = TypecastForward, inputs = [Tensor_12242], outputs = [Tensor_12246], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_12246: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12200: 19365888 Bnn)
Tensor_12247: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12247: 1179648 Ban)
Tensor_12248: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11821: 1536 Bnn)
Operation_999: operator = LinearForward, inputs = [Tensor_12246, Tensor_12247, Tensor_12248], outputs = [Tensor_12250], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_12250: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12257: 19365888 Ban)
Tensor_12251: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12253: 128 Ban)
Operation_1006: operator = ElemwiseUnaryForward, inputs = [Tensor_12251], outputs = [Tensor_12253], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_ADD scal1_float: 0.972727299]
Tensor_12253: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12253: 128 Bnn)
Operation_1008: operator = ElemwiseUnaryForward, inputs = [Tensor_12253], outputs = [Tensor_12255], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_FLOOR]
Tensor_12255: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12253: 128 Bnn)
Operation_1010: operator = ElemwiseUnaryForward, inputs = [Tensor_12250], outputs = [Tensor_12257], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_DIV scal1_float: 0.972727299]
Tensor_12257: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12257: 19365888 Bnn)
Operation_1012: operator = ReshapeForward, inputs = [Tensor_12255], outputs = [Tensor_12259], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 1 stride: 0 stride: 0]
Tensor_12259: shape = [128, 197, 768], stride = [1, 0, 0], offset = 0, size = 128, datatype = float16, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12253: 128 Bnn)
Operation_1014: operator = ElemwiseBinaryForward, inputs = [Tensor_12257, Tensor_12259], outputs = [Tensor_12261], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_MUL]
Tensor_12261: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12261: 19365888 Ban)
Tensor_12262: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12266: 38731776 Ban)
Operation_1018: operator = TypecastForward, inputs = [Tensor_12261], outputs = [Tensor_12264], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_12264: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12232: 38731776 Bnn)
Operation_1020: operator = ElemwiseBinaryForward, inputs = [Tensor_12262, Tensor_12264], outputs = [Tensor_12266], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_12266: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12266: 38731776 Bnn)
Operation_1022: operator = LayernormForward, inputs = [Tensor_12266, Tensor_11592, Tensor_11593], outputs = [Tensor_12268, Tensor_12269, Tensor_12270], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_12268: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12232: 38731776 Bnn)
Tensor_12269: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12269: 50432 Ban)
Tensor_12270: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12270: 50432 Ban)
Operation_1026: operator = ReshapeForward, inputs = [Tensor_12268], outputs = [Tensor_12272], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 25216 shape: 768 stride: 768 stride: 1]
Tensor_12272: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12232: 38731776 Bnn)
Operation_1028: operator = TypecastForward, inputs = [Tensor_12272], outputs = [Tensor_12274], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_12274: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12261: 19365888 Bnn)
Tensor_12275: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12275: 4718592 Ban)
Tensor_12276: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11969: 6144 Bnn)
Operation_1034: operator = LinearForward, inputs = [Tensor_12274, Tensor_12275, Tensor_12276], outputs = [Tensor_12278], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_12278: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12278: 77463552 Ban)
Operation_1036: operator = TypecastForward, inputs = [Tensor_12278], outputs = [Tensor_12280], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_12280: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12280: 154927104 Ban)
Operation_1038: operator = ElemwiseUnaryForward, inputs = [Tensor_12280], outputs = [Tensor_12282], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_GELU]
Tensor_12282: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12282: 154927104 Ban)
Operation_1040: operator = TypecastForward, inputs = [Tensor_12282], outputs = [Tensor_12284], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_12284: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12278: 77463552 Bnn)
Operation_1042: operator = TypecastForward, inputs = [Tensor_12284], outputs = [Tensor_12286], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_12286: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12286: 154927104 Ban)
Operation_1044: operator = LayernormForward, inputs = [Tensor_12286, Tensor_11590, Tensor_11591], outputs = [Tensor_12288, Tensor_12289, Tensor_12290], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_12288: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12288: 154927104 Ban)
Tensor_12289: shape = [25216], stride = [1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12289: 50432 Ban)
Tensor_12290: shape = [25216], stride = [1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12290: 50432 Ban)
Operation_1048: operator = TypecastForward, inputs = [Tensor_12288], outputs = [Tensor_12292], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_12292: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12278: 77463552 Bnn)
Operation_1050: operator = TypecastForward, inputs = [Tensor_11588], outputs = [Tensor_12294], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_12294: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12294: 4718592 Ban)
Operation_1052: operator = TypecastForward, inputs = [Tensor_11589], outputs = [Tensor_12296], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_12296: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11821: 1536 Bnn)
Operation_1054: operator = LinearForward, inputs = [Tensor_12292, Tensor_12294, Tensor_12296], outputs = [Tensor_12298], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_12298: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12305: 19365888 Ban)
Tensor_12299: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12301: 128 Ban)
Operation_1059: operator = ElemwiseUnaryForward, inputs = [Tensor_12299], outputs = [Tensor_12301], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_ADD scal1_float: 0.972727299]
Tensor_12301: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12301: 128 Bnn)
Operation_1061: operator = ElemwiseUnaryForward, inputs = [Tensor_12301], outputs = [Tensor_12303], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_FLOOR]
Tensor_12303: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12301: 128 Bnn)
Operation_1056: operator = ReshapeForward, inputs = [Tensor_12298], outputs = [Tensor_12305], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 151296 stride: 768 stride: 1]
Tensor_12305: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12305: 19365888 Bnn)
Operation_1063: operator = ElemwiseUnaryForward, inputs = [Tensor_12305], outputs = [Tensor_12307], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_DIV scal1_float: 0.972727299]
Tensor_12307: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12305: 19365888 Bnn)
Operation_1065: operator = ReshapeForward, inputs = [Tensor_12303], outputs = [Tensor_12309], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 1 stride: 0 stride: 0]
Tensor_12309: shape = [128, 197, 768], stride = [1, 0, 0], offset = 0, size = 128, datatype = float16, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12301: 128 Bnn)
Operation_1067: operator = ElemwiseBinaryForward, inputs = [Tensor_12307, Tensor_12309], outputs = [Tensor_12311], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_MUL]
Tensor_12311: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12311: 19365888 Ban)
Operation_1069: operator = ElemwiseUnaryForward, inputs = [Tensor_12266], outputs = [Tensor_12313], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 1]
Tensor_12313: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12317: 38731776 Ban)
Operation_1071: operator = TypecastForward, inputs = [Tensor_12311], outputs = [Tensor_12315], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_12315: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12315: 38731776 Ban)
Operation_1073: operator = ElemwiseBinaryForward, inputs = [Tensor_12313, Tensor_12315], outputs = [Tensor_12317], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_12317: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12317: 38731776 Bnn)
Operation_1075: operator = LayernormForward, inputs = [Tensor_12317, Tensor_11604, Tensor_11605], outputs = [Tensor_12319, Tensor_12320, Tensor_12321], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_12319: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12315: 38731776 Bnn)
Tensor_12320: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12320: 50432 Ban)
Tensor_12321: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12321: 50432 Ban)
Operation_1079: operator = TypecastForward, inputs = [Tensor_12319], outputs = [Tensor_12323], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_12323: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12311: 19365888 Bnn)
Operation_1081: operator = TypecastForward, inputs = [Tensor_11598], outputs = [Tensor_12325], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_12325: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12325: 1179648 Ban)
Operation_1083: operator = TypecastForward, inputs = [Tensor_11599], outputs = [Tensor_12327], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_12327: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11821: 1536 Bnn)
Operation_1085: operator = LinearForward, inputs = [Tensor_12323, Tensor_12325, Tensor_12327], outputs = [Tensor_12329], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_12329: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12347: 19365888 Ban)
Operation_1087: operator = TypecastForward, inputs = [Tensor_12319], outputs = [Tensor_12331], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_12331: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12331: 19365888 Ban)
Operation_1089: operator = TypecastForward, inputs = [Tensor_11594], outputs = [Tensor_12333], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_12333: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12333: 1179648 Ban)
Operation_1091: operator = TypecastForward, inputs = [Tensor_11595], outputs = [Tensor_12335], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_12335: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11821: 1536 Bnn)
Operation_1093: operator = LinearForward, inputs = [Tensor_12331, Tensor_12333, Tensor_12335], outputs = [Tensor_12337], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_12337: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12355: 19365888 Ban)
Operation_1095: operator = TypecastForward, inputs = [Tensor_12319], outputs = [Tensor_12339], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_12339: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12339: 19365888 Ban)
Operation_1097: operator = TypecastForward, inputs = [Tensor_11596], outputs = [Tensor_12341], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_12341: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12341: 1179648 Ban)
Operation_1099: operator = TypecastForward, inputs = [Tensor_11597], outputs = [Tensor_12343], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_12343: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11821: 1536 Bnn)
Operation_1101: operator = LinearForward, inputs = [Tensor_12339, Tensor_12341, Tensor_12343], outputs = [Tensor_12345], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_12345: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12359: 19365888 Ban)
Operation_1103: operator = ElemwiseUnaryForward, inputs = [Tensor_12329], outputs = [Tensor_12347], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 0.125]
Tensor_12347: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12347: 19365888 Bnn)
Operation_1107: operator = ReshapeForward, inputs = [Tensor_12347], outputs = [Tensor_12349], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 64 stride: 151296 stride: 64 stride: 768 stride: 1]
Tensor_12349: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12347: 19365888 Bnn)
Operation_1117: operator = ContiguousForward, inputs = [Tensor_12349], outputs = [Tensor_12351], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_12351: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12353: 19365888 Ban)
Operation_1119: operator = ReshapeForward, inputs = [Tensor_12351], outputs = [Tensor_12353], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_12353: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12353: 19365888 Bnn)
Operation_1111: operator = ReshapeForward, inputs = [Tensor_12337], outputs = [Tensor_12355], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 64 stride: 151296 stride: 64 stride: 768 stride: 1]
Tensor_12355: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12355: 19365888 Bnn)
Operation_1121: operator = ContiguousForward, inputs = [Tensor_12355], outputs = [Tensor_12357], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_12357: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12347: 19365888 Bnn)
Operation_1115: operator = ReshapeForward, inputs = [Tensor_12345], outputs = [Tensor_12359], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 64 stride: 151296 stride: 64 stride: 768 stride: 1]
Tensor_12359: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12359: 19365888 Bnn)
Operation_1125: operator = ContiguousForward, inputs = [Tensor_12359], outputs = [Tensor_12361], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_12361: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12355: 19365888 Bnn)
Operation_1127: operator = ReshapeForward, inputs = [Tensor_12361], outputs = [Tensor_12363], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_12363: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12355: 19365888 Bnn)
Operation_1129: operator = ReshapeForward, inputs = [Tensor_12357], outputs = [Tensor_12365], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 64 shape: 197 stride: 12608 stride: 1 stride: 64]
Tensor_12365: shape = [1536, 64, 197], stride = [12608, 1, 64], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12347: 19365888 Bnn)
Operation_1131: operator = BatchedMatmulForward, inputs = [Tensor_12353, Tensor_12365], outputs = [Tensor_12367], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_12367: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12369: 59610624 Ban)
Operation_1133: operator = ReshapeForward, inputs = [Tensor_12367], outputs = [Tensor_12369], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 197 stride: 465708 stride: 38809 stride: 197 stride: 1]
Tensor_12369: shape = [128, 12, 197, 197], stride = [465708, 38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12369: 59610624 Bnn)
Operation_1141: operator = MaskedFillForward, inputs = [Tensor_12369, Tensor_11912], outputs = [Tensor_12371], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [value_float: -inf]
Tensor_12371: shape = [128, 12, 197, 197], stride = [465708, 38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12369: 59610624 Bnn)
Operation_1143: operator = ReshapeForward, inputs = [Tensor_12371], outputs = [Tensor_12373], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 197 stride: 38809 stride: 197 stride: 1]
Tensor_12373: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12369: 59610624 Bnn)
Operation_1145: operator = TypecastForward, inputs = [Tensor_12373], outputs = [Tensor_12375], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_12375: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11920: 119221248 Bnn)
Operation_1147: operator = SoftmaxForward, inputs = [Tensor_12375], outputs = [Tensor_12377], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dim: 2]
Tensor_12377: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12377: 119221248 Ban)
Operation_1149: operator = TypecastForward, inputs = [Tensor_12377], outputs = [Tensor_12379], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_12379: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12369: 59610624 Bnn)
Operation_1151: operator = BatchedMatmulForward, inputs = [Tensor_12379, Tensor_12363], outputs = [Tensor_12381], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_12381: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12359: 19365888 Bnn)
Operation_7892: operator = AllGatherDuplicated, inputs = [Tensor_12381], outputs = [Tensor_12384], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = all->this->1153
  - Operand: [tag: 7891]
Operation_12383: operator = Consume, inputs = [Tensor_12381], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_12384: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12391: 38731776 Ban)
Operation_1169: operator = TypecastForward, inputs = [Tensor_11600], outputs = [Tensor_12406], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Operation_1171: operator = TypecastForward, inputs = [Tensor_11601], outputs = [Tensor_12407], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Operation_1190: operator = ElemwiseUnaryForward, inputs = [Tensor_12317], outputs = [Tensor_12421], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 1]
Operation_1204: operator = TypecastForward, inputs = [Tensor_11606], outputs = [Tensor_12434], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Operation_1206: operator = TypecastForward, inputs = [Tensor_11607], outputs = [Tensor_12435], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Operation_1153: operator = ReshapeForward, inputs = [Tensor_12384], outputs = [Tensor_12391], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = 7892->this
  - Operand: [shape: 197 shape: 1536 shape: 64 stride: 64 stride: 12608 stride: 1]
Tensor_12391: shape = [197, 1536, 64], stride = [64, 12608, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12391: 38731776 Bnn)
Operation_1155: operator = ContiguousForward, inputs = [Tensor_12391], outputs = [Tensor_12393], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_12393: shape = [197, 1536, 64], stride = [98304, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12395: 38731776 Ban)
Operation_1159: operator = ReshapeForward, inputs = [Tensor_12393], outputs = [Tensor_12395], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 768 stride: 98304 stride: 1]
Tensor_12395: shape = [128, 197, 768], stride = [768, 98304, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12395: 38731776 Bnn)
Operation_1161: operator = TypecastForward, inputs = [Tensor_12395], outputs = [Tensor_12397], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_12397: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12397: 77463552 Ban)
Operation_7902: operator = DistributeWithMemAllocForward, inputs = [Tensor_12397], outputs = [Tensor_12399], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_12399: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12391: 38731776 Bnn)
Operation_1163: operator = LayernormForward, inputs = [Tensor_12399, Tensor_11602, Tensor_11603], outputs = [Tensor_12401, Tensor_12402, Tensor_12403], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_12401: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12395: 38731776 Bnn)
Tensor_12402: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12402: 50432 Ban)
Tensor_12403: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12403: 50432 Ban)
Operation_1167: operator = TypecastForward, inputs = [Tensor_12401], outputs = [Tensor_12405], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_12405: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12359: 19365888 Bnn)
Tensor_12406: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12406: 1179648 Ban)
Tensor_12407: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11821: 1536 Bnn)
Operation_1173: operator = LinearForward, inputs = [Tensor_12405, Tensor_12406, Tensor_12407], outputs = [Tensor_12409], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_12409: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12416: 19365888 Ban)
Tensor_12410: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12412: 128 Ban)
Operation_1180: operator = ElemwiseUnaryForward, inputs = [Tensor_12410], outputs = [Tensor_12412], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_ADD scal1_float: 0.959090889]
Tensor_12412: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12412: 128 Bnn)
Operation_1182: operator = ElemwiseUnaryForward, inputs = [Tensor_12412], outputs = [Tensor_12414], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_FLOOR]
Tensor_12414: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12412: 128 Bnn)
Operation_1184: operator = ElemwiseUnaryForward, inputs = [Tensor_12409], outputs = [Tensor_12416], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_DIV scal1_float: 0.959090889]
Tensor_12416: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12416: 19365888 Bnn)
Operation_1186: operator = ReshapeForward, inputs = [Tensor_12414], outputs = [Tensor_12418], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 1 stride: 0 stride: 0]
Tensor_12418: shape = [128, 197, 768], stride = [1, 0, 0], offset = 0, size = 128, datatype = float16, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12412: 128 Bnn)
Operation_1188: operator = ElemwiseBinaryForward, inputs = [Tensor_12416, Tensor_12418], outputs = [Tensor_12420], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_MUL]
Tensor_12420: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12420: 19365888 Ban)
Tensor_12421: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12425: 38731776 Ban)
Operation_1192: operator = TypecastForward, inputs = [Tensor_12420], outputs = [Tensor_12423], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_12423: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12391: 38731776 Bnn)
Operation_1194: operator = ElemwiseBinaryForward, inputs = [Tensor_12421, Tensor_12423], outputs = [Tensor_12425], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_12425: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12425: 38731776 Bnn)
Operation_1196: operator = LayernormForward, inputs = [Tensor_12425, Tensor_11612, Tensor_11613], outputs = [Tensor_12427, Tensor_12428, Tensor_12429], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_12427: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12391: 38731776 Bnn)
Tensor_12428: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12428: 50432 Ban)
Tensor_12429: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12429: 50432 Ban)
Operation_1200: operator = ReshapeForward, inputs = [Tensor_12427], outputs = [Tensor_12431], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 25216 shape: 768 stride: 768 stride: 1]
Tensor_12431: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12391: 38731776 Bnn)
Operation_1202: operator = TypecastForward, inputs = [Tensor_12431], outputs = [Tensor_12433], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_12433: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12420: 19365888 Bnn)
Tensor_12434: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12434: 4718592 Ban)
Tensor_12435: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11969: 6144 Bnn)
Operation_1208: operator = LinearForward, inputs = [Tensor_12433, Tensor_12434, Tensor_12435], outputs = [Tensor_12437], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_12437: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12437: 77463552 Ban)
Operation_1210: operator = TypecastForward, inputs = [Tensor_12437], outputs = [Tensor_12439], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_12439: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12439: 154927104 Ban)
Operation_1212: operator = ElemwiseUnaryForward, inputs = [Tensor_12439], outputs = [Tensor_12441], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_GELU]
Tensor_12441: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12441: 154927104 Ban)
Operation_1214: operator = TypecastForward, inputs = [Tensor_12441], outputs = [Tensor_12443], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_12443: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12437: 77463552 Bnn)
Operation_1216: operator = TypecastForward, inputs = [Tensor_12443], outputs = [Tensor_12445], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_12445: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12445: 154927104 Ban)
Operation_1218: operator = LayernormForward, inputs = [Tensor_12445, Tensor_11610, Tensor_11611], outputs = [Tensor_12447, Tensor_12448, Tensor_12449], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_12447: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12447: 154927104 Ban)
Tensor_12448: shape = [25216], stride = [1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12448: 50432 Ban)
Tensor_12449: shape = [25216], stride = [1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12449: 50432 Ban)
Operation_1222: operator = TypecastForward, inputs = [Tensor_12447], outputs = [Tensor_12451], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_12451: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12437: 77463552 Bnn)
Operation_1224: operator = TypecastForward, inputs = [Tensor_11608], outputs = [Tensor_12453], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_12453: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12453: 4718592 Ban)
Operation_1226: operator = TypecastForward, inputs = [Tensor_11609], outputs = [Tensor_12455], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_12455: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11821: 1536 Bnn)
Operation_1228: operator = LinearForward, inputs = [Tensor_12451, Tensor_12453, Tensor_12455], outputs = [Tensor_12457], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_12457: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12464: 19365888 Ban)
Tensor_12458: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12460: 128 Ban)
Operation_1233: operator = ElemwiseUnaryForward, inputs = [Tensor_12458], outputs = [Tensor_12460], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_ADD scal1_float: 0.959090889]
Tensor_12460: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12460: 128 Bnn)
Operation_1235: operator = ElemwiseUnaryForward, inputs = [Tensor_12460], outputs = [Tensor_12462], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_FLOOR]
Tensor_12462: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12460: 128 Bnn)
Operation_1230: operator = ReshapeForward, inputs = [Tensor_12457], outputs = [Tensor_12464], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 151296 stride: 768 stride: 1]
Tensor_12464: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12464: 19365888 Bnn)
Operation_1237: operator = ElemwiseUnaryForward, inputs = [Tensor_12464], outputs = [Tensor_12466], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_DIV scal1_float: 0.959090889]
Tensor_12466: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12464: 19365888 Bnn)
Operation_1239: operator = ReshapeForward, inputs = [Tensor_12462], outputs = [Tensor_12468], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 1 stride: 0 stride: 0]
Tensor_12468: shape = [128, 197, 768], stride = [1, 0, 0], offset = 0, size = 128, datatype = float16, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12460: 128 Bnn)
Operation_1241: operator = ElemwiseBinaryForward, inputs = [Tensor_12466, Tensor_12468], outputs = [Tensor_12470], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_MUL]
Tensor_12470: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12470: 19365888 Ban)
Operation_1243: operator = ElemwiseUnaryForward, inputs = [Tensor_12425], outputs = [Tensor_12472], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 1]
Tensor_12472: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12476: 38731776 Ban)
Operation_1245: operator = TypecastForward, inputs = [Tensor_12470], outputs = [Tensor_12474], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_12474: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12474: 38731776 Ban)
Operation_1247: operator = ElemwiseBinaryForward, inputs = [Tensor_12472, Tensor_12474], outputs = [Tensor_12476], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_12476: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12476: 38731776 Bnn)
Operation_1249: operator = LayernormForward, inputs = [Tensor_12476, Tensor_11624, Tensor_11625], outputs = [Tensor_12478, Tensor_12479, Tensor_12480], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_12478: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12474: 38731776 Bnn)
Tensor_12479: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12479: 50432 Ban)
Tensor_12480: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12480: 50432 Ban)
Operation_1253: operator = TypecastForward, inputs = [Tensor_12478], outputs = [Tensor_12482], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_12482: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12470: 19365888 Bnn)
Operation_1255: operator = TypecastForward, inputs = [Tensor_11618], outputs = [Tensor_12484], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_12484: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12484: 1179648 Ban)
Operation_1257: operator = TypecastForward, inputs = [Tensor_11619], outputs = [Tensor_12486], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_12486: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11821: 1536 Bnn)
Operation_1259: operator = LinearForward, inputs = [Tensor_12482, Tensor_12484, Tensor_12486], outputs = [Tensor_12488], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_12488: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12506: 19365888 Ban)
Operation_1261: operator = TypecastForward, inputs = [Tensor_12478], outputs = [Tensor_12490], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_12490: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12490: 19365888 Ban)
Operation_1263: operator = TypecastForward, inputs = [Tensor_11614], outputs = [Tensor_12492], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_12492: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12492: 1179648 Ban)
Operation_1265: operator = TypecastForward, inputs = [Tensor_11615], outputs = [Tensor_12494], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_12494: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11821: 1536 Bnn)
Operation_1267: operator = LinearForward, inputs = [Tensor_12490, Tensor_12492, Tensor_12494], outputs = [Tensor_12496], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_12496: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12514: 19365888 Ban)
Operation_1269: operator = TypecastForward, inputs = [Tensor_12478], outputs = [Tensor_12498], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_12498: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12498: 19365888 Ban)
Operation_1271: operator = TypecastForward, inputs = [Tensor_11616], outputs = [Tensor_12500], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_12500: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12500: 1179648 Ban)
Operation_1273: operator = TypecastForward, inputs = [Tensor_11617], outputs = [Tensor_12502], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_12502: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11821: 1536 Bnn)
Operation_1275: operator = LinearForward, inputs = [Tensor_12498, Tensor_12500, Tensor_12502], outputs = [Tensor_12504], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_12504: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12518: 19365888 Ban)
Operation_1277: operator = ElemwiseUnaryForward, inputs = [Tensor_12488], outputs = [Tensor_12506], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 0.125]
Tensor_12506: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12506: 19365888 Bnn)
Operation_1281: operator = ReshapeForward, inputs = [Tensor_12506], outputs = [Tensor_12508], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 64 stride: 151296 stride: 64 stride: 768 stride: 1]
Tensor_12508: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12506: 19365888 Bnn)
Operation_1291: operator = ContiguousForward, inputs = [Tensor_12508], outputs = [Tensor_12510], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_12510: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12512: 19365888 Ban)
Operation_1293: operator = ReshapeForward, inputs = [Tensor_12510], outputs = [Tensor_12512], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_12512: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12512: 19365888 Bnn)
Operation_1285: operator = ReshapeForward, inputs = [Tensor_12496], outputs = [Tensor_12514], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 64 stride: 151296 stride: 64 stride: 768 stride: 1]
Tensor_12514: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12514: 19365888 Bnn)
Operation_1295: operator = ContiguousForward, inputs = [Tensor_12514], outputs = [Tensor_12516], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_12516: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12506: 19365888 Bnn)
Operation_1289: operator = ReshapeForward, inputs = [Tensor_12504], outputs = [Tensor_12518], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 64 stride: 151296 stride: 64 stride: 768 stride: 1]
Tensor_12518: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12518: 19365888 Bnn)
Operation_1299: operator = ContiguousForward, inputs = [Tensor_12518], outputs = [Tensor_12520], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_12520: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12514: 19365888 Bnn)
Operation_1301: operator = ReshapeForward, inputs = [Tensor_12520], outputs = [Tensor_12522], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_12522: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12514: 19365888 Bnn)
Operation_1303: operator = ReshapeForward, inputs = [Tensor_12516], outputs = [Tensor_12524], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 64 shape: 197 stride: 12608 stride: 1 stride: 64]
Tensor_12524: shape = [1536, 64, 197], stride = [12608, 1, 64], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12506: 19365888 Bnn)
Operation_1305: operator = BatchedMatmulForward, inputs = [Tensor_12512, Tensor_12524], outputs = [Tensor_12526], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_12526: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12528: 59610624 Ban)
Operation_1307: operator = ReshapeForward, inputs = [Tensor_12526], outputs = [Tensor_12528], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 197 stride: 465708 stride: 38809 stride: 197 stride: 1]
Tensor_12528: shape = [128, 12, 197, 197], stride = [465708, 38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12528: 59610624 Bnn)
Operation_1315: operator = MaskedFillForward, inputs = [Tensor_12528, Tensor_11912], outputs = [Tensor_12530], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [value_float: -inf]
Tensor_12530: shape = [128, 12, 197, 197], stride = [465708, 38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12528: 59610624 Bnn)
Operation_1317: operator = ReshapeForward, inputs = [Tensor_12530], outputs = [Tensor_12532], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 197 stride: 38809 stride: 197 stride: 1]
Tensor_12532: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12528: 59610624 Bnn)
Operation_1319: operator = TypecastForward, inputs = [Tensor_12532], outputs = [Tensor_12534], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_12534: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11920: 119221248 Bnn)
Operation_1321: operator = SoftmaxForward, inputs = [Tensor_12534], outputs = [Tensor_12536], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dim: 2]
Tensor_12536: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12536: 119221248 Ban)
Operation_1323: operator = TypecastForward, inputs = [Tensor_12536], outputs = [Tensor_12538], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_12538: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12528: 59610624 Bnn)
Operation_1325: operator = BatchedMatmulForward, inputs = [Tensor_12538, Tensor_12522], outputs = [Tensor_12540], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_12540: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12518: 19365888 Bnn)
Operation_8051: operator = AllGatherDuplicated, inputs = [Tensor_12540], outputs = [Tensor_12543], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = all->this->1327
  - Operand: [tag: 8050]
Operation_12542: operator = Consume, inputs = [Tensor_12540], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_12543: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12550: 38731776 Ban)
Operation_1343: operator = TypecastForward, inputs = [Tensor_11620], outputs = [Tensor_12565], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Operation_1345: operator = TypecastForward, inputs = [Tensor_11621], outputs = [Tensor_12566], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Operation_1364: operator = ElemwiseUnaryForward, inputs = [Tensor_12476], outputs = [Tensor_12580], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 1]
Operation_1378: operator = TypecastForward, inputs = [Tensor_11626], outputs = [Tensor_12593], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Operation_1380: operator = TypecastForward, inputs = [Tensor_11627], outputs = [Tensor_12594], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Operation_1327: operator = ReshapeForward, inputs = [Tensor_12543], outputs = [Tensor_12550], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = 8051->this
  - Operand: [shape: 197 shape: 1536 shape: 64 stride: 64 stride: 12608 stride: 1]
Tensor_12550: shape = [197, 1536, 64], stride = [64, 12608, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12550: 38731776 Bnn)
Operation_1329: operator = ContiguousForward, inputs = [Tensor_12550], outputs = [Tensor_12552], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_12552: shape = [197, 1536, 64], stride = [98304, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12554: 38731776 Ban)
Operation_1333: operator = ReshapeForward, inputs = [Tensor_12552], outputs = [Tensor_12554], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 768 stride: 98304 stride: 1]
Tensor_12554: shape = [128, 197, 768], stride = [768, 98304, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12554: 38731776 Bnn)
Operation_1335: operator = TypecastForward, inputs = [Tensor_12554], outputs = [Tensor_12556], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_12556: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12556: 77463552 Ban)
Operation_8061: operator = DistributeWithMemAllocForward, inputs = [Tensor_12556], outputs = [Tensor_12558], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_12558: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12550: 38731776 Bnn)
Operation_1337: operator = LayernormForward, inputs = [Tensor_12558, Tensor_11622, Tensor_11623], outputs = [Tensor_12560, Tensor_12561, Tensor_12562], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_12560: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12554: 38731776 Bnn)
Tensor_12561: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12561: 50432 Ban)
Tensor_12562: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12562: 50432 Ban)
Operation_1341: operator = TypecastForward, inputs = [Tensor_12560], outputs = [Tensor_12564], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_12564: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12518: 19365888 Bnn)
Tensor_12565: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12565: 1179648 Ban)
Tensor_12566: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11821: 1536 Bnn)
Operation_1347: operator = LinearForward, inputs = [Tensor_12564, Tensor_12565, Tensor_12566], outputs = [Tensor_12568], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_12568: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12575: 19365888 Ban)
Tensor_12569: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12571: 128 Ban)
Operation_1354: operator = ElemwiseUnaryForward, inputs = [Tensor_12569], outputs = [Tensor_12571], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_ADD scal1_float: 0.945454538]
Tensor_12571: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12571: 128 Bnn)
Operation_1356: operator = ElemwiseUnaryForward, inputs = [Tensor_12571], outputs = [Tensor_12573], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_FLOOR]
Tensor_12573: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12571: 128 Bnn)
Operation_1358: operator = ElemwiseUnaryForward, inputs = [Tensor_12568], outputs = [Tensor_12575], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_DIV scal1_float: 0.945454538]
Tensor_12575: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12575: 19365888 Bnn)
Operation_1360: operator = ReshapeForward, inputs = [Tensor_12573], outputs = [Tensor_12577], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 1 stride: 0 stride: 0]
Tensor_12577: shape = [128, 197, 768], stride = [1, 0, 0], offset = 0, size = 128, datatype = float16, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12571: 128 Bnn)
Operation_1362: operator = ElemwiseBinaryForward, inputs = [Tensor_12575, Tensor_12577], outputs = [Tensor_12579], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_MUL]
Tensor_12579: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12579: 19365888 Ban)
Tensor_12580: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12584: 38731776 Ban)
Operation_1366: operator = TypecastForward, inputs = [Tensor_12579], outputs = [Tensor_12582], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_12582: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12550: 38731776 Bnn)
Operation_1368: operator = ElemwiseBinaryForward, inputs = [Tensor_12580, Tensor_12582], outputs = [Tensor_12584], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_12584: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12584: 38731776 Bnn)
Operation_1370: operator = LayernormForward, inputs = [Tensor_12584, Tensor_11632, Tensor_11633], outputs = [Tensor_12586, Tensor_12587, Tensor_12588], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_12586: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12550: 38731776 Bnn)
Tensor_12587: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12587: 50432 Ban)
Tensor_12588: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12588: 50432 Ban)
Operation_1374: operator = ReshapeForward, inputs = [Tensor_12586], outputs = [Tensor_12590], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 25216 shape: 768 stride: 768 stride: 1]
Tensor_12590: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12550: 38731776 Bnn)
Operation_1376: operator = TypecastForward, inputs = [Tensor_12590], outputs = [Tensor_12592], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_12592: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12579: 19365888 Bnn)
Tensor_12593: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12593: 4718592 Ban)
Tensor_12594: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11969: 6144 Bnn)
Operation_1382: operator = LinearForward, inputs = [Tensor_12592, Tensor_12593, Tensor_12594], outputs = [Tensor_12596], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_12596: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12596: 77463552 Ban)
Operation_1384: operator = TypecastForward, inputs = [Tensor_12596], outputs = [Tensor_12598], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_12598: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12598: 154927104 Ban)
Operation_1386: operator = ElemwiseUnaryForward, inputs = [Tensor_12598], outputs = [Tensor_12600], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_GELU]
Tensor_12600: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12600: 154927104 Ban)
Operation_1388: operator = TypecastForward, inputs = [Tensor_12600], outputs = [Tensor_12602], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_12602: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12596: 77463552 Bnn)
Operation_1390: operator = TypecastForward, inputs = [Tensor_12602], outputs = [Tensor_12604], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_12604: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12604: 154927104 Ban)
Operation_1392: operator = LayernormForward, inputs = [Tensor_12604, Tensor_11630, Tensor_11631], outputs = [Tensor_12606, Tensor_12607, Tensor_12608], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_12606: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12606: 154927104 Ban)
Tensor_12607: shape = [25216], stride = [1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12607: 50432 Ban)
Tensor_12608: shape = [25216], stride = [1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12608: 50432 Ban)
Operation_1396: operator = TypecastForward, inputs = [Tensor_12606], outputs = [Tensor_12610], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_12610: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12596: 77463552 Bnn)
Operation_1398: operator = TypecastForward, inputs = [Tensor_11628], outputs = [Tensor_12612], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_12612: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12612: 4718592 Ban)
Operation_1400: operator = TypecastForward, inputs = [Tensor_11629], outputs = [Tensor_12614], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_12614: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11821: 1536 Bnn)
Operation_1402: operator = LinearForward, inputs = [Tensor_12610, Tensor_12612, Tensor_12614], outputs = [Tensor_12616], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_12616: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12623: 19365888 Ban)
Tensor_12617: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12619: 128 Ban)
Operation_1407: operator = ElemwiseUnaryForward, inputs = [Tensor_12617], outputs = [Tensor_12619], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_ADD scal1_float: 0.945454538]
Tensor_12619: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12619: 128 Bnn)
Operation_1409: operator = ElemwiseUnaryForward, inputs = [Tensor_12619], outputs = [Tensor_12621], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_FLOOR]
Tensor_12621: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12619: 128 Bnn)
Operation_1404: operator = ReshapeForward, inputs = [Tensor_12616], outputs = [Tensor_12623], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 151296 stride: 768 stride: 1]
Tensor_12623: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12623: 19365888 Bnn)
Operation_1411: operator = ElemwiseUnaryForward, inputs = [Tensor_12623], outputs = [Tensor_12625], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_DIV scal1_float: 0.945454538]
Tensor_12625: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12623: 19365888 Bnn)
Operation_1413: operator = ReshapeForward, inputs = [Tensor_12621], outputs = [Tensor_12627], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 1 stride: 0 stride: 0]
Tensor_12627: shape = [128, 197, 768], stride = [1, 0, 0], offset = 0, size = 128, datatype = float16, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12619: 128 Bnn)
Operation_1415: operator = ElemwiseBinaryForward, inputs = [Tensor_12625, Tensor_12627], outputs = [Tensor_12629], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_MUL]
Tensor_12629: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12629: 19365888 Ban)
Operation_1417: operator = ElemwiseUnaryForward, inputs = [Tensor_12584], outputs = [Tensor_12631], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 1]
Tensor_12631: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12635: 38731776 Ban)
Operation_1419: operator = TypecastForward, inputs = [Tensor_12629], outputs = [Tensor_12633], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_12633: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12633: 38731776 Ban)
Operation_1421: operator = ElemwiseBinaryForward, inputs = [Tensor_12631, Tensor_12633], outputs = [Tensor_12635], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_12635: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12635: 38731776 Bnn)
Operation_1423: operator = LayernormForward, inputs = [Tensor_12635, Tensor_11644, Tensor_11645], outputs = [Tensor_12637, Tensor_12638, Tensor_12639], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_12637: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12633: 38731776 Bnn)
Tensor_12638: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12638: 50432 Ban)
Tensor_12639: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12639: 50432 Ban)
Operation_1427: operator = TypecastForward, inputs = [Tensor_12637], outputs = [Tensor_12641], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_12641: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12629: 19365888 Bnn)
Operation_1429: operator = TypecastForward, inputs = [Tensor_11638], outputs = [Tensor_12643], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_12643: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12643: 1179648 Ban)
Operation_1431: operator = TypecastForward, inputs = [Tensor_11639], outputs = [Tensor_12645], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_12645: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11821: 1536 Bnn)
Operation_1433: operator = LinearForward, inputs = [Tensor_12641, Tensor_12643, Tensor_12645], outputs = [Tensor_12647], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_12647: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12665: 19365888 Ban)
Operation_1435: operator = TypecastForward, inputs = [Tensor_12637], outputs = [Tensor_12649], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_12649: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12649: 19365888 Ban)
Operation_1437: operator = TypecastForward, inputs = [Tensor_11634], outputs = [Tensor_12651], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_12651: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12651: 1179648 Ban)
Operation_1439: operator = TypecastForward, inputs = [Tensor_11635], outputs = [Tensor_12653], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_12653: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11821: 1536 Bnn)
Operation_1441: operator = LinearForward, inputs = [Tensor_12649, Tensor_12651, Tensor_12653], outputs = [Tensor_12655], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_12655: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12673: 19365888 Ban)
Operation_1443: operator = TypecastForward, inputs = [Tensor_12637], outputs = [Tensor_12657], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_12657: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12657: 19365888 Ban)
Operation_1445: operator = TypecastForward, inputs = [Tensor_11636], outputs = [Tensor_12659], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_12659: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12659: 1179648 Ban)
Operation_1447: operator = TypecastForward, inputs = [Tensor_11637], outputs = [Tensor_12661], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_12661: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11821: 1536 Bnn)
Operation_1449: operator = LinearForward, inputs = [Tensor_12657, Tensor_12659, Tensor_12661], outputs = [Tensor_12663], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_12663: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12677: 19365888 Ban)
Operation_1451: operator = ElemwiseUnaryForward, inputs = [Tensor_12647], outputs = [Tensor_12665], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 0.125]
Tensor_12665: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12665: 19365888 Bnn)
Operation_1455: operator = ReshapeForward, inputs = [Tensor_12665], outputs = [Tensor_12667], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 64 stride: 151296 stride: 64 stride: 768 stride: 1]
Tensor_12667: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12665: 19365888 Bnn)
Operation_1465: operator = ContiguousForward, inputs = [Tensor_12667], outputs = [Tensor_12669], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_12669: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12671: 19365888 Ban)
Operation_1467: operator = ReshapeForward, inputs = [Tensor_12669], outputs = [Tensor_12671], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_12671: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12671: 19365888 Bnn)
Operation_1459: operator = ReshapeForward, inputs = [Tensor_12655], outputs = [Tensor_12673], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 64 stride: 151296 stride: 64 stride: 768 stride: 1]
Tensor_12673: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12673: 19365888 Bnn)
Operation_1469: operator = ContiguousForward, inputs = [Tensor_12673], outputs = [Tensor_12675], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_12675: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12665: 19365888 Bnn)
Operation_1463: operator = ReshapeForward, inputs = [Tensor_12663], outputs = [Tensor_12677], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 64 stride: 151296 stride: 64 stride: 768 stride: 1]
Tensor_12677: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12677: 19365888 Bnn)
Operation_1473: operator = ContiguousForward, inputs = [Tensor_12677], outputs = [Tensor_12679], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_12679: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12673: 19365888 Bnn)
Operation_1475: operator = ReshapeForward, inputs = [Tensor_12679], outputs = [Tensor_12681], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_12681: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12673: 19365888 Bnn)
Operation_1477: operator = ReshapeForward, inputs = [Tensor_12675], outputs = [Tensor_12683], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 64 shape: 197 stride: 12608 stride: 1 stride: 64]
Tensor_12683: shape = [1536, 64, 197], stride = [12608, 1, 64], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12665: 19365888 Bnn)
Operation_1479: operator = BatchedMatmulForward, inputs = [Tensor_12671, Tensor_12683], outputs = [Tensor_12685], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_12685: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12687: 59610624 Ban)
Operation_1481: operator = ReshapeForward, inputs = [Tensor_12685], outputs = [Tensor_12687], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 197 stride: 465708 stride: 38809 stride: 197 stride: 1]
Tensor_12687: shape = [128, 12, 197, 197], stride = [465708, 38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12687: 59610624 Bnn)
Operation_1489: operator = MaskedFillForward, inputs = [Tensor_12687, Tensor_11912], outputs = [Tensor_12689], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [value_float: -inf]
Tensor_12689: shape = [128, 12, 197, 197], stride = [465708, 38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12687: 59610624 Bnn)
Operation_1491: operator = ReshapeForward, inputs = [Tensor_12689], outputs = [Tensor_12691], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 197 stride: 38809 stride: 197 stride: 1]
Tensor_12691: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12687: 59610624 Bnn)
Operation_1493: operator = TypecastForward, inputs = [Tensor_12691], outputs = [Tensor_12693], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_12693: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11920: 119221248 Bnn)
Operation_1495: operator = SoftmaxForward, inputs = [Tensor_12693], outputs = [Tensor_12695], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dim: 2]
Tensor_12695: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12695: 119221248 Ban)
Operation_1497: operator = TypecastForward, inputs = [Tensor_12695], outputs = [Tensor_12697], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_12697: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12687: 59610624 Bnn)
Operation_1499: operator = BatchedMatmulForward, inputs = [Tensor_12697, Tensor_12681], outputs = [Tensor_12699], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_12699: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12677: 19365888 Bnn)
Operation_8210: operator = AllGatherDuplicated, inputs = [Tensor_12699], outputs = [Tensor_12702], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = all->this->1501
  - Operand: [tag: 8209]
Operation_12701: operator = Consume, inputs = [Tensor_12699], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_12702: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12709: 38731776 Ban)
Operation_1517: operator = TypecastForward, inputs = [Tensor_11640], outputs = [Tensor_12724], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Operation_1519: operator = TypecastForward, inputs = [Tensor_11641], outputs = [Tensor_12725], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Operation_1538: operator = ElemwiseUnaryForward, inputs = [Tensor_12635], outputs = [Tensor_12739], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 1]
Operation_1552: operator = TypecastForward, inputs = [Tensor_11646], outputs = [Tensor_12752], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Operation_1554: operator = TypecastForward, inputs = [Tensor_11647], outputs = [Tensor_12753], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Operation_1501: operator = ReshapeForward, inputs = [Tensor_12702], outputs = [Tensor_12709], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = 8210->this
  - Operand: [shape: 197 shape: 1536 shape: 64 stride: 64 stride: 12608 stride: 1]
Tensor_12709: shape = [197, 1536, 64], stride = [64, 12608, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12709: 38731776 Bnn)
Operation_1503: operator = ContiguousForward, inputs = [Tensor_12709], outputs = [Tensor_12711], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_12711: shape = [197, 1536, 64], stride = [98304, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12713: 38731776 Ban)
Operation_1507: operator = ReshapeForward, inputs = [Tensor_12711], outputs = [Tensor_12713], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 768 stride: 98304 stride: 1]
Tensor_12713: shape = [128, 197, 768], stride = [768, 98304, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12713: 38731776 Bnn)
Operation_1509: operator = TypecastForward, inputs = [Tensor_12713], outputs = [Tensor_12715], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_12715: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12715: 77463552 Ban)
Operation_8220: operator = DistributeWithMemAllocForward, inputs = [Tensor_12715], outputs = [Tensor_12717], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_12717: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12709: 38731776 Bnn)
Operation_1511: operator = LayernormForward, inputs = [Tensor_12717, Tensor_11642, Tensor_11643], outputs = [Tensor_12719, Tensor_12720, Tensor_12721], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_12719: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12713: 38731776 Bnn)
Tensor_12720: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12720: 50432 Ban)
Tensor_12721: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12721: 50432 Ban)
Operation_1515: operator = TypecastForward, inputs = [Tensor_12719], outputs = [Tensor_12723], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_12723: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12677: 19365888 Bnn)
Tensor_12724: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12724: 1179648 Ban)
Tensor_12725: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11821: 1536 Bnn)
Operation_1521: operator = LinearForward, inputs = [Tensor_12723, Tensor_12724, Tensor_12725], outputs = [Tensor_12727], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_12727: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12734: 19365888 Ban)
Tensor_12728: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12730: 128 Ban)
Operation_1528: operator = ElemwiseUnaryForward, inputs = [Tensor_12728], outputs = [Tensor_12730], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_ADD scal1_float: 0.931818187]
Tensor_12730: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12730: 128 Bnn)
Operation_1530: operator = ElemwiseUnaryForward, inputs = [Tensor_12730], outputs = [Tensor_12732], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_FLOOR]
Tensor_12732: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12730: 128 Bnn)
Operation_1532: operator = ElemwiseUnaryForward, inputs = [Tensor_12727], outputs = [Tensor_12734], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_DIV scal1_float: 0.931818187]
Tensor_12734: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12734: 19365888 Bnn)
Operation_1534: operator = ReshapeForward, inputs = [Tensor_12732], outputs = [Tensor_12736], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 1 stride: 0 stride: 0]
Tensor_12736: shape = [128, 197, 768], stride = [1, 0, 0], offset = 0, size = 128, datatype = float16, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12730: 128 Bnn)
Operation_1536: operator = ElemwiseBinaryForward, inputs = [Tensor_12734, Tensor_12736], outputs = [Tensor_12738], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_MUL]
Tensor_12738: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12738: 19365888 Ban)
Tensor_12739: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12743: 38731776 Ban)
Operation_1540: operator = TypecastForward, inputs = [Tensor_12738], outputs = [Tensor_12741], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_12741: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12709: 38731776 Bnn)
Operation_1542: operator = ElemwiseBinaryForward, inputs = [Tensor_12739, Tensor_12741], outputs = [Tensor_12743], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_12743: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12743: 38731776 Bnn)
Operation_1544: operator = LayernormForward, inputs = [Tensor_12743, Tensor_11652, Tensor_11653], outputs = [Tensor_12745, Tensor_12746, Tensor_12747], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_12745: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12709: 38731776 Bnn)
Tensor_12746: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12746: 50432 Ban)
Tensor_12747: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12747: 50432 Ban)
Operation_1548: operator = ReshapeForward, inputs = [Tensor_12745], outputs = [Tensor_12749], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 25216 shape: 768 stride: 768 stride: 1]
Tensor_12749: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12709: 38731776 Bnn)
Operation_1550: operator = TypecastForward, inputs = [Tensor_12749], outputs = [Tensor_12751], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_12751: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12738: 19365888 Bnn)
Tensor_12752: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12752: 4718592 Ban)
Tensor_12753: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11969: 6144 Bnn)
Operation_1556: operator = LinearForward, inputs = [Tensor_12751, Tensor_12752, Tensor_12753], outputs = [Tensor_12755], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_12755: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12755: 77463552 Ban)
Operation_1558: operator = TypecastForward, inputs = [Tensor_12755], outputs = [Tensor_12757], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_12757: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12757: 154927104 Ban)
Operation_1560: operator = ElemwiseUnaryForward, inputs = [Tensor_12757], outputs = [Tensor_12759], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_GELU]
Tensor_12759: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12759: 154927104 Ban)
Operation_1562: operator = TypecastForward, inputs = [Tensor_12759], outputs = [Tensor_12761], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_12761: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12755: 77463552 Bnn)
Operation_1564: operator = TypecastForward, inputs = [Tensor_12761], outputs = [Tensor_12763], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_12763: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12763: 154927104 Ban)
Operation_1566: operator = LayernormForward, inputs = [Tensor_12763, Tensor_11650, Tensor_11651], outputs = [Tensor_12765, Tensor_12766, Tensor_12767], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_12765: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12765: 154927104 Ban)
Tensor_12766: shape = [25216], stride = [1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12766: 50432 Ban)
Tensor_12767: shape = [25216], stride = [1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12767: 50432 Ban)
Operation_1570: operator = TypecastForward, inputs = [Tensor_12765], outputs = [Tensor_12769], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_12769: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12755: 77463552 Bnn)
Operation_1572: operator = TypecastForward, inputs = [Tensor_11648], outputs = [Tensor_12771], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_12771: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12771: 4718592 Ban)
Operation_1574: operator = TypecastForward, inputs = [Tensor_11649], outputs = [Tensor_12773], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_12773: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11821: 1536 Bnn)
Operation_1576: operator = LinearForward, inputs = [Tensor_12769, Tensor_12771, Tensor_12773], outputs = [Tensor_12775], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_12775: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12782: 19365888 Ban)
Tensor_12776: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12778: 128 Ban)
Operation_1581: operator = ElemwiseUnaryForward, inputs = [Tensor_12776], outputs = [Tensor_12778], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_ADD scal1_float: 0.931818187]
Tensor_12778: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12778: 128 Bnn)
Operation_1583: operator = ElemwiseUnaryForward, inputs = [Tensor_12778], outputs = [Tensor_12780], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_FLOOR]
Tensor_12780: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12778: 128 Bnn)
Operation_1578: operator = ReshapeForward, inputs = [Tensor_12775], outputs = [Tensor_12782], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 151296 stride: 768 stride: 1]
Tensor_12782: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12782: 19365888 Bnn)
Operation_1585: operator = ElemwiseUnaryForward, inputs = [Tensor_12782], outputs = [Tensor_12784], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_DIV scal1_float: 0.931818187]
Tensor_12784: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12782: 19365888 Bnn)
Operation_1587: operator = ReshapeForward, inputs = [Tensor_12780], outputs = [Tensor_12786], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 1 stride: 0 stride: 0]
Tensor_12786: shape = [128, 197, 768], stride = [1, 0, 0], offset = 0, size = 128, datatype = float16, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12778: 128 Bnn)
Operation_1589: operator = ElemwiseBinaryForward, inputs = [Tensor_12784, Tensor_12786], outputs = [Tensor_12788], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_MUL]
Tensor_12788: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12788: 19365888 Ban)
Operation_1591: operator = ElemwiseUnaryForward, inputs = [Tensor_12743], outputs = [Tensor_12790], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 1]
Tensor_12790: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12794: 38731776 Ban)
Operation_1593: operator = TypecastForward, inputs = [Tensor_12788], outputs = [Tensor_12792], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_12792: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12792: 38731776 Ban)
Operation_1595: operator = ElemwiseBinaryForward, inputs = [Tensor_12790, Tensor_12792], outputs = [Tensor_12794], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_12794: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12794: 38731776 Bnn)
Operation_1597: operator = LayernormForward, inputs = [Tensor_12794, Tensor_11664, Tensor_11665], outputs = [Tensor_12796, Tensor_12797, Tensor_12798], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_12796: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12792: 38731776 Bnn)
Tensor_12797: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12797: 50432 Ban)
Tensor_12798: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12798: 50432 Ban)
Operation_1601: operator = TypecastForward, inputs = [Tensor_12796], outputs = [Tensor_12800], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_12800: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12788: 19365888 Bnn)
Operation_1603: operator = TypecastForward, inputs = [Tensor_11658], outputs = [Tensor_12802], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_12802: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12802: 1179648 Ban)
Operation_1605: operator = TypecastForward, inputs = [Tensor_11659], outputs = [Tensor_12804], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_12804: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11821: 1536 Bnn)
Operation_1607: operator = LinearForward, inputs = [Tensor_12800, Tensor_12802, Tensor_12804], outputs = [Tensor_12806], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_12806: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12824: 19365888 Ban)
Operation_1609: operator = TypecastForward, inputs = [Tensor_12796], outputs = [Tensor_12808], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_12808: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12808: 19365888 Ban)
Operation_1611: operator = TypecastForward, inputs = [Tensor_11654], outputs = [Tensor_12810], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_12810: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12810: 1179648 Ban)
Operation_1613: operator = TypecastForward, inputs = [Tensor_11655], outputs = [Tensor_12812], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_12812: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11821: 1536 Bnn)
Operation_1615: operator = LinearForward, inputs = [Tensor_12808, Tensor_12810, Tensor_12812], outputs = [Tensor_12814], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_12814: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12832: 19365888 Ban)
Operation_1617: operator = TypecastForward, inputs = [Tensor_12796], outputs = [Tensor_12816], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_12816: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12816: 19365888 Ban)
Operation_1619: operator = TypecastForward, inputs = [Tensor_11656], outputs = [Tensor_12818], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_12818: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12818: 1179648 Ban)
Operation_1621: operator = TypecastForward, inputs = [Tensor_11657], outputs = [Tensor_12820], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_12820: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11821: 1536 Bnn)
Operation_1623: operator = LinearForward, inputs = [Tensor_12816, Tensor_12818, Tensor_12820], outputs = [Tensor_12822], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_12822: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12836: 19365888 Ban)
Operation_1625: operator = ElemwiseUnaryForward, inputs = [Tensor_12806], outputs = [Tensor_12824], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 0.125]
Tensor_12824: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12824: 19365888 Bnn)
Operation_1629: operator = ReshapeForward, inputs = [Tensor_12824], outputs = [Tensor_12826], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 64 stride: 151296 stride: 64 stride: 768 stride: 1]
Tensor_12826: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12824: 19365888 Bnn)
Operation_1639: operator = ContiguousForward, inputs = [Tensor_12826], outputs = [Tensor_12828], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_12828: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12830: 19365888 Ban)
Operation_1641: operator = ReshapeForward, inputs = [Tensor_12828], outputs = [Tensor_12830], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_12830: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12830: 19365888 Bnn)
Operation_1633: operator = ReshapeForward, inputs = [Tensor_12814], outputs = [Tensor_12832], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 64 stride: 151296 stride: 64 stride: 768 stride: 1]
Tensor_12832: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12832: 19365888 Bnn)
Operation_1643: operator = ContiguousForward, inputs = [Tensor_12832], outputs = [Tensor_12834], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_12834: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12824: 19365888 Bnn)
Operation_1637: operator = ReshapeForward, inputs = [Tensor_12822], outputs = [Tensor_12836], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 64 stride: 151296 stride: 64 stride: 768 stride: 1]
Tensor_12836: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12836: 19365888 Bnn)
Operation_1647: operator = ContiguousForward, inputs = [Tensor_12836], outputs = [Tensor_12838], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_12838: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12832: 19365888 Bnn)
Operation_1649: operator = ReshapeForward, inputs = [Tensor_12838], outputs = [Tensor_12840], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_12840: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12832: 19365888 Bnn)
Operation_1651: operator = ReshapeForward, inputs = [Tensor_12834], outputs = [Tensor_12842], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 64 shape: 197 stride: 12608 stride: 1 stride: 64]
Tensor_12842: shape = [1536, 64, 197], stride = [12608, 1, 64], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12824: 19365888 Bnn)
Operation_1653: operator = BatchedMatmulForward, inputs = [Tensor_12830, Tensor_12842], outputs = [Tensor_12844], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_12844: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12846: 59610624 Ban)
Operation_1655: operator = ReshapeForward, inputs = [Tensor_12844], outputs = [Tensor_12846], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 197 stride: 465708 stride: 38809 stride: 197 stride: 1]
Tensor_12846: shape = [128, 12, 197, 197], stride = [465708, 38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12846: 59610624 Bnn)
Operation_1663: operator = MaskedFillForward, inputs = [Tensor_12846, Tensor_11912], outputs = [Tensor_12848], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [value_float: -inf]
Tensor_12848: shape = [128, 12, 197, 197], stride = [465708, 38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12846: 59610624 Bnn)
Operation_1665: operator = ReshapeForward, inputs = [Tensor_12848], outputs = [Tensor_12850], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 197 stride: 38809 stride: 197 stride: 1]
Tensor_12850: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12846: 59610624 Bnn)
Operation_1667: operator = TypecastForward, inputs = [Tensor_12850], outputs = [Tensor_12852], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_12852: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11920: 119221248 Bnn)
Operation_1669: operator = SoftmaxForward, inputs = [Tensor_12852], outputs = [Tensor_12854], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dim: 2]
Tensor_12854: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12854: 119221248 Ban)
Operation_1671: operator = TypecastForward, inputs = [Tensor_12854], outputs = [Tensor_12856], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_12856: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12846: 59610624 Bnn)
Operation_1673: operator = BatchedMatmulForward, inputs = [Tensor_12856, Tensor_12840], outputs = [Tensor_12858], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_12858: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12836: 19365888 Bnn)
Operation_8369: operator = AllGatherDuplicated, inputs = [Tensor_12858], outputs = [Tensor_12861], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = all->this->1675
  - Operand: [tag: 8368]
Operation_12860: operator = Consume, inputs = [Tensor_12858], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_12861: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12868: 38731776 Ban)
Operation_1691: operator = TypecastForward, inputs = [Tensor_11660], outputs = [Tensor_12883], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Operation_1693: operator = TypecastForward, inputs = [Tensor_11661], outputs = [Tensor_12884], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Operation_1712: operator = ElemwiseUnaryForward, inputs = [Tensor_12794], outputs = [Tensor_12898], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 1]
Operation_1726: operator = TypecastForward, inputs = [Tensor_11666], outputs = [Tensor_12911], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Operation_1728: operator = TypecastForward, inputs = [Tensor_11667], outputs = [Tensor_12912], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Operation_1675: operator = ReshapeForward, inputs = [Tensor_12861], outputs = [Tensor_12868], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = 8369->this
  - Operand: [shape: 197 shape: 1536 shape: 64 stride: 64 stride: 12608 stride: 1]
Tensor_12868: shape = [197, 1536, 64], stride = [64, 12608, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12868: 38731776 Bnn)
Operation_1677: operator = ContiguousForward, inputs = [Tensor_12868], outputs = [Tensor_12870], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_12870: shape = [197, 1536, 64], stride = [98304, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12872: 38731776 Ban)
Operation_1681: operator = ReshapeForward, inputs = [Tensor_12870], outputs = [Tensor_12872], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 768 stride: 98304 stride: 1]
Tensor_12872: shape = [128, 197, 768], stride = [768, 98304, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12872: 38731776 Bnn)
Operation_1683: operator = TypecastForward, inputs = [Tensor_12872], outputs = [Tensor_12874], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_12874: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12874: 77463552 Ban)
Operation_8379: operator = DistributeWithMemAllocForward, inputs = [Tensor_12874], outputs = [Tensor_12876], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_12876: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12868: 38731776 Bnn)
Operation_1685: operator = LayernormForward, inputs = [Tensor_12876, Tensor_11662, Tensor_11663], outputs = [Tensor_12878, Tensor_12879, Tensor_12880], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_12878: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12872: 38731776 Bnn)
Tensor_12879: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12879: 50432 Ban)
Tensor_12880: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12880: 50432 Ban)
Operation_1689: operator = TypecastForward, inputs = [Tensor_12878], outputs = [Tensor_12882], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_12882: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12836: 19365888 Bnn)
Tensor_12883: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12883: 1179648 Ban)
Tensor_12884: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11821: 1536 Bnn)
Operation_1695: operator = LinearForward, inputs = [Tensor_12882, Tensor_12883, Tensor_12884], outputs = [Tensor_12886], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_12886: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12893: 19365888 Ban)
Tensor_12887: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12889: 128 Ban)
Operation_1702: operator = ElemwiseUnaryForward, inputs = [Tensor_12887], outputs = [Tensor_12889], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_ADD scal1_float: 0.918181837]
Tensor_12889: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12889: 128 Bnn)
Operation_1704: operator = ElemwiseUnaryForward, inputs = [Tensor_12889], outputs = [Tensor_12891], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_FLOOR]
Tensor_12891: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12889: 128 Bnn)
Operation_1706: operator = ElemwiseUnaryForward, inputs = [Tensor_12886], outputs = [Tensor_12893], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_DIV scal1_float: 0.918181837]
Tensor_12893: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12893: 19365888 Bnn)
Operation_1708: operator = ReshapeForward, inputs = [Tensor_12891], outputs = [Tensor_12895], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 1 stride: 0 stride: 0]
Tensor_12895: shape = [128, 197, 768], stride = [1, 0, 0], offset = 0, size = 128, datatype = float16, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12889: 128 Bnn)
Operation_1710: operator = ElemwiseBinaryForward, inputs = [Tensor_12893, Tensor_12895], outputs = [Tensor_12897], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_MUL]
Tensor_12897: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12897: 19365888 Ban)
Tensor_12898: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12902: 38731776 Ban)
Operation_1714: operator = TypecastForward, inputs = [Tensor_12897], outputs = [Tensor_12900], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_12900: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12868: 38731776 Bnn)
Operation_1716: operator = ElemwiseBinaryForward, inputs = [Tensor_12898, Tensor_12900], outputs = [Tensor_12902], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_12902: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12902: 38731776 Bnn)
Operation_1718: operator = LayernormForward, inputs = [Tensor_12902, Tensor_11672, Tensor_11673], outputs = [Tensor_12904, Tensor_12905, Tensor_12906], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_12904: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12868: 38731776 Bnn)
Tensor_12905: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12905: 50432 Ban)
Tensor_12906: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12906: 50432 Ban)
Operation_1722: operator = ReshapeForward, inputs = [Tensor_12904], outputs = [Tensor_12908], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 25216 shape: 768 stride: 768 stride: 1]
Tensor_12908: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12868: 38731776 Bnn)
Operation_1724: operator = TypecastForward, inputs = [Tensor_12908], outputs = [Tensor_12910], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_12910: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12897: 19365888 Bnn)
Tensor_12911: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12911: 4718592 Ban)
Tensor_12912: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11969: 6144 Bnn)
Operation_1730: operator = LinearForward, inputs = [Tensor_12910, Tensor_12911, Tensor_12912], outputs = [Tensor_12914], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_12914: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12914: 77463552 Ban)
Operation_1732: operator = TypecastForward, inputs = [Tensor_12914], outputs = [Tensor_12916], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_12916: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12916: 154927104 Ban)
Operation_1734: operator = ElemwiseUnaryForward, inputs = [Tensor_12916], outputs = [Tensor_12918], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_GELU]
Tensor_12918: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12918: 154927104 Ban)
Operation_1736: operator = TypecastForward, inputs = [Tensor_12918], outputs = [Tensor_12920], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_12920: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12914: 77463552 Bnn)
Operation_1738: operator = TypecastForward, inputs = [Tensor_12920], outputs = [Tensor_12922], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_12922: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12922: 154927104 Ban)
Operation_1740: operator = LayernormForward, inputs = [Tensor_12922, Tensor_11670, Tensor_11671], outputs = [Tensor_12924, Tensor_12925, Tensor_12926], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_12924: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12924: 154927104 Ban)
Tensor_12925: shape = [25216], stride = [1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12925: 50432 Ban)
Tensor_12926: shape = [25216], stride = [1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12926: 50432 Ban)
Operation_1744: operator = TypecastForward, inputs = [Tensor_12924], outputs = [Tensor_12928], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_12928: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12914: 77463552 Bnn)
Operation_1746: operator = TypecastForward, inputs = [Tensor_11668], outputs = [Tensor_12930], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_12930: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12930: 4718592 Ban)
Operation_1748: operator = TypecastForward, inputs = [Tensor_11669], outputs = [Tensor_12932], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_12932: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11821: 1536 Bnn)
Operation_1750: operator = LinearForward, inputs = [Tensor_12928, Tensor_12930, Tensor_12932], outputs = [Tensor_12934], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_12934: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12941: 19365888 Ban)
Tensor_12935: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12937: 128 Ban)
Operation_1755: operator = ElemwiseUnaryForward, inputs = [Tensor_12935], outputs = [Tensor_12937], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_ADD scal1_float: 0.918181837]
Tensor_12937: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12937: 128 Bnn)
Operation_1757: operator = ElemwiseUnaryForward, inputs = [Tensor_12937], outputs = [Tensor_12939], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_FLOOR]
Tensor_12939: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12937: 128 Bnn)
Operation_1752: operator = ReshapeForward, inputs = [Tensor_12934], outputs = [Tensor_12941], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 151296 stride: 768 stride: 1]
Tensor_12941: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12941: 19365888 Bnn)
Operation_1759: operator = ElemwiseUnaryForward, inputs = [Tensor_12941], outputs = [Tensor_12943], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_DIV scal1_float: 0.918181837]
Tensor_12943: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12941: 19365888 Bnn)
Operation_1761: operator = ReshapeForward, inputs = [Tensor_12939], outputs = [Tensor_12945], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 1 stride: 0 stride: 0]
Tensor_12945: shape = [128, 197, 768], stride = [1, 0, 0], offset = 0, size = 128, datatype = float16, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12937: 128 Bnn)
Operation_1763: operator = ElemwiseBinaryForward, inputs = [Tensor_12943, Tensor_12945], outputs = [Tensor_12947], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_MUL]
Tensor_12947: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12947: 19365888 Ban)
Operation_1765: operator = ElemwiseUnaryForward, inputs = [Tensor_12902], outputs = [Tensor_12949], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 1]
Tensor_12949: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12953: 38731776 Ban)
Operation_1767: operator = TypecastForward, inputs = [Tensor_12947], outputs = [Tensor_12951], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_12951: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12951: 38731776 Ban)
Operation_1769: operator = ElemwiseBinaryForward, inputs = [Tensor_12949, Tensor_12951], outputs = [Tensor_12953], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_12953: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12953: 38731776 Bnn)
Operation_1771: operator = LayernormForward, inputs = [Tensor_12953, Tensor_11684, Tensor_11685], outputs = [Tensor_12955, Tensor_12956, Tensor_12957], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_12955: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12951: 38731776 Bnn)
Tensor_12956: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12956: 50432 Ban)
Tensor_12957: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12957: 50432 Ban)
Operation_1775: operator = TypecastForward, inputs = [Tensor_12955], outputs = [Tensor_12959], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_12959: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12947: 19365888 Bnn)
Operation_1777: operator = TypecastForward, inputs = [Tensor_11678], outputs = [Tensor_12961], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_12961: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12961: 1179648 Ban)
Operation_1779: operator = TypecastForward, inputs = [Tensor_11679], outputs = [Tensor_12963], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_12963: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11821: 1536 Bnn)
Operation_1781: operator = LinearForward, inputs = [Tensor_12959, Tensor_12961, Tensor_12963], outputs = [Tensor_12965], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_12965: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12983: 19365888 Ban)
Operation_1783: operator = TypecastForward, inputs = [Tensor_12955], outputs = [Tensor_12967], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_12967: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12967: 19365888 Ban)
Operation_1785: operator = TypecastForward, inputs = [Tensor_11674], outputs = [Tensor_12969], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_12969: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12969: 1179648 Ban)
Operation_1787: operator = TypecastForward, inputs = [Tensor_11675], outputs = [Tensor_12971], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_12971: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11821: 1536 Bnn)
Operation_1789: operator = LinearForward, inputs = [Tensor_12967, Tensor_12969, Tensor_12971], outputs = [Tensor_12973], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_12973: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12991: 19365888 Ban)
Operation_1791: operator = TypecastForward, inputs = [Tensor_12955], outputs = [Tensor_12975], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_12975: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12975: 19365888 Ban)
Operation_1793: operator = TypecastForward, inputs = [Tensor_11676], outputs = [Tensor_12977], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_12977: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12977: 1179648 Ban)
Operation_1795: operator = TypecastForward, inputs = [Tensor_11677], outputs = [Tensor_12979], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_12979: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11821: 1536 Bnn)
Operation_1797: operator = LinearForward, inputs = [Tensor_12975, Tensor_12977, Tensor_12979], outputs = [Tensor_12981], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_12981: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12995: 19365888 Ban)
Operation_1799: operator = ElemwiseUnaryForward, inputs = [Tensor_12965], outputs = [Tensor_12983], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 0.125]
Tensor_12983: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12983: 19365888 Bnn)
Operation_1803: operator = ReshapeForward, inputs = [Tensor_12983], outputs = [Tensor_12985], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 64 stride: 151296 stride: 64 stride: 768 stride: 1]
Tensor_12985: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12983: 19365888 Bnn)
Operation_1813: operator = ContiguousForward, inputs = [Tensor_12985], outputs = [Tensor_12987], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_12987: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12989: 19365888 Ban)
Operation_1815: operator = ReshapeForward, inputs = [Tensor_12987], outputs = [Tensor_12989], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_12989: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12989: 19365888 Bnn)
Operation_1807: operator = ReshapeForward, inputs = [Tensor_12973], outputs = [Tensor_12991], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 64 stride: 151296 stride: 64 stride: 768 stride: 1]
Tensor_12991: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12991: 19365888 Bnn)
Operation_1817: operator = ContiguousForward, inputs = [Tensor_12991], outputs = [Tensor_12993], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_12993: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12983: 19365888 Bnn)
Operation_1811: operator = ReshapeForward, inputs = [Tensor_12981], outputs = [Tensor_12995], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 64 stride: 151296 stride: 64 stride: 768 stride: 1]
Tensor_12995: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12995: 19365888 Bnn)
Operation_1821: operator = ContiguousForward, inputs = [Tensor_12995], outputs = [Tensor_12997], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_12997: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12991: 19365888 Bnn)
Operation_1823: operator = ReshapeForward, inputs = [Tensor_12997], outputs = [Tensor_12999], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_12999: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12991: 19365888 Bnn)
Operation_1825: operator = ReshapeForward, inputs = [Tensor_12993], outputs = [Tensor_13001], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 64 shape: 197 stride: 12608 stride: 1 stride: 64]
Tensor_13001: shape = [1536, 64, 197], stride = [12608, 1, 64], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12983: 19365888 Bnn)
Operation_1827: operator = BatchedMatmulForward, inputs = [Tensor_12989, Tensor_13001], outputs = [Tensor_13003], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_13003: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13005: 59610624 Ban)
Operation_1829: operator = ReshapeForward, inputs = [Tensor_13003], outputs = [Tensor_13005], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 197 stride: 465708 stride: 38809 stride: 197 stride: 1]
Tensor_13005: shape = [128, 12, 197, 197], stride = [465708, 38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13005: 59610624 Bnn)
Operation_1837: operator = MaskedFillForward, inputs = [Tensor_13005, Tensor_11912], outputs = [Tensor_13007], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [value_float: -inf]
Tensor_13007: shape = [128, 12, 197, 197], stride = [465708, 38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13005: 59610624 Bnn)
Operation_1839: operator = ReshapeForward, inputs = [Tensor_13007], outputs = [Tensor_13009], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 197 stride: 38809 stride: 197 stride: 1]
Tensor_13009: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13005: 59610624 Bnn)
Operation_1841: operator = TypecastForward, inputs = [Tensor_13009], outputs = [Tensor_13011], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_13011: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11920: 119221248 Bnn)
Operation_1843: operator = SoftmaxForward, inputs = [Tensor_13011], outputs = [Tensor_13013], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dim: 2]
Tensor_13013: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13013: 119221248 Ban)
Operation_1845: operator = TypecastForward, inputs = [Tensor_13013], outputs = [Tensor_13015], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_13015: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13005: 59610624 Bnn)
Operation_1847: operator = BatchedMatmulForward, inputs = [Tensor_13015, Tensor_12999], outputs = [Tensor_13017], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_13017: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12995: 19365888 Bnn)
Operation_8528: operator = AllGatherDuplicated, inputs = [Tensor_13017], outputs = [Tensor_13020], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = all->this->1849
  - Operand: [tag: 8527]
Operation_13019: operator = Consume, inputs = [Tensor_13017], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_13020: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13027: 38731776 Ban)
Operation_1865: operator = TypecastForward, inputs = [Tensor_11680], outputs = [Tensor_13042], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Operation_1867: operator = TypecastForward, inputs = [Tensor_11681], outputs = [Tensor_13043], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Operation_1886: operator = ElemwiseUnaryForward, inputs = [Tensor_12953], outputs = [Tensor_13057], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 1]
Operation_1900: operator = TypecastForward, inputs = [Tensor_11686], outputs = [Tensor_13070], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Operation_1902: operator = TypecastForward, inputs = [Tensor_11687], outputs = [Tensor_13071], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Operation_1849: operator = ReshapeForward, inputs = [Tensor_13020], outputs = [Tensor_13027], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = 8528->this
  - Operand: [shape: 197 shape: 1536 shape: 64 stride: 64 stride: 12608 stride: 1]
Tensor_13027: shape = [197, 1536, 64], stride = [64, 12608, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13027: 38731776 Bnn)
Operation_1851: operator = ContiguousForward, inputs = [Tensor_13027], outputs = [Tensor_13029], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_13029: shape = [197, 1536, 64], stride = [98304, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13031: 38731776 Ban)
Operation_1855: operator = ReshapeForward, inputs = [Tensor_13029], outputs = [Tensor_13031], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 768 stride: 98304 stride: 1]
Tensor_13031: shape = [128, 197, 768], stride = [768, 98304, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13031: 38731776 Bnn)
Operation_1857: operator = TypecastForward, inputs = [Tensor_13031], outputs = [Tensor_13033], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_13033: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13033: 77463552 Ban)
Operation_8538: operator = DistributeWithMemAllocForward, inputs = [Tensor_13033], outputs = [Tensor_13035], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_13035: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13027: 38731776 Bnn)
Operation_1859: operator = LayernormForward, inputs = [Tensor_13035, Tensor_11682, Tensor_11683], outputs = [Tensor_13037, Tensor_13038, Tensor_13039], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_13037: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13031: 38731776 Bnn)
Tensor_13038: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13038: 50432 Ban)
Tensor_13039: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13039: 50432 Ban)
Operation_1863: operator = TypecastForward, inputs = [Tensor_13037], outputs = [Tensor_13041], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_13041: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12995: 19365888 Bnn)
Tensor_13042: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13042: 1179648 Ban)
Tensor_13043: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11821: 1536 Bnn)
Operation_1869: operator = LinearForward, inputs = [Tensor_13041, Tensor_13042, Tensor_13043], outputs = [Tensor_13045], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_13045: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13052: 19365888 Ban)
Tensor_13046: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13048: 128 Ban)
Operation_1876: operator = ElemwiseUnaryForward, inputs = [Tensor_13046], outputs = [Tensor_13048], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_ADD scal1_float: 0.904545426]
Tensor_13048: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13048: 128 Bnn)
Operation_1878: operator = ElemwiseUnaryForward, inputs = [Tensor_13048], outputs = [Tensor_13050], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_FLOOR]
Tensor_13050: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13048: 128 Bnn)
Operation_1880: operator = ElemwiseUnaryForward, inputs = [Tensor_13045], outputs = [Tensor_13052], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_DIV scal1_float: 0.904545426]
Tensor_13052: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13052: 19365888 Bnn)
Operation_1882: operator = ReshapeForward, inputs = [Tensor_13050], outputs = [Tensor_13054], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 1 stride: 0 stride: 0]
Tensor_13054: shape = [128, 197, 768], stride = [1, 0, 0], offset = 0, size = 128, datatype = float16, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13048: 128 Bnn)
Operation_1884: operator = ElemwiseBinaryForward, inputs = [Tensor_13052, Tensor_13054], outputs = [Tensor_13056], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_MUL]
Tensor_13056: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13056: 19365888 Ban)
Tensor_13057: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13061: 38731776 Ban)
Operation_1888: operator = TypecastForward, inputs = [Tensor_13056], outputs = [Tensor_13059], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_13059: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13027: 38731776 Bnn)
Operation_1890: operator = ElemwiseBinaryForward, inputs = [Tensor_13057, Tensor_13059], outputs = [Tensor_13061], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_13061: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13061: 38731776 Bnn)
Operation_1892: operator = LayernormForward, inputs = [Tensor_13061, Tensor_11692, Tensor_11693], outputs = [Tensor_13063, Tensor_13064, Tensor_13065], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_13063: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13027: 38731776 Bnn)
Tensor_13064: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13064: 50432 Ban)
Tensor_13065: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13065: 50432 Ban)
Operation_1896: operator = ReshapeForward, inputs = [Tensor_13063], outputs = [Tensor_13067], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 25216 shape: 768 stride: 768 stride: 1]
Tensor_13067: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13027: 38731776 Bnn)
Operation_1898: operator = TypecastForward, inputs = [Tensor_13067], outputs = [Tensor_13069], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_13069: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13056: 19365888 Bnn)
Tensor_13070: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13070: 4718592 Ban)
Tensor_13071: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11969: 6144 Bnn)
Operation_1904: operator = LinearForward, inputs = [Tensor_13069, Tensor_13070, Tensor_13071], outputs = [Tensor_13073], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_13073: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13073: 77463552 Ban)
Operation_1906: operator = TypecastForward, inputs = [Tensor_13073], outputs = [Tensor_13075], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_13075: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13075: 154927104 Ban)
Operation_1908: operator = ElemwiseUnaryForward, inputs = [Tensor_13075], outputs = [Tensor_13077], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_GELU]
Tensor_13077: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13077: 154927104 Ban)
Operation_1910: operator = TypecastForward, inputs = [Tensor_13077], outputs = [Tensor_13079], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_13079: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13073: 77463552 Bnn)
Operation_1912: operator = TypecastForward, inputs = [Tensor_13079], outputs = [Tensor_13081], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_13081: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13081: 154927104 Ban)
Operation_1914: operator = LayernormForward, inputs = [Tensor_13081, Tensor_11690, Tensor_11691], outputs = [Tensor_13083, Tensor_13084, Tensor_13085], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_13083: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13083: 154927104 Ban)
Tensor_13084: shape = [25216], stride = [1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13084: 50432 Ban)
Tensor_13085: shape = [25216], stride = [1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13085: 50432 Ban)
Operation_1918: operator = TypecastForward, inputs = [Tensor_13083], outputs = [Tensor_13087], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_13087: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13073: 77463552 Bnn)
Operation_1920: operator = TypecastForward, inputs = [Tensor_11688], outputs = [Tensor_13089], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_13089: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13089: 4718592 Ban)
Operation_1922: operator = TypecastForward, inputs = [Tensor_11689], outputs = [Tensor_13091], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_13091: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11821: 1536 Bnn)
Operation_1924: operator = LinearForward, inputs = [Tensor_13087, Tensor_13089, Tensor_13091], outputs = [Tensor_13093], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_13093: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13100: 19365888 Ban)
Tensor_13094: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13096: 128 Ban)
Operation_1929: operator = ElemwiseUnaryForward, inputs = [Tensor_13094], outputs = [Tensor_13096], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_ADD scal1_float: 0.904545426]
Tensor_13096: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13096: 128 Bnn)
Operation_1931: operator = ElemwiseUnaryForward, inputs = [Tensor_13096], outputs = [Tensor_13098], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_FLOOR]
Tensor_13098: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13096: 128 Bnn)
Operation_1926: operator = ReshapeForward, inputs = [Tensor_13093], outputs = [Tensor_13100], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 151296 stride: 768 stride: 1]
Tensor_13100: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13100: 19365888 Bnn)
Operation_1933: operator = ElemwiseUnaryForward, inputs = [Tensor_13100], outputs = [Tensor_13102], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_DIV scal1_float: 0.904545426]
Tensor_13102: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13100: 19365888 Bnn)
Operation_1935: operator = ReshapeForward, inputs = [Tensor_13098], outputs = [Tensor_13104], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 1 stride: 0 stride: 0]
Tensor_13104: shape = [128, 197, 768], stride = [1, 0, 0], offset = 0, size = 128, datatype = float16, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13096: 128 Bnn)
Operation_1937: operator = ElemwiseBinaryForward, inputs = [Tensor_13102, Tensor_13104], outputs = [Tensor_13106], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_MUL]
Tensor_13106: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13106: 19365888 Ban)
Operation_1939: operator = ElemwiseUnaryForward, inputs = [Tensor_13061], outputs = [Tensor_13108], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 1]
Tensor_13108: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13112: 38731776 Ban)
Operation_1941: operator = TypecastForward, inputs = [Tensor_13106], outputs = [Tensor_13110], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_13110: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13110: 38731776 Ban)
Operation_1943: operator = ElemwiseBinaryForward, inputs = [Tensor_13108, Tensor_13110], outputs = [Tensor_13112], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_13112: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13112: 38731776 Bnn)
Operation_1945: operator = LayernormForward, inputs = [Tensor_13112, Tensor_11704, Tensor_11705], outputs = [Tensor_13114, Tensor_13115, Tensor_13116], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_13114: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13110: 38731776 Bnn)
Tensor_13115: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13115: 50432 Ban)
Tensor_13116: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13116: 50432 Ban)
Operation_1949: operator = TypecastForward, inputs = [Tensor_13114], outputs = [Tensor_13118], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_13118: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13106: 19365888 Bnn)
Operation_1951: operator = TypecastForward, inputs = [Tensor_11698], outputs = [Tensor_13120], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_13120: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13120: 1179648 Ban)
Operation_1953: operator = TypecastForward, inputs = [Tensor_11699], outputs = [Tensor_13122], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_13122: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11821: 1536 Bnn)
Operation_1955: operator = LinearForward, inputs = [Tensor_13118, Tensor_13120, Tensor_13122], outputs = [Tensor_13124], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_13124: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13142: 19365888 Ban)
Operation_1957: operator = TypecastForward, inputs = [Tensor_13114], outputs = [Tensor_13126], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_13126: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13126: 19365888 Ban)
Operation_1959: operator = TypecastForward, inputs = [Tensor_11694], outputs = [Tensor_13128], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_13128: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13128: 1179648 Ban)
Operation_1961: operator = TypecastForward, inputs = [Tensor_11695], outputs = [Tensor_13130], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_13130: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11821: 1536 Bnn)
Operation_1963: operator = LinearForward, inputs = [Tensor_13126, Tensor_13128, Tensor_13130], outputs = [Tensor_13132], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_13132: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13150: 19365888 Ban)
Operation_1965: operator = TypecastForward, inputs = [Tensor_13114], outputs = [Tensor_13134], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_13134: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13134: 19365888 Ban)
Operation_1967: operator = TypecastForward, inputs = [Tensor_11696], outputs = [Tensor_13136], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_13136: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13136: 1179648 Ban)
Operation_1969: operator = TypecastForward, inputs = [Tensor_11697], outputs = [Tensor_13138], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_13138: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11821: 1536 Bnn)
Operation_1971: operator = LinearForward, inputs = [Tensor_13134, Tensor_13136, Tensor_13138], outputs = [Tensor_13140], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_13140: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13154: 19365888 Ban)
Operation_1973: operator = ElemwiseUnaryForward, inputs = [Tensor_13124], outputs = [Tensor_13142], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 0.125]
Tensor_13142: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13142: 19365888 Bnn)
Operation_1977: operator = ReshapeForward, inputs = [Tensor_13142], outputs = [Tensor_13144], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 64 stride: 151296 stride: 64 stride: 768 stride: 1]
Tensor_13144: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13142: 19365888 Bnn)
Operation_1987: operator = ContiguousForward, inputs = [Tensor_13144], outputs = [Tensor_13146], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_13146: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13148: 19365888 Ban)
Operation_1989: operator = ReshapeForward, inputs = [Tensor_13146], outputs = [Tensor_13148], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_13148: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13148: 19365888 Bnn)
Operation_1981: operator = ReshapeForward, inputs = [Tensor_13132], outputs = [Tensor_13150], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 64 stride: 151296 stride: 64 stride: 768 stride: 1]
Tensor_13150: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13150: 19365888 Bnn)
Operation_1991: operator = ContiguousForward, inputs = [Tensor_13150], outputs = [Tensor_13152], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_13152: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13142: 19365888 Bnn)
Operation_1985: operator = ReshapeForward, inputs = [Tensor_13140], outputs = [Tensor_13154], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 64 stride: 151296 stride: 64 stride: 768 stride: 1]
Tensor_13154: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13154: 19365888 Bnn)
Operation_1995: operator = ContiguousForward, inputs = [Tensor_13154], outputs = [Tensor_13156], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_13156: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13150: 19365888 Bnn)
Operation_1997: operator = ReshapeForward, inputs = [Tensor_13156], outputs = [Tensor_13158], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_13158: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13150: 19365888 Bnn)
Operation_1999: operator = ReshapeForward, inputs = [Tensor_13152], outputs = [Tensor_13160], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 64 shape: 197 stride: 12608 stride: 1 stride: 64]
Tensor_13160: shape = [1536, 64, 197], stride = [12608, 1, 64], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13142: 19365888 Bnn)
Operation_2001: operator = BatchedMatmulForward, inputs = [Tensor_13148, Tensor_13160], outputs = [Tensor_13162], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_13162: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13164: 59610624 Ban)
Operation_2003: operator = ReshapeForward, inputs = [Tensor_13162], outputs = [Tensor_13164], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 197 stride: 465708 stride: 38809 stride: 197 stride: 1]
Tensor_13164: shape = [128, 12, 197, 197], stride = [465708, 38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13164: 59610624 Bnn)
Operation_2011: operator = MaskedFillForward, inputs = [Tensor_13164, Tensor_11912], outputs = [Tensor_13166], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [value_float: -inf]
Tensor_13166: shape = [128, 12, 197, 197], stride = [465708, 38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13164: 59610624 Bnn)
Operation_2013: operator = ReshapeForward, inputs = [Tensor_13166], outputs = [Tensor_13168], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 197 stride: 38809 stride: 197 stride: 1]
Tensor_13168: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13164: 59610624 Bnn)
Operation_2015: operator = TypecastForward, inputs = [Tensor_13168], outputs = [Tensor_13170], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_13170: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11920: 119221248 Bnn)
Operation_2017: operator = SoftmaxForward, inputs = [Tensor_13170], outputs = [Tensor_13172], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dim: 2]
Tensor_13172: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13172: 119221248 Ban)
Operation_2019: operator = TypecastForward, inputs = [Tensor_13172], outputs = [Tensor_13174], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_13174: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13164: 59610624 Bnn)
Operation_2021: operator = BatchedMatmulForward, inputs = [Tensor_13174, Tensor_13158], outputs = [Tensor_13176], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_13176: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13154: 19365888 Bnn)
Operation_8687: operator = AllGatherDuplicated, inputs = [Tensor_13176], outputs = [Tensor_13179], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = all->this->2023
  - Operand: [tag: 8686]
Operation_13178: operator = Consume, inputs = [Tensor_13176], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_13179: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13186: 38731776 Ban)
Operation_2039: operator = TypecastForward, inputs = [Tensor_11700], outputs = [Tensor_13201], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Operation_2041: operator = TypecastForward, inputs = [Tensor_11701], outputs = [Tensor_13202], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Operation_2060: operator = ElemwiseUnaryForward, inputs = [Tensor_13112], outputs = [Tensor_13216], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 1]
Operation_2074: operator = TypecastForward, inputs = [Tensor_11706], outputs = [Tensor_13229], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Operation_2076: operator = TypecastForward, inputs = [Tensor_11707], outputs = [Tensor_13230], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Operation_2023: operator = ReshapeForward, inputs = [Tensor_13179], outputs = [Tensor_13186], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = 8687->this
  - Operand: [shape: 197 shape: 1536 shape: 64 stride: 64 stride: 12608 stride: 1]
Tensor_13186: shape = [197, 1536, 64], stride = [64, 12608, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13186: 38731776 Bnn)
Operation_2025: operator = ContiguousForward, inputs = [Tensor_13186], outputs = [Tensor_13188], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_13188: shape = [197, 1536, 64], stride = [98304, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13190: 38731776 Ban)
Operation_2029: operator = ReshapeForward, inputs = [Tensor_13188], outputs = [Tensor_13190], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 768 stride: 98304 stride: 1]
Tensor_13190: shape = [128, 197, 768], stride = [768, 98304, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13190: 38731776 Bnn)
Operation_2031: operator = TypecastForward, inputs = [Tensor_13190], outputs = [Tensor_13192], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_13192: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13192: 77463552 Ban)
Operation_8697: operator = DistributeWithMemAllocForward, inputs = [Tensor_13192], outputs = [Tensor_13194], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_13194: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13186: 38731776 Bnn)
Operation_2033: operator = LayernormForward, inputs = [Tensor_13194, Tensor_11702, Tensor_11703], outputs = [Tensor_13196, Tensor_13197, Tensor_13198], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_13196: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13190: 38731776 Bnn)
Tensor_13197: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13197: 50432 Ban)
Tensor_13198: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13198: 50432 Ban)
Operation_2037: operator = TypecastForward, inputs = [Tensor_13196], outputs = [Tensor_13200], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_13200: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13154: 19365888 Bnn)
Tensor_13201: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13201: 1179648 Ban)
Tensor_13202: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11821: 1536 Bnn)
Operation_2043: operator = LinearForward, inputs = [Tensor_13200, Tensor_13201, Tensor_13202], outputs = [Tensor_13204], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_13204: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13211: 19365888 Ban)
Tensor_13205: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13207: 128 Ban)
Operation_2050: operator = ElemwiseUnaryForward, inputs = [Tensor_13205], outputs = [Tensor_13207], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_ADD scal1_float: 0.890909076]
Tensor_13207: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13207: 128 Bnn)
Operation_2052: operator = ElemwiseUnaryForward, inputs = [Tensor_13207], outputs = [Tensor_13209], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_FLOOR]
Tensor_13209: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13207: 128 Bnn)
Operation_2054: operator = ElemwiseUnaryForward, inputs = [Tensor_13204], outputs = [Tensor_13211], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_DIV scal1_float: 0.890909076]
Tensor_13211: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13211: 19365888 Bnn)
Operation_2056: operator = ReshapeForward, inputs = [Tensor_13209], outputs = [Tensor_13213], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 1 stride: 0 stride: 0]
Tensor_13213: shape = [128, 197, 768], stride = [1, 0, 0], offset = 0, size = 128, datatype = float16, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13207: 128 Bnn)
Operation_2058: operator = ElemwiseBinaryForward, inputs = [Tensor_13211, Tensor_13213], outputs = [Tensor_13215], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_MUL]
Tensor_13215: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13215: 19365888 Ban)
Tensor_13216: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13220: 38731776 Ban)
Operation_2062: operator = TypecastForward, inputs = [Tensor_13215], outputs = [Tensor_13218], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_13218: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13186: 38731776 Bnn)
Operation_2064: operator = ElemwiseBinaryForward, inputs = [Tensor_13216, Tensor_13218], outputs = [Tensor_13220], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_13220: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13220: 38731776 Bnn)
Operation_2066: operator = LayernormForward, inputs = [Tensor_13220, Tensor_11712, Tensor_11713], outputs = [Tensor_13222, Tensor_13223, Tensor_13224], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_13222: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13186: 38731776 Bnn)
Tensor_13223: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13223: 50432 Ban)
Tensor_13224: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13224: 50432 Ban)
Operation_2070: operator = ReshapeForward, inputs = [Tensor_13222], outputs = [Tensor_13226], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 25216 shape: 768 stride: 768 stride: 1]
Tensor_13226: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13186: 38731776 Bnn)
Operation_2072: operator = TypecastForward, inputs = [Tensor_13226], outputs = [Tensor_13228], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_13228: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13215: 19365888 Bnn)
Tensor_13229: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13229: 4718592 Ban)
Tensor_13230: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11969: 6144 Bnn)
Operation_2078: operator = LinearForward, inputs = [Tensor_13228, Tensor_13229, Tensor_13230], outputs = [Tensor_13232], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_13232: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13232: 77463552 Ban)
Operation_2080: operator = TypecastForward, inputs = [Tensor_13232], outputs = [Tensor_13234], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_13234: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13234: 154927104 Ban)
Operation_2082: operator = ElemwiseUnaryForward, inputs = [Tensor_13234], outputs = [Tensor_13236], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_GELU]
Tensor_13236: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13236: 154927104 Ban)
Operation_2084: operator = TypecastForward, inputs = [Tensor_13236], outputs = [Tensor_13238], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_13238: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13232: 77463552 Bnn)
Operation_2086: operator = TypecastForward, inputs = [Tensor_13238], outputs = [Tensor_13240], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_13240: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13240: 154927104 Ban)
Operation_2088: operator = LayernormForward, inputs = [Tensor_13240, Tensor_11710, Tensor_11711], outputs = [Tensor_13242, Tensor_13243, Tensor_13244], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_13242: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13242: 154927104 Ban)
Tensor_13243: shape = [25216], stride = [1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13243: 50432 Ban)
Tensor_13244: shape = [25216], stride = [1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13244: 50432 Ban)
Operation_2092: operator = TypecastForward, inputs = [Tensor_13242], outputs = [Tensor_13246], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_13246: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13232: 77463552 Bnn)
Operation_2094: operator = TypecastForward, inputs = [Tensor_11708], outputs = [Tensor_13248], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_13248: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13248: 4718592 Ban)
Operation_2096: operator = TypecastForward, inputs = [Tensor_11709], outputs = [Tensor_13250], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_13250: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11821: 1536 Bnn)
Operation_2098: operator = LinearForward, inputs = [Tensor_13246, Tensor_13248, Tensor_13250], outputs = [Tensor_13252], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_13252: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13259: 19365888 Ban)
Tensor_13253: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13255: 128 Ban)
Operation_2103: operator = ElemwiseUnaryForward, inputs = [Tensor_13253], outputs = [Tensor_13255], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_ADD scal1_float: 0.890909076]
Tensor_13255: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13255: 128 Bnn)
Operation_2105: operator = ElemwiseUnaryForward, inputs = [Tensor_13255], outputs = [Tensor_13257], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_FLOOR]
Tensor_13257: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13255: 128 Bnn)
Operation_2100: operator = ReshapeForward, inputs = [Tensor_13252], outputs = [Tensor_13259], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 151296 stride: 768 stride: 1]
Tensor_13259: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13259: 19365888 Bnn)
Operation_2107: operator = ElemwiseUnaryForward, inputs = [Tensor_13259], outputs = [Tensor_13261], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_DIV scal1_float: 0.890909076]
Tensor_13261: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13259: 19365888 Bnn)
Operation_2109: operator = ReshapeForward, inputs = [Tensor_13257], outputs = [Tensor_13263], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 1 stride: 0 stride: 0]
Tensor_13263: shape = [128, 197, 768], stride = [1, 0, 0], offset = 0, size = 128, datatype = float16, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13255: 128 Bnn)
Operation_2111: operator = ElemwiseBinaryForward, inputs = [Tensor_13261, Tensor_13263], outputs = [Tensor_13265], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_MUL]
Tensor_13265: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13265: 19365888 Ban)
Operation_2113: operator = ElemwiseUnaryForward, inputs = [Tensor_13220], outputs = [Tensor_13267], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 1]
Tensor_13267: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13271: 38731776 Ban)
Operation_2115: operator = TypecastForward, inputs = [Tensor_13265], outputs = [Tensor_13269], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_13269: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13269: 38731776 Ban)
Operation_2117: operator = ElemwiseBinaryForward, inputs = [Tensor_13267, Tensor_13269], outputs = [Tensor_13271], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_13271: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13271: 38731776 Bnn)
Operation_2119: operator = LayernormForward, inputs = [Tensor_13271, Tensor_11724, Tensor_11725], outputs = [Tensor_13273, Tensor_13274, Tensor_13275], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_13273: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13269: 38731776 Bnn)
Tensor_13274: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13274: 50432 Ban)
Tensor_13275: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13275: 50432 Ban)
Operation_2123: operator = TypecastForward, inputs = [Tensor_13273], outputs = [Tensor_13277], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_13277: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13265: 19365888 Bnn)
Operation_2125: operator = TypecastForward, inputs = [Tensor_11718], outputs = [Tensor_13279], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_13279: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13279: 1179648 Ban)
Operation_2127: operator = TypecastForward, inputs = [Tensor_11719], outputs = [Tensor_13281], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_13281: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11821: 1536 Bnn)
Operation_2129: operator = LinearForward, inputs = [Tensor_13277, Tensor_13279, Tensor_13281], outputs = [Tensor_13283], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_13283: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13301: 19365888 Ban)
Operation_2131: operator = TypecastForward, inputs = [Tensor_13273], outputs = [Tensor_13285], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_13285: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13285: 19365888 Ban)
Operation_2133: operator = TypecastForward, inputs = [Tensor_11714], outputs = [Tensor_13287], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_13287: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13287: 1179648 Ban)
Operation_2135: operator = TypecastForward, inputs = [Tensor_11715], outputs = [Tensor_13289], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_13289: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11821: 1536 Bnn)
Operation_2137: operator = LinearForward, inputs = [Tensor_13285, Tensor_13287, Tensor_13289], outputs = [Tensor_13291], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_13291: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13309: 19365888 Ban)
Operation_2139: operator = TypecastForward, inputs = [Tensor_13273], outputs = [Tensor_13293], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_13293: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13293: 19365888 Ban)
Operation_2141: operator = TypecastForward, inputs = [Tensor_11716], outputs = [Tensor_13295], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_13295: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13295: 1179648 Ban)
Operation_2143: operator = TypecastForward, inputs = [Tensor_11717], outputs = [Tensor_13297], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_13297: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11821: 1536 Bnn)
Operation_2145: operator = LinearForward, inputs = [Tensor_13293, Tensor_13295, Tensor_13297], outputs = [Tensor_13299], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_13299: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13313: 19365888 Ban)
Operation_2147: operator = ElemwiseUnaryForward, inputs = [Tensor_13283], outputs = [Tensor_13301], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 0.125]
Tensor_13301: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13301: 19365888 Bnn)
Operation_2151: operator = ReshapeForward, inputs = [Tensor_13301], outputs = [Tensor_13303], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 64 stride: 151296 stride: 64 stride: 768 stride: 1]
Tensor_13303: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13301: 19365888 Bnn)
Operation_2161: operator = ContiguousForward, inputs = [Tensor_13303], outputs = [Tensor_13305], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_13305: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13307: 19365888 Ban)
Operation_2163: operator = ReshapeForward, inputs = [Tensor_13305], outputs = [Tensor_13307], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_13307: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13307: 19365888 Bnn)
Operation_2155: operator = ReshapeForward, inputs = [Tensor_13291], outputs = [Tensor_13309], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 64 stride: 151296 stride: 64 stride: 768 stride: 1]
Tensor_13309: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13309: 19365888 Bnn)
Operation_2165: operator = ContiguousForward, inputs = [Tensor_13309], outputs = [Tensor_13311], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_13311: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13301: 19365888 Bnn)
Operation_2159: operator = ReshapeForward, inputs = [Tensor_13299], outputs = [Tensor_13313], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 64 stride: 151296 stride: 64 stride: 768 stride: 1]
Tensor_13313: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13313: 19365888 Bnn)
Operation_2169: operator = ContiguousForward, inputs = [Tensor_13313], outputs = [Tensor_13315], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_13315: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13309: 19365888 Bnn)
Operation_2171: operator = ReshapeForward, inputs = [Tensor_13315], outputs = [Tensor_13317], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_13317: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13309: 19365888 Bnn)
Operation_2173: operator = ReshapeForward, inputs = [Tensor_13311], outputs = [Tensor_13319], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 64 shape: 197 stride: 12608 stride: 1 stride: 64]
Tensor_13319: shape = [1536, 64, 197], stride = [12608, 1, 64], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13301: 19365888 Bnn)
Operation_2175: operator = BatchedMatmulForward, inputs = [Tensor_13307, Tensor_13319], outputs = [Tensor_13321], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_13321: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13323: 59610624 Ban)
Operation_2177: operator = ReshapeForward, inputs = [Tensor_13321], outputs = [Tensor_13323], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 197 stride: 465708 stride: 38809 stride: 197 stride: 1]
Tensor_13323: shape = [128, 12, 197, 197], stride = [465708, 38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13323: 59610624 Bnn)
Operation_2185: operator = MaskedFillForward, inputs = [Tensor_13323, Tensor_11912], outputs = [Tensor_13325], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [value_float: -inf]
Tensor_13325: shape = [128, 12, 197, 197], stride = [465708, 38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13323: 59610624 Bnn)
Operation_2187: operator = ReshapeForward, inputs = [Tensor_13325], outputs = [Tensor_13327], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 197 stride: 38809 stride: 197 stride: 1]
Tensor_13327: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13323: 59610624 Bnn)
Operation_2189: operator = TypecastForward, inputs = [Tensor_13327], outputs = [Tensor_13329], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_13329: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11920: 119221248 Bnn)
Operation_2191: operator = SoftmaxForward, inputs = [Tensor_13329], outputs = [Tensor_13331], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dim: 2]
Tensor_13331: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13331: 119221248 Ban)
Operation_2193: operator = TypecastForward, inputs = [Tensor_13331], outputs = [Tensor_13333], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_13333: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13323: 59610624 Bnn)
Operation_2195: operator = BatchedMatmulForward, inputs = [Tensor_13333, Tensor_13317], outputs = [Tensor_13335], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_13335: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13313: 19365888 Bnn)
Operation_8846: operator = AllGatherDuplicated, inputs = [Tensor_13335], outputs = [Tensor_13338], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = all->this->2197
  - Operand: [tag: 8845]
Operation_13337: operator = Consume, inputs = [Tensor_13335], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_13338: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13345: 38731776 Ban)
Operation_2213: operator = TypecastForward, inputs = [Tensor_11720], outputs = [Tensor_13360], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Operation_2215: operator = TypecastForward, inputs = [Tensor_11721], outputs = [Tensor_13361], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Operation_2234: operator = ElemwiseUnaryForward, inputs = [Tensor_13271], outputs = [Tensor_13375], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 1]
Operation_2248: operator = TypecastForward, inputs = [Tensor_11726], outputs = [Tensor_13388], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Operation_2250: operator = TypecastForward, inputs = [Tensor_11727], outputs = [Tensor_13389], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Operation_2197: operator = ReshapeForward, inputs = [Tensor_13338], outputs = [Tensor_13345], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = 8846->this
  - Operand: [shape: 197 shape: 1536 shape: 64 stride: 64 stride: 12608 stride: 1]
Tensor_13345: shape = [197, 1536, 64], stride = [64, 12608, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13345: 38731776 Bnn)
Operation_2199: operator = ContiguousForward, inputs = [Tensor_13345], outputs = [Tensor_13347], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_13347: shape = [197, 1536, 64], stride = [98304, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13349: 38731776 Ban)
Operation_2203: operator = ReshapeForward, inputs = [Tensor_13347], outputs = [Tensor_13349], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 768 stride: 98304 stride: 1]
Tensor_13349: shape = [128, 197, 768], stride = [768, 98304, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13349: 38731776 Bnn)
Operation_2205: operator = TypecastForward, inputs = [Tensor_13349], outputs = [Tensor_13351], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_13351: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13351: 77463552 Ban)
Operation_8856: operator = DistributeWithMemAllocForward, inputs = [Tensor_13351], outputs = [Tensor_13353], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_13353: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13345: 38731776 Bnn)
Operation_2207: operator = LayernormForward, inputs = [Tensor_13353, Tensor_11722, Tensor_11723], outputs = [Tensor_13355, Tensor_13356, Tensor_13357], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_13355: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13349: 38731776 Bnn)
Tensor_13356: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13356: 50432 Ban)
Tensor_13357: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13357: 50432 Ban)
Operation_2211: operator = TypecastForward, inputs = [Tensor_13355], outputs = [Tensor_13359], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_13359: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13313: 19365888 Bnn)
Tensor_13360: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13360: 1179648 Ban)
Tensor_13361: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11821: 1536 Bnn)
Operation_2217: operator = LinearForward, inputs = [Tensor_13359, Tensor_13360, Tensor_13361], outputs = [Tensor_13363], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_13363: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13370: 19365888 Ban)
Tensor_13364: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13366: 128 Ban)
Operation_2224: operator = ElemwiseUnaryForward, inputs = [Tensor_13364], outputs = [Tensor_13366], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_ADD scal1_float: 0.877272725]
Tensor_13366: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13366: 128 Bnn)
Operation_2226: operator = ElemwiseUnaryForward, inputs = [Tensor_13366], outputs = [Tensor_13368], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_FLOOR]
Tensor_13368: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13366: 128 Bnn)
Operation_2228: operator = ElemwiseUnaryForward, inputs = [Tensor_13363], outputs = [Tensor_13370], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_DIV scal1_float: 0.877272725]
Tensor_13370: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13370: 19365888 Bnn)
Operation_2230: operator = ReshapeForward, inputs = [Tensor_13368], outputs = [Tensor_13372], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 1 stride: 0 stride: 0]
Tensor_13372: shape = [128, 197, 768], stride = [1, 0, 0], offset = 0, size = 128, datatype = float16, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13366: 128 Bnn)
Operation_2232: operator = ElemwiseBinaryForward, inputs = [Tensor_13370, Tensor_13372], outputs = [Tensor_13374], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_MUL]
Tensor_13374: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13374: 19365888 Ban)
Tensor_13375: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13379: 38731776 Ban)
Operation_2236: operator = TypecastForward, inputs = [Tensor_13374], outputs = [Tensor_13377], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_13377: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13345: 38731776 Bnn)
Operation_2238: operator = ElemwiseBinaryForward, inputs = [Tensor_13375, Tensor_13377], outputs = [Tensor_13379], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_13379: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13379: 38731776 Bnn)
Operation_2240: operator = LayernormForward, inputs = [Tensor_13379, Tensor_11732, Tensor_11733], outputs = [Tensor_13381, Tensor_13382, Tensor_13383], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_13381: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13345: 38731776 Bnn)
Tensor_13382: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13382: 50432 Ban)
Tensor_13383: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13383: 50432 Ban)
Operation_2244: operator = ReshapeForward, inputs = [Tensor_13381], outputs = [Tensor_13385], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 25216 shape: 768 stride: 768 stride: 1]
Tensor_13385: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13345: 38731776 Bnn)
Operation_2246: operator = TypecastForward, inputs = [Tensor_13385], outputs = [Tensor_13387], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_13387: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13374: 19365888 Bnn)
Tensor_13388: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13388: 4718592 Ban)
Tensor_13389: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11969: 6144 Bnn)
Operation_2252: operator = LinearForward, inputs = [Tensor_13387, Tensor_13388, Tensor_13389], outputs = [Tensor_13391], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_13391: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13391: 77463552 Ban)
Operation_2254: operator = TypecastForward, inputs = [Tensor_13391], outputs = [Tensor_13393], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_13393: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13393: 154927104 Ban)
Operation_2256: operator = ElemwiseUnaryForward, inputs = [Tensor_13393], outputs = [Tensor_13395], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_GELU]
Tensor_13395: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13395: 154927104 Ban)
Operation_2258: operator = TypecastForward, inputs = [Tensor_13395], outputs = [Tensor_13397], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_13397: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13391: 77463552 Bnn)
Operation_2260: operator = TypecastForward, inputs = [Tensor_13397], outputs = [Tensor_13399], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_13399: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13399: 154927104 Ban)
Operation_2262: operator = LayernormForward, inputs = [Tensor_13399, Tensor_11730, Tensor_11731], outputs = [Tensor_13401, Tensor_13402, Tensor_13403], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_13401: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13401: 154927104 Ban)
Tensor_13402: shape = [25216], stride = [1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13402: 50432 Ban)
Tensor_13403: shape = [25216], stride = [1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13403: 50432 Ban)
Operation_2266: operator = TypecastForward, inputs = [Tensor_13401], outputs = [Tensor_13405], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_13405: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13391: 77463552 Bnn)
Operation_2268: operator = TypecastForward, inputs = [Tensor_11728], outputs = [Tensor_13407], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_13407: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13407: 4718592 Ban)
Operation_2270: operator = TypecastForward, inputs = [Tensor_11729], outputs = [Tensor_13409], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_13409: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11821: 1536 Bnn)
Operation_2272: operator = LinearForward, inputs = [Tensor_13405, Tensor_13407, Tensor_13409], outputs = [Tensor_13411], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_13411: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13418: 19365888 Ban)
Tensor_13412: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13414: 128 Ban)
Operation_2277: operator = ElemwiseUnaryForward, inputs = [Tensor_13412], outputs = [Tensor_13414], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_ADD scal1_float: 0.877272725]
Tensor_13414: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13414: 128 Bnn)
Operation_2279: operator = ElemwiseUnaryForward, inputs = [Tensor_13414], outputs = [Tensor_13416], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_FLOOR]
Tensor_13416: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13414: 128 Bnn)
Operation_2274: operator = ReshapeForward, inputs = [Tensor_13411], outputs = [Tensor_13418], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 151296 stride: 768 stride: 1]
Tensor_13418: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13418: 19365888 Bnn)
Operation_2281: operator = ElemwiseUnaryForward, inputs = [Tensor_13418], outputs = [Tensor_13420], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_DIV scal1_float: 0.877272725]
Tensor_13420: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13418: 19365888 Bnn)
Operation_2283: operator = ReshapeForward, inputs = [Tensor_13416], outputs = [Tensor_13422], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 1 stride: 0 stride: 0]
Tensor_13422: shape = [128, 197, 768], stride = [1, 0, 0], offset = 0, size = 128, datatype = float16, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13414: 128 Bnn)
Operation_2285: operator = ElemwiseBinaryForward, inputs = [Tensor_13420, Tensor_13422], outputs = [Tensor_13424], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_MUL]
Tensor_13424: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13424: 19365888 Ban)
Operation_2287: operator = ElemwiseUnaryForward, inputs = [Tensor_13379], outputs = [Tensor_13426], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 1]
Tensor_13426: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13430: 38731776 Ban)
Operation_2289: operator = TypecastForward, inputs = [Tensor_13424], outputs = [Tensor_13428], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_13428: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13428: 38731776 Ban)
Operation_2291: operator = ElemwiseBinaryForward, inputs = [Tensor_13426, Tensor_13428], outputs = [Tensor_13430], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_13430: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13430: 38731776 Bnn)
Operation_2293: operator = LayernormForward, inputs = [Tensor_13430, Tensor_11744, Tensor_11745], outputs = [Tensor_13432, Tensor_13433, Tensor_13434], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_13432: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13428: 38731776 Bnn)
Tensor_13433: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13433: 50432 Ban)
Tensor_13434: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13434: 50432 Ban)
Operation_2297: operator = TypecastForward, inputs = [Tensor_13432], outputs = [Tensor_13436], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_13436: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13424: 19365888 Bnn)
Operation_2299: operator = TypecastForward, inputs = [Tensor_11738], outputs = [Tensor_13438], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_13438: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13438: 1179648 Ban)
Operation_2301: operator = TypecastForward, inputs = [Tensor_11739], outputs = [Tensor_13440], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_13440: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11821: 1536 Bnn)
Operation_2303: operator = LinearForward, inputs = [Tensor_13436, Tensor_13438, Tensor_13440], outputs = [Tensor_13442], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_13442: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13460: 19365888 Ban)
Operation_2305: operator = TypecastForward, inputs = [Tensor_13432], outputs = [Tensor_13444], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_13444: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13444: 19365888 Ban)
Operation_2307: operator = TypecastForward, inputs = [Tensor_11734], outputs = [Tensor_13446], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_13446: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13446: 1179648 Ban)
Operation_2309: operator = TypecastForward, inputs = [Tensor_11735], outputs = [Tensor_13448], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_13448: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11821: 1536 Bnn)
Operation_2311: operator = LinearForward, inputs = [Tensor_13444, Tensor_13446, Tensor_13448], outputs = [Tensor_13450], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_13450: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13468: 19365888 Ban)
Operation_2313: operator = TypecastForward, inputs = [Tensor_13432], outputs = [Tensor_13452], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_13452: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13452: 19365888 Ban)
Operation_2315: operator = TypecastForward, inputs = [Tensor_11736], outputs = [Tensor_13454], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_13454: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13454: 1179648 Ban)
Operation_2317: operator = TypecastForward, inputs = [Tensor_11737], outputs = [Tensor_13456], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_13456: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11821: 1536 Bnn)
Operation_2319: operator = LinearForward, inputs = [Tensor_13452, Tensor_13454, Tensor_13456], outputs = [Tensor_13458], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_13458: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13472: 19365888 Ban)
Operation_2321: operator = ElemwiseUnaryForward, inputs = [Tensor_13442], outputs = [Tensor_13460], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 0.125]
Tensor_13460: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13460: 19365888 Bnn)
Operation_2325: operator = ReshapeForward, inputs = [Tensor_13460], outputs = [Tensor_13462], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 64 stride: 151296 stride: 64 stride: 768 stride: 1]
Tensor_13462: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13460: 19365888 Bnn)
Operation_2335: operator = ContiguousForward, inputs = [Tensor_13462], outputs = [Tensor_13464], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_13464: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13466: 19365888 Ban)
Operation_2337: operator = ReshapeForward, inputs = [Tensor_13464], outputs = [Tensor_13466], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_13466: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13466: 19365888 Bnn)
Operation_2329: operator = ReshapeForward, inputs = [Tensor_13450], outputs = [Tensor_13468], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 64 stride: 151296 stride: 64 stride: 768 stride: 1]
Tensor_13468: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13468: 19365888 Bnn)
Operation_2339: operator = ContiguousForward, inputs = [Tensor_13468], outputs = [Tensor_13470], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_13470: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13460: 19365888 Bnn)
Operation_2333: operator = ReshapeForward, inputs = [Tensor_13458], outputs = [Tensor_13472], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 64 stride: 151296 stride: 64 stride: 768 stride: 1]
Tensor_13472: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13472: 19365888 Bnn)
Operation_2343: operator = ContiguousForward, inputs = [Tensor_13472], outputs = [Tensor_13474], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_13474: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13468: 19365888 Bnn)
Operation_2345: operator = ReshapeForward, inputs = [Tensor_13474], outputs = [Tensor_13476], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_13476: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13468: 19365888 Bnn)
Operation_2347: operator = ReshapeForward, inputs = [Tensor_13470], outputs = [Tensor_13478], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 64 shape: 197 stride: 12608 stride: 1 stride: 64]
Tensor_13478: shape = [1536, 64, 197], stride = [12608, 1, 64], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13460: 19365888 Bnn)
Operation_2349: operator = BatchedMatmulForward, inputs = [Tensor_13466, Tensor_13478], outputs = [Tensor_13480], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_13480: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13482: 59610624 Ban)
Operation_2351: operator = ReshapeForward, inputs = [Tensor_13480], outputs = [Tensor_13482], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 197 stride: 465708 stride: 38809 stride: 197 stride: 1]
Tensor_13482: shape = [128, 12, 197, 197], stride = [465708, 38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13482: 59610624 Bnn)
Operation_2359: operator = MaskedFillForward, inputs = [Tensor_13482, Tensor_11912], outputs = [Tensor_13484], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [value_float: -inf]
Tensor_13484: shape = [128, 12, 197, 197], stride = [465708, 38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13482: 59610624 Bnn)
Operation_2361: operator = ReshapeForward, inputs = [Tensor_13484], outputs = [Tensor_13486], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 197 stride: 38809 stride: 197 stride: 1]
Tensor_13486: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13482: 59610624 Bnn)
Operation_2363: operator = TypecastForward, inputs = [Tensor_13486], outputs = [Tensor_13488], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_13488: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11920: 119221248 Bnn)
Operation_2365: operator = SoftmaxForward, inputs = [Tensor_13488], outputs = [Tensor_13490], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dim: 2]
Tensor_13490: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13490: 119221248 Ban)
Operation_2367: operator = TypecastForward, inputs = [Tensor_13490], outputs = [Tensor_13492], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_13492: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13482: 59610624 Bnn)
Operation_2369: operator = BatchedMatmulForward, inputs = [Tensor_13492, Tensor_13476], outputs = [Tensor_13494], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_13494: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13472: 19365888 Bnn)
Operation_9005: operator = AllGatherDuplicated, inputs = [Tensor_13494], outputs = [Tensor_13497], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = all->this->2371
  - Operand: [tag: 9004]
Operation_13496: operator = Consume, inputs = [Tensor_13494], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_13497: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13504: 38731776 Ban)
Operation_2387: operator = TypecastForward, inputs = [Tensor_11740], outputs = [Tensor_13519], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Operation_2389: operator = TypecastForward, inputs = [Tensor_11741], outputs = [Tensor_13520], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Operation_2408: operator = ElemwiseUnaryForward, inputs = [Tensor_13430], outputs = [Tensor_13534], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 1]
Operation_2422: operator = TypecastForward, inputs = [Tensor_11746], outputs = [Tensor_13547], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Operation_2424: operator = TypecastForward, inputs = [Tensor_11747], outputs = [Tensor_13548], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Operation_2371: operator = ReshapeForward, inputs = [Tensor_13497], outputs = [Tensor_13504], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = 9005->this
  - Operand: [shape: 197 shape: 1536 shape: 64 stride: 64 stride: 12608 stride: 1]
Tensor_13504: shape = [197, 1536, 64], stride = [64, 12608, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13504: 38731776 Bnn)
Operation_2373: operator = ContiguousForward, inputs = [Tensor_13504], outputs = [Tensor_13506], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_13506: shape = [197, 1536, 64], stride = [98304, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13508: 38731776 Ban)
Operation_2377: operator = ReshapeForward, inputs = [Tensor_13506], outputs = [Tensor_13508], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 768 stride: 98304 stride: 1]
Tensor_13508: shape = [128, 197, 768], stride = [768, 98304, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13508: 38731776 Bnn)
Operation_2379: operator = TypecastForward, inputs = [Tensor_13508], outputs = [Tensor_13510], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_13510: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13510: 77463552 Ban)
Operation_9015: operator = DistributeWithMemAllocForward, inputs = [Tensor_13510], outputs = [Tensor_13512], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_13512: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13504: 38731776 Bnn)
Operation_2381: operator = LayernormForward, inputs = [Tensor_13512, Tensor_11742, Tensor_11743], outputs = [Tensor_13514, Tensor_13515, Tensor_13516], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_13514: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13508: 38731776 Bnn)
Tensor_13515: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13515: 50432 Ban)
Tensor_13516: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13516: 50432 Ban)
Operation_2385: operator = TypecastForward, inputs = [Tensor_13514], outputs = [Tensor_13518], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_13518: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13472: 19365888 Bnn)
Tensor_13519: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13519: 1179648 Ban)
Tensor_13520: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11821: 1536 Bnn)
Operation_2391: operator = LinearForward, inputs = [Tensor_13518, Tensor_13519, Tensor_13520], outputs = [Tensor_13522], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_13522: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13529: 19365888 Ban)
Tensor_13523: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13525: 128 Ban)
Operation_2398: operator = ElemwiseUnaryForward, inputs = [Tensor_13523], outputs = [Tensor_13525], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_ADD scal1_float: 0.863636374]
Tensor_13525: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13525: 128 Bnn)
Operation_2400: operator = ElemwiseUnaryForward, inputs = [Tensor_13525], outputs = [Tensor_13527], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_FLOOR]
Tensor_13527: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13525: 128 Bnn)
Operation_2402: operator = ElemwiseUnaryForward, inputs = [Tensor_13522], outputs = [Tensor_13529], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_DIV scal1_float: 0.863636374]
Tensor_13529: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13529: 19365888 Bnn)
Operation_2404: operator = ReshapeForward, inputs = [Tensor_13527], outputs = [Tensor_13531], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 1 stride: 0 stride: 0]
Tensor_13531: shape = [128, 197, 768], stride = [1, 0, 0], offset = 0, size = 128, datatype = float16, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13525: 128 Bnn)
Operation_2406: operator = ElemwiseBinaryForward, inputs = [Tensor_13529, Tensor_13531], outputs = [Tensor_13533], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_MUL]
Tensor_13533: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13533: 19365888 Ban)
Tensor_13534: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13538: 38731776 Ban)
Operation_2410: operator = TypecastForward, inputs = [Tensor_13533], outputs = [Tensor_13536], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_13536: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13504: 38731776 Bnn)
Operation_2412: operator = ElemwiseBinaryForward, inputs = [Tensor_13534, Tensor_13536], outputs = [Tensor_13538], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_13538: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13538: 38731776 Bnn)
Operation_2414: operator = LayernormForward, inputs = [Tensor_13538, Tensor_11752, Tensor_11753], outputs = [Tensor_13540, Tensor_13541, Tensor_13542], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_13540: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13504: 38731776 Bnn)
Tensor_13541: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13541: 50432 Ban)
Tensor_13542: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13542: 50432 Ban)
Operation_2418: operator = ReshapeForward, inputs = [Tensor_13540], outputs = [Tensor_13544], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 25216 shape: 768 stride: 768 stride: 1]
Tensor_13544: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13504: 38731776 Bnn)
Operation_2420: operator = TypecastForward, inputs = [Tensor_13544], outputs = [Tensor_13546], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_13546: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13533: 19365888 Bnn)
Tensor_13547: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13547: 4718592 Ban)
Tensor_13548: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11969: 6144 Bnn)
Operation_2426: operator = LinearForward, inputs = [Tensor_13546, Tensor_13547, Tensor_13548], outputs = [Tensor_13550], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_13550: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13550: 77463552 Ban)
Operation_2428: operator = TypecastForward, inputs = [Tensor_13550], outputs = [Tensor_13552], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_13552: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13552: 154927104 Ban)
Operation_2430: operator = ElemwiseUnaryForward, inputs = [Tensor_13552], outputs = [Tensor_13554], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_GELU]
Tensor_13554: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13554: 154927104 Ban)
Operation_2432: operator = TypecastForward, inputs = [Tensor_13554], outputs = [Tensor_13556], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_13556: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13550: 77463552 Bnn)
Operation_2434: operator = TypecastForward, inputs = [Tensor_13556], outputs = [Tensor_13558], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_13558: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13558: 154927104 Ban)
Operation_2436: operator = LayernormForward, inputs = [Tensor_13558, Tensor_11750, Tensor_11751], outputs = [Tensor_13560, Tensor_13561, Tensor_13562], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_13560: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13560: 154927104 Ban)
Tensor_13561: shape = [25216], stride = [1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13561: 50432 Ban)
Tensor_13562: shape = [25216], stride = [1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13562: 50432 Ban)
Operation_2440: operator = TypecastForward, inputs = [Tensor_13560], outputs = [Tensor_13564], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_13564: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13550: 77463552 Bnn)
Operation_2442: operator = TypecastForward, inputs = [Tensor_11748], outputs = [Tensor_13566], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_13566: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13566: 4718592 Ban)
Operation_2444: operator = TypecastForward, inputs = [Tensor_11749], outputs = [Tensor_13568], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_13568: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11821: 1536 Bnn)
Operation_2446: operator = LinearForward, inputs = [Tensor_13564, Tensor_13566, Tensor_13568], outputs = [Tensor_13570], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_13570: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13577: 19365888 Ban)
Tensor_13571: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13573: 128 Ban)
Operation_2451: operator = ElemwiseUnaryForward, inputs = [Tensor_13571], outputs = [Tensor_13573], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_ADD scal1_float: 0.863636374]
Tensor_13573: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13573: 128 Bnn)
Operation_2453: operator = ElemwiseUnaryForward, inputs = [Tensor_13573], outputs = [Tensor_13575], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_FLOOR]
Tensor_13575: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13573: 128 Bnn)
Operation_2448: operator = ReshapeForward, inputs = [Tensor_13570], outputs = [Tensor_13577], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 151296 stride: 768 stride: 1]
Tensor_13577: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13577: 19365888 Bnn)
Operation_2455: operator = ElemwiseUnaryForward, inputs = [Tensor_13577], outputs = [Tensor_13579], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_DIV scal1_float: 0.863636374]
Tensor_13579: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13577: 19365888 Bnn)
Operation_2457: operator = ReshapeForward, inputs = [Tensor_13575], outputs = [Tensor_13581], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 1 stride: 0 stride: 0]
Tensor_13581: shape = [128, 197, 768], stride = [1, 0, 0], offset = 0, size = 128, datatype = float16, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13573: 128 Bnn)
Operation_2459: operator = ElemwiseBinaryForward, inputs = [Tensor_13579, Tensor_13581], outputs = [Tensor_13583], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_MUL]
Tensor_13583: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13583: 19365888 Ban)
Operation_2461: operator = ElemwiseUnaryForward, inputs = [Tensor_13538], outputs = [Tensor_13585], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 1]
Tensor_13585: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13589: 38731776 Ban)
Operation_2463: operator = TypecastForward, inputs = [Tensor_13583], outputs = [Tensor_13587], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_13587: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13587: 38731776 Ban)
Operation_2465: operator = ElemwiseBinaryForward, inputs = [Tensor_13585, Tensor_13587], outputs = [Tensor_13589], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_13589: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13589: 38731776 Bnn)
Operation_2467: operator = LayernormForward, inputs = [Tensor_13589, Tensor_11764, Tensor_11765], outputs = [Tensor_13591, Tensor_13592, Tensor_13593], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_13591: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13587: 38731776 Bnn)
Tensor_13592: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13592: 50432 Ban)
Tensor_13593: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13593: 50432 Ban)
Operation_2471: operator = TypecastForward, inputs = [Tensor_13591], outputs = [Tensor_13595], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_13595: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13583: 19365888 Bnn)
Operation_2473: operator = TypecastForward, inputs = [Tensor_11758], outputs = [Tensor_13597], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_13597: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13597: 1179648 Ban)
Operation_2475: operator = TypecastForward, inputs = [Tensor_11759], outputs = [Tensor_13599], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_13599: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11821: 1536 Bnn)
Operation_2477: operator = LinearForward, inputs = [Tensor_13595, Tensor_13597, Tensor_13599], outputs = [Tensor_13601], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_13601: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13619: 19365888 Ban)
Operation_2479: operator = TypecastForward, inputs = [Tensor_13591], outputs = [Tensor_13603], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_13603: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13603: 19365888 Ban)
Operation_2481: operator = TypecastForward, inputs = [Tensor_11754], outputs = [Tensor_13605], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_13605: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13605: 1179648 Ban)
Operation_2483: operator = TypecastForward, inputs = [Tensor_11755], outputs = [Tensor_13607], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_13607: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11821: 1536 Bnn)
Operation_2485: operator = LinearForward, inputs = [Tensor_13603, Tensor_13605, Tensor_13607], outputs = [Tensor_13609], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_13609: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13627: 19365888 Ban)
Operation_2487: operator = TypecastForward, inputs = [Tensor_13591], outputs = [Tensor_13611], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_13611: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13611: 19365888 Ban)
Operation_2489: operator = TypecastForward, inputs = [Tensor_11756], outputs = [Tensor_13613], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_13613: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13613: 1179648 Ban)
Operation_2491: operator = TypecastForward, inputs = [Tensor_11757], outputs = [Tensor_13615], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_13615: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11821: 1536 Bnn)
Operation_2493: operator = LinearForward, inputs = [Tensor_13611, Tensor_13613, Tensor_13615], outputs = [Tensor_13617], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_13617: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13631: 19365888 Ban)
Operation_2495: operator = ElemwiseUnaryForward, inputs = [Tensor_13601], outputs = [Tensor_13619], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 0.125]
Tensor_13619: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13619: 19365888 Bnn)
Operation_2499: operator = ReshapeForward, inputs = [Tensor_13619], outputs = [Tensor_13621], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 64 stride: 151296 stride: 64 stride: 768 stride: 1]
Tensor_13621: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13619: 19365888 Bnn)
Operation_2509: operator = ContiguousForward, inputs = [Tensor_13621], outputs = [Tensor_13623], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_13623: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13625: 19365888 Ban)
Operation_2511: operator = ReshapeForward, inputs = [Tensor_13623], outputs = [Tensor_13625], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_13625: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13625: 19365888 Bnn)
Operation_2503: operator = ReshapeForward, inputs = [Tensor_13609], outputs = [Tensor_13627], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 64 stride: 151296 stride: 64 stride: 768 stride: 1]
Tensor_13627: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13627: 19365888 Bnn)
Operation_2513: operator = ContiguousForward, inputs = [Tensor_13627], outputs = [Tensor_13629], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_13629: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13619: 19365888 Bnn)
Operation_2507: operator = ReshapeForward, inputs = [Tensor_13617], outputs = [Tensor_13631], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 64 stride: 151296 stride: 64 stride: 768 stride: 1]
Tensor_13631: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13631: 19365888 Bnn)
Operation_2517: operator = ContiguousForward, inputs = [Tensor_13631], outputs = [Tensor_13633], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_13633: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13627: 19365888 Bnn)
Operation_2519: operator = ReshapeForward, inputs = [Tensor_13633], outputs = [Tensor_13635], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_13635: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13627: 19365888 Bnn)
Operation_2521: operator = ReshapeForward, inputs = [Tensor_13629], outputs = [Tensor_13637], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 64 shape: 197 stride: 12608 stride: 1 stride: 64]
Tensor_13637: shape = [1536, 64, 197], stride = [12608, 1, 64], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13619: 19365888 Bnn)
Operation_2523: operator = BatchedMatmulForward, inputs = [Tensor_13625, Tensor_13637], outputs = [Tensor_13639], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_13639: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13641: 59610624 Ban)
Operation_2525: operator = ReshapeForward, inputs = [Tensor_13639], outputs = [Tensor_13641], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 197 stride: 465708 stride: 38809 stride: 197 stride: 1]
Tensor_13641: shape = [128, 12, 197, 197], stride = [465708, 38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13641: 59610624 Bnn)
Operation_2533: operator = MaskedFillForward, inputs = [Tensor_13641, Tensor_11912], outputs = [Tensor_13643], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [value_float: -inf]
Tensor_13643: shape = [128, 12, 197, 197], stride = [465708, 38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13641: 59610624 Bnn)
Operation_2535: operator = ReshapeForward, inputs = [Tensor_13643], outputs = [Tensor_13645], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 197 stride: 38809 stride: 197 stride: 1]
Tensor_13645: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13641: 59610624 Bnn)
Operation_2537: operator = TypecastForward, inputs = [Tensor_13645], outputs = [Tensor_13647], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_13647: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11920: 119221248 Bnr)
Operation_2539: operator = SoftmaxForward, inputs = [Tensor_13647], outputs = [Tensor_13649], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dim: 2]
Tensor_13649: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13649: 119221248 Ban)
Operation_2541: operator = TypecastForward, inputs = [Tensor_13649], outputs = [Tensor_13651], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_13651: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13641: 59610624 Bnn)
Operation_2543: operator = BatchedMatmulForward, inputs = [Tensor_13651, Tensor_13635], outputs = [Tensor_13653], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_13653: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13631: 19365888 Bnn)
Operation_9164: operator = AllGatherDuplicated, inputs = [Tensor_13653], outputs = [Tensor_13656], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = all->this->2545
  - Operand: [tag: 9163]
Operation_13655: operator = Consume, inputs = [Tensor_13653], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_13656: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13663: 38731776 Ban)
Operation_2561: operator = TypecastForward, inputs = [Tensor_11760], outputs = [Tensor_13678], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Operation_2563: operator = TypecastForward, inputs = [Tensor_11761], outputs = [Tensor_13679], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Operation_2582: operator = ElemwiseUnaryForward, inputs = [Tensor_13589], outputs = [Tensor_13693], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 1]
Operation_2596: operator = TypecastForward, inputs = [Tensor_11766], outputs = [Tensor_13706], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Operation_2598: operator = TypecastForward, inputs = [Tensor_11767], outputs = [Tensor_13707], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Operation_2545: operator = ReshapeForward, inputs = [Tensor_13656], outputs = [Tensor_13663], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = 9164->this
  - Operand: [shape: 197 shape: 1536 shape: 64 stride: 64 stride: 12608 stride: 1]
Tensor_13663: shape = [197, 1536, 64], stride = [64, 12608, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13663: 38731776 Bnn)
Operation_2547: operator = ContiguousForward, inputs = [Tensor_13663], outputs = [Tensor_13665], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_13665: shape = [197, 1536, 64], stride = [98304, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13667: 38731776 Ban)
Operation_2551: operator = ReshapeForward, inputs = [Tensor_13665], outputs = [Tensor_13667], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 768 stride: 98304 stride: 1]
Tensor_13667: shape = [128, 197, 768], stride = [768, 98304, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13667: 38731776 Bnn)
Operation_2553: operator = TypecastForward, inputs = [Tensor_13667], outputs = [Tensor_13669], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_13669: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13669: 77463552 Ban)
Operation_9174: operator = DistributeWithMemAllocForward, inputs = [Tensor_13669], outputs = [Tensor_13671], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_13671: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13663: 38731776 Bnn)
Operation_2555: operator = LayernormForward, inputs = [Tensor_13671, Tensor_11762, Tensor_11763], outputs = [Tensor_13673, Tensor_13674, Tensor_13675], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_13673: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13667: 38731776 Bnn)
Tensor_13674: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13674: 50432 Ban)
Tensor_13675: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13675: 50432 Ban)
Operation_2559: operator = TypecastForward, inputs = [Tensor_13673], outputs = [Tensor_13677], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_13677: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13631: 19365888 Bnn)
Tensor_13678: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13678: 1179648 Ban)
Tensor_13679: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11821: 1536 Bnn)
Operation_2565: operator = LinearForward, inputs = [Tensor_13677, Tensor_13678, Tensor_13679], outputs = [Tensor_13681], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_13681: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13688: 19365888 Ban)
Tensor_13682: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13684: 128 Ban)
Operation_2572: operator = ElemwiseUnaryForward, inputs = [Tensor_13682], outputs = [Tensor_13684], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_ADD scal1_float: 0.85]
Tensor_13684: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13684: 128 Bnn)
Operation_2574: operator = ElemwiseUnaryForward, inputs = [Tensor_13684], outputs = [Tensor_13686], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_FLOOR]
Tensor_13686: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13684: 128 Bnn)
Operation_2576: operator = ElemwiseUnaryForward, inputs = [Tensor_13681], outputs = [Tensor_13688], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_DIV scal1_float: 0.85]
Tensor_13688: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13688: 19365888 Bnn)
Operation_2578: operator = ReshapeForward, inputs = [Tensor_13686], outputs = [Tensor_13690], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 1 stride: 0 stride: 0]
Tensor_13690: shape = [128, 197, 768], stride = [1, 0, 0], offset = 0, size = 128, datatype = float16, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13684: 128 Bnn)
Operation_2580: operator = ElemwiseBinaryForward, inputs = [Tensor_13688, Tensor_13690], outputs = [Tensor_13692], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_MUL]
Tensor_13692: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13692: 19365888 Ban)
Tensor_13693: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13697: 38731776 Ban)
Operation_2584: operator = TypecastForward, inputs = [Tensor_13692], outputs = [Tensor_13695], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_13695: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13663: 38731776 Bnn)
Operation_2586: operator = ElemwiseBinaryForward, inputs = [Tensor_13693, Tensor_13695], outputs = [Tensor_13697], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_13697: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13697: 38731776 Bnn)
Operation_2588: operator = LayernormForward, inputs = [Tensor_13697, Tensor_11772, Tensor_11773], outputs = [Tensor_13699, Tensor_13700, Tensor_13701], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_13699: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13663: 38731776 Bnn)
Tensor_13700: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13700: 50432 Ban)
Tensor_13701: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13701: 50432 Ban)
Operation_2592: operator = ReshapeForward, inputs = [Tensor_13699], outputs = [Tensor_13703], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 25216 shape: 768 stride: 768 stride: 1]
Tensor_13703: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13663: 38731776 Bnn)
Operation_2594: operator = TypecastForward, inputs = [Tensor_13703], outputs = [Tensor_13705], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_13705: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13692: 19365888 Bnn)
Tensor_13706: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13706: 4718592 Ban)
Tensor_13707: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11969: 6144 Bnr)
Operation_2600: operator = LinearForward, inputs = [Tensor_13705, Tensor_13706, Tensor_13707], outputs = [Tensor_13709], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_13709: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13709: 77463552 Ban)
Operation_2602: operator = TypecastForward, inputs = [Tensor_13709], outputs = [Tensor_13711], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_13711: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13711: 154927104 Ban)
Operation_2604: operator = ElemwiseUnaryForward, inputs = [Tensor_13711], outputs = [Tensor_13713], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_GELU]
Tensor_13713: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13713: 154927104 Ban)
Operation_2606: operator = TypecastForward, inputs = [Tensor_13713], outputs = [Tensor_13715], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_13715: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13709: 77463552 Bnn)
Operation_2608: operator = TypecastForward, inputs = [Tensor_13715], outputs = [Tensor_13717], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_13717: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13717: 154927104 Ban)
Operation_2610: operator = LayernormForward, inputs = [Tensor_13717, Tensor_11770, Tensor_11771], outputs = [Tensor_13719, Tensor_13720, Tensor_13721], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_13719: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13719: 154927104 Ban)
Tensor_13720: shape = [25216], stride = [1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13720: 50432 Ban)
Tensor_13721: shape = [25216], stride = [1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13721: 50432 Ban)
Operation_2614: operator = TypecastForward, inputs = [Tensor_13719], outputs = [Tensor_13723], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_13723: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13709: 77463552 Bnn)
Operation_2616: operator = TypecastForward, inputs = [Tensor_11768], outputs = [Tensor_13725], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_13725: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13725: 4718592 Ban)
Operation_2618: operator = TypecastForward, inputs = [Tensor_11769], outputs = [Tensor_13727], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_13727: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11821: 1536 Bnr)
Operation_2620: operator = LinearForward, inputs = [Tensor_13723, Tensor_13725, Tensor_13727], outputs = [Tensor_13729], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_13729: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13736: 19365888 Ban)
Tensor_13730: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13732: 128 Ban)
Operation_2625: operator = ElemwiseUnaryForward, inputs = [Tensor_13730], outputs = [Tensor_13732], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_ADD scal1_float: 0.85]
Tensor_13732: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13732: 128 Bnn)
Operation_2627: operator = ElemwiseUnaryForward, inputs = [Tensor_13732], outputs = [Tensor_13734], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_FLOOR]
Tensor_13734: shape = [128, 1, 1], stride = [1, 1, 1], offset = 0, size = 128, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13732: 128 Bnn)
Operation_2622: operator = ReshapeForward, inputs = [Tensor_13729], outputs = [Tensor_13736], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 151296 stride: 768 stride: 1]
Tensor_13736: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13736: 19365888 Bnn)
Operation_2629: operator = ElemwiseUnaryForward, inputs = [Tensor_13736], outputs = [Tensor_13738], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_DIV scal1_float: 0.85]
Tensor_13738: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13736: 19365888 Bnn)
Operation_2631: operator = ReshapeForward, inputs = [Tensor_13734], outputs = [Tensor_13740], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 1 stride: 0 stride: 0]
Tensor_13740: shape = [128, 197, 768], stride = [1, 0, 0], offset = 0, size = 128, datatype = float16, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13732: 128 Bnn)
Operation_2633: operator = ElemwiseBinaryForward, inputs = [Tensor_13738, Tensor_13740], outputs = [Tensor_13742], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_MUL]
Tensor_13742: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13742: 19365888 Bar)
Operation_2635: operator = ElemwiseUnaryForward, inputs = [Tensor_13697], outputs = [Tensor_13744], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 1]
Tensor_13744: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13748: 38731776 Ban)
Operation_2637: operator = TypecastForward, inputs = [Tensor_13742], outputs = [Tensor_13746], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_13746: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13746: 38731776 Bar)
Operation_2639: operator = ElemwiseBinaryForward, inputs = [Tensor_13744, Tensor_13746], outputs = [Tensor_13748], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_13748: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13748: 38731776 Bnn)
Operation_2641: operator = ReshapeForward, inputs = [Tensor_13748], outputs = [Tensor_13750], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 196 shape: 768 stride: 151296 stride: 768 stride: 1 offset: 768]
Tensor_13750: shape = [128, 196, 768], stride = [151296, 768, 1], offset = 768, size = 19365888, datatype = float32, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13748: 38731776 Bnr)
Operation_2643: operator = MeanForward, inputs = [Tensor_13750], outputs = [Tensor_13752], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dims: 1 divisor: 196]
Tensor_13752: shape = [128, 768], stride = [768, 1], offset = 0, size = 98304, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13752: 196608 Ban)
Operation_2645: operator = LayernormForward, inputs = [Tensor_13752, Tensor_11774, Tensor_11775], outputs = [Tensor_13754, Tensor_13755, Tensor_13756], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_13754: shape = [128, 768], stride = [768, 1], offset = 0, size = 98304, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13754: 196608 Ban)
Tensor_13755: shape = [128], stride = [1], offset = 0, size = 128, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13755: 256 Ban)
Tensor_13756: shape = [128], stride = [1], offset = 0, size = 128, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13756: 256 Ban)
Operation_2649: operator = TypecastForward, inputs = [Tensor_13754], outputs = [Tensor_13758], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_13758: shape = [128, 768], stride = [768, 1], offset = 0, size = 98304, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13758: 98304 Ban)
Operation_2651: operator = TypecastForward, inputs = [Tensor_11776], outputs = [Tensor_13760], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_13760: shape = [1000, 768], stride = [768, 1], offset = 0, size = 768000, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13760: 1536000 Ban)
Operation_2653: operator = TypecastForward, inputs = [Tensor_11777], outputs = [Tensor_13762], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_13762: shape = [1000], stride = [1], offset = 0, size = 1000, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13762: 2000 Bar)
Operation_2655: operator = LinearForward, inputs = [Tensor_13758, Tensor_13760, Tensor_13762], outputs = [Tensor_13764], # backwardable inputs = 3, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_13764: shape = [128, 1000], stride = [1000, 1], offset = 0, size = 128000, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13764: 128000 Bar)
Operation_2657: operator = ElemwiseUnaryForward, inputs = [Tensor_11815], outputs = [Tensor_13766], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_NEG]
Tensor_13766: shape = [128, 1000], stride = [1000, 1], offset = 0, size = 128000, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11802: 512000 Bnn)
Operation_2659: operator = TypecastForward, inputs = [Tensor_13764], outputs = [Tensor_13768], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_13768: shape = [128, 1000], stride = [1000, 1], offset = 0, size = 128000, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13768: 256000 Ban)
Operation_2661: operator = SoftmaxForward, inputs = [Tensor_13768], outputs = [Tensor_13770], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [method: SOFTMAX_LOG dim: 1]
Tensor_13770: shape = [128, 1000], stride = [1000, 1], offset = 0, size = 128000, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13770: 256000 Ban)
Operation_9280: operator = DistributeWithMemAllocForward, inputs = [Tensor_13766], outputs = [Tensor_13772], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_13772: shape = [128, 1000], stride = [1000, 1], offset = 0, size = 128000, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13768: 256000 Bnn)
Operation_2663: operator = ElemwiseBinaryForward, inputs = [Tensor_13772, Tensor_13770], outputs = [Tensor_13774], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_MUL]
Tensor_13774: shape = [128, 1000], stride = [1000, 1], offset = 0, size = 128000, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13768: 256000 Bnr)
Operation_2665: operator = SumForward, inputs = [Tensor_13774], outputs = [Tensor_13776], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dims: 1]
Tensor_13776: shape = [128], stride = [1], offset = 0, size = 128, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13776: 256 Bar)
Operation_2667: operator = MeanForward, inputs = [Tensor_13776], outputs = [Tensor_13778], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = this->9289
  - Operand: [dims: 0 divisor: 128]
Tensor_13778: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (11795: 4 Bnn)
Operation_9289: operator = AllReduceDuplicated, inputs = [Tensor_13778, nullptr], outputs = [Tensor_13781, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 2667->this->last
  - Operand: [subop: SUM tag: 9288]
Operation_13780: operator = Consume, inputs = [Tensor_13778], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_13781: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11795: 4 Bnn)

Move-in tensors (source -> target):
Tensor_2 -> Tensor_11530
Tensor_3 -> Tensor_11531
Tensor_5 -> Tensor_11532
Tensor_6 -> Tensor_11533
Tensor_8 -> Tensor_11534
Tensor_9 -> Tensor_11535
Tensor_12 -> Tensor_11536
Tensor_13 -> Tensor_11537
Tensor_16 -> Tensor_11538
Tensor_17 -> Tensor_11539
Tensor_20 -> Tensor_11540
Tensor_21 -> Tensor_11541
Tensor_24 -> Tensor_11542
Tensor_25 -> Tensor_11543
Tensor_28 -> Tensor_11544
Tensor_29 -> Tensor_11545
Tensor_32 -> Tensor_11546
Tensor_33 -> Tensor_11547
Tensor_34 -> Tensor_11548
Tensor_35 -> Tensor_11549
Tensor_36 -> Tensor_11550
Tensor_37 -> Tensor_11551
Tensor_44 -> Tensor_11552
Tensor_45 -> Tensor_11553
Tensor_48 -> Tensor_11554
Tensor_49 -> Tensor_11555
Tensor_52 -> Tensor_11556
Tensor_53 -> Tensor_11557
Tensor_56 -> Tensor_11558
Tensor_57 -> Tensor_11559
Tensor_60 -> Tensor_11560
Tensor_61 -> Tensor_11561
Tensor_64 -> Tensor_11562
Tensor_65 -> Tensor_11563
Tensor_68 -> Tensor_11564
Tensor_69 -> Tensor_11565
Tensor_72 -> Tensor_11566
Tensor_73 -> Tensor_11567
Tensor_74 -> Tensor_11568
Tensor_75 -> Tensor_11569
Tensor_76 -> Tensor_11570
Tensor_77 -> Tensor_11571
Tensor_84 -> Tensor_11572
Tensor_85 -> Tensor_11573
Tensor_88 -> Tensor_11574
Tensor_89 -> Tensor_11575
Tensor_92 -> Tensor_11576
Tensor_93 -> Tensor_11577
Tensor_96 -> Tensor_11578
Tensor_97 -> Tensor_11579
Tensor_100 -> Tensor_11580
Tensor_101 -> Tensor_11581
Tensor_104 -> Tensor_11582
Tensor_105 -> Tensor_11583
Tensor_108 -> Tensor_11584
Tensor_109 -> Tensor_11585
Tensor_112 -> Tensor_11586
Tensor_113 -> Tensor_11587
Tensor_114 -> Tensor_11588
Tensor_115 -> Tensor_11589
Tensor_116 -> Tensor_11590
Tensor_117 -> Tensor_11591
Tensor_124 -> Tensor_11592
Tensor_125 -> Tensor_11593
Tensor_128 -> Tensor_11594
Tensor_129 -> Tensor_11595
Tensor_132 -> Tensor_11596
Tensor_133 -> Tensor_11597
Tensor_136 -> Tensor_11598
Tensor_137 -> Tensor_11599
Tensor_140 -> Tensor_11600
Tensor_141 -> Tensor_11601
Tensor_144 -> Tensor_11602
Tensor_145 -> Tensor_11603
Tensor_148 -> Tensor_11604
Tensor_149 -> Tensor_11605
Tensor_152 -> Tensor_11606
Tensor_153 -> Tensor_11607
Tensor_154 -> Tensor_11608
Tensor_155 -> Tensor_11609
Tensor_156 -> Tensor_11610
Tensor_157 -> Tensor_11611
Tensor_164 -> Tensor_11612
Tensor_165 -> Tensor_11613
Tensor_168 -> Tensor_11614
Tensor_169 -> Tensor_11615
Tensor_172 -> Tensor_11616
Tensor_173 -> Tensor_11617
Tensor_176 -> Tensor_11618
Tensor_177 -> Tensor_11619
Tensor_180 -> Tensor_11620
Tensor_181 -> Tensor_11621
Tensor_184 -> Tensor_11622
Tensor_185 -> Tensor_11623
Tensor_188 -> Tensor_11624
Tensor_189 -> Tensor_11625
Tensor_192 -> Tensor_11626
Tensor_193 -> Tensor_11627
Tensor_194 -> Tensor_11628
Tensor_195 -> Tensor_11629
Tensor_196 -> Tensor_11630
Tensor_197 -> Tensor_11631
Tensor_204 -> Tensor_11632
Tensor_205 -> Tensor_11633
Tensor_208 -> Tensor_11634
Tensor_209 -> Tensor_11635
Tensor_212 -> Tensor_11636
Tensor_213 -> Tensor_11637
Tensor_216 -> Tensor_11638
Tensor_217 -> Tensor_11639
Tensor_220 -> Tensor_11640
Tensor_221 -> Tensor_11641
Tensor_224 -> Tensor_11642
Tensor_225 -> Tensor_11643
Tensor_228 -> Tensor_11644
Tensor_229 -> Tensor_11645
Tensor_232 -> Tensor_11646
Tensor_233 -> Tensor_11647
Tensor_234 -> Tensor_11648
Tensor_235 -> Tensor_11649
Tensor_236 -> Tensor_11650
Tensor_237 -> Tensor_11651
Tensor_244 -> Tensor_11652
Tensor_245 -> Tensor_11653
Tensor_248 -> Tensor_11654
Tensor_249 -> Tensor_11655
Tensor_252 -> Tensor_11656
Tensor_253 -> Tensor_11657
Tensor_256 -> Tensor_11658
Tensor_257 -> Tensor_11659
Tensor_260 -> Tensor_11660
Tensor_261 -> Tensor_11661
Tensor_264 -> Tensor_11662
Tensor_265 -> Tensor_11663
Tensor_268 -> Tensor_11664
Tensor_269 -> Tensor_11665
Tensor_272 -> Tensor_11666
Tensor_273 -> Tensor_11667
Tensor_274 -> Tensor_11668
Tensor_275 -> Tensor_11669
Tensor_276 -> Tensor_11670
Tensor_277 -> Tensor_11671
Tensor_284 -> Tensor_11672
Tensor_285 -> Tensor_11673
Tensor_288 -> Tensor_11674
Tensor_289 -> Tensor_11675
Tensor_292 -> Tensor_11676
Tensor_293 -> Tensor_11677
Tensor_296 -> Tensor_11678
Tensor_297 -> Tensor_11679
Tensor_300 -> Tensor_11680
Tensor_301 -> Tensor_11681
Tensor_304 -> Tensor_11682
Tensor_305 -> Tensor_11683
Tensor_308 -> Tensor_11684
Tensor_309 -> Tensor_11685
Tensor_312 -> Tensor_11686
Tensor_313 -> Tensor_11687
Tensor_314 -> Tensor_11688
Tensor_315 -> Tensor_11689
Tensor_316 -> Tensor_11690
Tensor_317 -> Tensor_11691
Tensor_324 -> Tensor_11692
Tensor_325 -> Tensor_11693
Tensor_328 -> Tensor_11694
Tensor_329 -> Tensor_11695
Tensor_332 -> Tensor_11696
Tensor_333 -> Tensor_11697
Tensor_336 -> Tensor_11698
Tensor_337 -> Tensor_11699
Tensor_340 -> Tensor_11700
Tensor_341 -> Tensor_11701
Tensor_344 -> Tensor_11702
Tensor_345 -> Tensor_11703
Tensor_348 -> Tensor_11704
Tensor_349 -> Tensor_11705
Tensor_352 -> Tensor_11706
Tensor_353 -> Tensor_11707
Tensor_354 -> Tensor_11708
Tensor_355 -> Tensor_11709
Tensor_356 -> Tensor_11710
Tensor_357 -> Tensor_11711
Tensor_364 -> Tensor_11712
Tensor_365 -> Tensor_11713
Tensor_368 -> Tensor_11714
Tensor_369 -> Tensor_11715
Tensor_372 -> Tensor_11716
Tensor_373 -> Tensor_11717
Tensor_376 -> Tensor_11718
Tensor_377 -> Tensor_11719
Tensor_380 -> Tensor_11720
Tensor_381 -> Tensor_11721
Tensor_384 -> Tensor_11722
Tensor_385 -> Tensor_11723
Tensor_388 -> Tensor_11724
Tensor_389 -> Tensor_11725
Tensor_392 -> Tensor_11726
Tensor_393 -> Tensor_11727
Tensor_394 -> Tensor_11728
Tensor_395 -> Tensor_11729
Tensor_396 -> Tensor_11730
Tensor_397 -> Tensor_11731
Tensor_404 -> Tensor_11732
Tensor_405 -> Tensor_11733
Tensor_408 -> Tensor_11734
Tensor_409 -> Tensor_11735
Tensor_412 -> Tensor_11736
Tensor_413 -> Tensor_11737
Tensor_416 -> Tensor_11738
Tensor_417 -> Tensor_11739
Tensor_420 -> Tensor_11740
Tensor_421 -> Tensor_11741
Tensor_424 -> Tensor_11742
Tensor_425 -> Tensor_11743
Tensor_428 -> Tensor_11744
Tensor_429 -> Tensor_11745
Tensor_432 -> Tensor_11746
Tensor_433 -> Tensor_11747
Tensor_434 -> Tensor_11748
Tensor_435 -> Tensor_11749
Tensor_436 -> Tensor_11750
Tensor_437 -> Tensor_11751
Tensor_444 -> Tensor_11752
Tensor_445 -> Tensor_11753
Tensor_448 -> Tensor_11754
Tensor_449 -> Tensor_11755
Tensor_452 -> Tensor_11756
Tensor_453 -> Tensor_11757
Tensor_456 -> Tensor_11758
Tensor_457 -> Tensor_11759
Tensor_460 -> Tensor_11760
Tensor_461 -> Tensor_11761
Tensor_464 -> Tensor_11762
Tensor_465 -> Tensor_11763
Tensor_468 -> Tensor_11764
Tensor_469 -> Tensor_11765
Tensor_472 -> Tensor_11766
Tensor_473 -> Tensor_11767
Tensor_474 -> Tensor_11768
Tensor_475 -> Tensor_11769
Tensor_476 -> Tensor_11770
Tensor_477 -> Tensor_11771
Tensor_484 -> Tensor_11772
Tensor_485 -> Tensor_11773
Tensor_488 -> Tensor_11774
Tensor_489 -> Tensor_11775
Tensor_490 -> Tensor_11776
Tensor_491 -> Tensor_11777
Tensor_492 -> Tensor_11778
Tensor_493 -> Tensor_11779
Tensor_504 -> Tensor_11788
Tensor_507 -> Tensor_11791
Tensor_516 -> Tensor_11800
Tensor_519 -> Tensor_11803
Tensor_548 -> Tensor_11834
Tensor_831 -> Tensor_12092
Tensor_884 -> Tensor_12140
Tensor_1005 -> Tensor_12251
Tensor_1058 -> Tensor_12299
Tensor_1179 -> Tensor_12410
Tensor_1232 -> Tensor_12458
Tensor_1353 -> Tensor_12569
Tensor_1406 -> Tensor_12617
Tensor_1527 -> Tensor_12728
Tensor_1580 -> Tensor_12776
Tensor_1701 -> Tensor_12887
Tensor_1754 -> Tensor_12935
Tensor_1875 -> Tensor_13046
Tensor_1928 -> Tensor_13094
Tensor_2049 -> Tensor_13205
Tensor_2102 -> Tensor_13253
Tensor_2223 -> Tensor_13364
Tensor_2276 -> Tensor_13412
Tensor_2397 -> Tensor_13523
Tensor_2450 -> Tensor_13571
Tensor_2571 -> Tensor_13682
Tensor_2624 -> Tensor_13730

Move-out tensors (source <- target):
Tensor_2 <- Tensor_11530
Tensor_3 <- Tensor_11531
Tensor_5 <- Tensor_11532
Tensor_6 <- Tensor_11533
Tensor_8 <- Tensor_11534
Tensor_9 <- Tensor_11535
Tensor_12 <- Tensor_11536
Tensor_13 <- Tensor_11537
Tensor_16 <- Tensor_11538
Tensor_17 <- Tensor_11539
Tensor_20 <- Tensor_11540
Tensor_21 <- Tensor_11541
Tensor_24 <- Tensor_11542
Tensor_25 <- Tensor_11543
Tensor_28 <- Tensor_11544
Tensor_29 <- Tensor_11545
Tensor_32 <- Tensor_11546
Tensor_33 <- Tensor_11547
Tensor_34 <- Tensor_11548
Tensor_35 <- Tensor_11549
Tensor_36 <- Tensor_11550
Tensor_37 <- Tensor_11551
Tensor_44 <- Tensor_11552
Tensor_45 <- Tensor_11553
Tensor_48 <- Tensor_11554
Tensor_49 <- Tensor_11555
Tensor_52 <- Tensor_11556
Tensor_53 <- Tensor_11557
Tensor_56 <- Tensor_11558
Tensor_57 <- Tensor_11559
Tensor_60 <- Tensor_11560
Tensor_61 <- Tensor_11561
Tensor_64 <- Tensor_11562
Tensor_65 <- Tensor_11563
Tensor_68 <- Tensor_11564
Tensor_69 <- Tensor_11565
Tensor_72 <- Tensor_11566
Tensor_73 <- Tensor_11567
Tensor_74 <- Tensor_11568
Tensor_75 <- Tensor_11569
Tensor_76 <- Tensor_11570
Tensor_77 <- Tensor_11571
Tensor_84 <- Tensor_11572
Tensor_85 <- Tensor_11573
Tensor_88 <- Tensor_11574
Tensor_89 <- Tensor_11575
Tensor_92 <- Tensor_11576
Tensor_93 <- Tensor_11577
Tensor_96 <- Tensor_11578
Tensor_97 <- Tensor_11579
Tensor_100 <- Tensor_11580
Tensor_101 <- Tensor_11581
Tensor_104 <- Tensor_11582
Tensor_105 <- Tensor_11583
Tensor_108 <- Tensor_11584
Tensor_109 <- Tensor_11585
Tensor_112 <- Tensor_11586
Tensor_113 <- Tensor_11587
Tensor_114 <- Tensor_11588
Tensor_115 <- Tensor_11589
Tensor_116 <- Tensor_11590
Tensor_117 <- Tensor_11591
Tensor_124 <- Tensor_11592
Tensor_125 <- Tensor_11593
Tensor_128 <- Tensor_11594
Tensor_129 <- Tensor_11595
Tensor_132 <- Tensor_11596
Tensor_133 <- Tensor_11597
Tensor_136 <- Tensor_11598
Tensor_137 <- Tensor_11599
Tensor_140 <- Tensor_11600
Tensor_141 <- Tensor_11601
Tensor_144 <- Tensor_11602
Tensor_145 <- Tensor_11603
Tensor_148 <- Tensor_11604
Tensor_149 <- Tensor_11605
Tensor_152 <- Tensor_11606
Tensor_153 <- Tensor_11607
Tensor_154 <- Tensor_11608
Tensor_155 <- Tensor_11609
Tensor_156 <- Tensor_11610
Tensor_157 <- Tensor_11611
Tensor_164 <- Tensor_11612
Tensor_165 <- Tensor_11613
Tensor_168 <- Tensor_11614
Tensor_169 <- Tensor_11615
Tensor_172 <- Tensor_11616
Tensor_173 <- Tensor_11617
Tensor_176 <- Tensor_11618
Tensor_177 <- Tensor_11619
Tensor_180 <- Tensor_11620
Tensor_181 <- Tensor_11621
Tensor_184 <- Tensor_11622
Tensor_185 <- Tensor_11623
Tensor_188 <- Tensor_11624
Tensor_189 <- Tensor_11625
Tensor_192 <- Tensor_11626
Tensor_193 <- Tensor_11627
Tensor_194 <- Tensor_11628
Tensor_195 <- Tensor_11629
Tensor_196 <- Tensor_11630
Tensor_197 <- Tensor_11631
Tensor_204 <- Tensor_11632
Tensor_205 <- Tensor_11633
Tensor_208 <- Tensor_11634
Tensor_209 <- Tensor_11635
Tensor_212 <- Tensor_11636
Tensor_213 <- Tensor_11637
Tensor_216 <- Tensor_11638
Tensor_217 <- Tensor_11639
Tensor_220 <- Tensor_11640
Tensor_221 <- Tensor_11641
Tensor_224 <- Tensor_11642
Tensor_225 <- Tensor_11643
Tensor_228 <- Tensor_11644
Tensor_229 <- Tensor_11645
Tensor_232 <- Tensor_11646
Tensor_233 <- Tensor_11647
Tensor_234 <- Tensor_11648
Tensor_235 <- Tensor_11649
Tensor_236 <- Tensor_11650
Tensor_237 <- Tensor_11651
Tensor_244 <- Tensor_11652
Tensor_245 <- Tensor_11653
Tensor_248 <- Tensor_11654
Tensor_249 <- Tensor_11655
Tensor_252 <- Tensor_11656
Tensor_253 <- Tensor_11657
Tensor_256 <- Tensor_11658
Tensor_257 <- Tensor_11659
Tensor_260 <- Tensor_11660
Tensor_261 <- Tensor_11661
Tensor_264 <- Tensor_11662
Tensor_265 <- Tensor_11663
Tensor_268 <- Tensor_11664
Tensor_269 <- Tensor_11665
Tensor_272 <- Tensor_11666
Tensor_273 <- Tensor_11667
Tensor_274 <- Tensor_11668
Tensor_275 <- Tensor_11669
Tensor_276 <- Tensor_11670
Tensor_277 <- Tensor_11671
Tensor_284 <- Tensor_11672
Tensor_285 <- Tensor_11673
Tensor_288 <- Tensor_11674
Tensor_289 <- Tensor_11675
Tensor_292 <- Tensor_11676
Tensor_293 <- Tensor_11677
Tensor_296 <- Tensor_11678
Tensor_297 <- Tensor_11679
Tensor_300 <- Tensor_11680
Tensor_301 <- Tensor_11681
Tensor_304 <- Tensor_11682
Tensor_305 <- Tensor_11683
Tensor_308 <- Tensor_11684
Tensor_309 <- Tensor_11685
Tensor_312 <- Tensor_11686
Tensor_313 <- Tensor_11687
Tensor_314 <- Tensor_11688
Tensor_315 <- Tensor_11689
Tensor_316 <- Tensor_11690
Tensor_317 <- Tensor_11691
Tensor_324 <- Tensor_11692
Tensor_325 <- Tensor_11693
Tensor_328 <- Tensor_11694
Tensor_329 <- Tensor_11695
Tensor_332 <- Tensor_11696
Tensor_333 <- Tensor_11697
Tensor_336 <- Tensor_11698
Tensor_337 <- Tensor_11699
Tensor_340 <- Tensor_11700
Tensor_341 <- Tensor_11701
Tensor_344 <- Tensor_11702
Tensor_345 <- Tensor_11703
Tensor_348 <- Tensor_11704
Tensor_349 <- Tensor_11705
Tensor_352 <- Tensor_11706
Tensor_353 <- Tensor_11707
Tensor_354 <- Tensor_11708
Tensor_355 <- Tensor_11709
Tensor_356 <- Tensor_11710
Tensor_357 <- Tensor_11711
Tensor_364 <- Tensor_11712
Tensor_365 <- Tensor_11713
Tensor_368 <- Tensor_11714
Tensor_369 <- Tensor_11715
Tensor_372 <- Tensor_11716
Tensor_373 <- Tensor_11717
Tensor_376 <- Tensor_11718
Tensor_377 <- Tensor_11719
Tensor_380 <- Tensor_11720
Tensor_381 <- Tensor_11721
Tensor_384 <- Tensor_11722
Tensor_385 <- Tensor_11723
Tensor_388 <- Tensor_11724
Tensor_389 <- Tensor_11725
Tensor_392 <- Tensor_11726
Tensor_393 <- Tensor_11727
Tensor_394 <- Tensor_11728
Tensor_395 <- Tensor_11729
Tensor_396 <- Tensor_11730
Tensor_397 <- Tensor_11731
Tensor_404 <- Tensor_11732
Tensor_405 <- Tensor_11733
Tensor_408 <- Tensor_11734
Tensor_409 <- Tensor_11735
Tensor_412 <- Tensor_11736
Tensor_413 <- Tensor_11737
Tensor_416 <- Tensor_11738
Tensor_417 <- Tensor_11739
Tensor_420 <- Tensor_11740
Tensor_421 <- Tensor_11741
Tensor_424 <- Tensor_11742
Tensor_425 <- Tensor_11743
Tensor_428 <- Tensor_11744
Tensor_429 <- Tensor_11745
Tensor_432 <- Tensor_11746
Tensor_433 <- Tensor_11747
Tensor_434 <- Tensor_11748
Tensor_435 <- Tensor_11749
Tensor_436 <- Tensor_11750
Tensor_437 <- Tensor_11751
Tensor_444 <- Tensor_11752
Tensor_445 <- Tensor_11753
Tensor_448 <- Tensor_11754
Tensor_449 <- Tensor_11755
Tensor_452 <- Tensor_11756
Tensor_453 <- Tensor_11757
Tensor_456 <- Tensor_11758
Tensor_457 <- Tensor_11759
Tensor_460 <- Tensor_11760
Tensor_461 <- Tensor_11761
Tensor_464 <- Tensor_11762
Tensor_465 <- Tensor_11763
Tensor_468 <- Tensor_11764
Tensor_469 <- Tensor_11765
Tensor_472 <- Tensor_11766
Tensor_473 <- Tensor_11767
Tensor_474 <- Tensor_11768
Tensor_475 <- Tensor_11769
Tensor_476 <- Tensor_11770
Tensor_477 <- Tensor_11771
Tensor_484 <- Tensor_11772
Tensor_485 <- Tensor_11773
Tensor_488 <- Tensor_11774
Tensor_489 <- Tensor_11775
Tensor_490 <- Tensor_11776
Tensor_491 <- Tensor_11777
Tensor_501 <- Tensor_11787
Tensor_529 <- Tensor_11815
Tensor_531 <- Tensor_11817
Tensor_533 <- Tensor_11819
Tensor_543 <- Tensor_11827
Tensor_545 <- Tensor_11831
Tensor_558 <- Tensor_11844
Tensor_564 <- Tensor_11850
Tensor_572 <- Tensor_11858
Tensor_574 <- Tensor_11860
Tensor_576 <- Tensor_11862
Tensor_577 <- Tensor_11863
Tensor_578 <- Tensor_11864
Tensor_580 <- Tensor_11866
Tensor_582 <- Tensor_11868
Tensor_588 <- Tensor_11874
Tensor_590 <- Tensor_11876
Tensor_596 <- Tensor_11882
Tensor_598 <- Tensor_11884
Tensor_620 <- Tensor_11896
Tensor_628 <- Tensor_11906
Tensor_630 <- Tensor_11908
Tensor_640 <- Tensor_11912
Tensor_648 <- Tensor_11922
Tensor_650 <- Tensor_11924
Tensor_662 <- Tensor_11942
Tensor_664 <- Tensor_11946
Tensor_665 <- Tensor_11947
Tensor_666 <- Tensor_11948
Tensor_668 <- Tensor_11950
Tensor_670 <- Tensor_11951
Tensor_684 <- Tensor_11959
Tensor_686 <- Tensor_11961
Tensor_687 <- Tensor_11962
Tensor_688 <- Tensor_11963
Tensor_692 <- Tensor_11967
Tensor_694 <- Tensor_11968
Tensor_700 <- Tensor_11973
Tensor_702 <- Tensor_11975
Tensor_706 <- Tensor_11979
Tensor_708 <- Tensor_11981
Tensor_709 <- Tensor_11982
Tensor_710 <- Tensor_11983
Tensor_712 <- Tensor_11985
Tensor_714 <- Tensor_11987
Tensor_726 <- Tensor_11999
Tensor_728 <- Tensor_12001
Tensor_729 <- Tensor_12002
Tensor_730 <- Tensor_12003
Tensor_732 <- Tensor_12005
Tensor_734 <- Tensor_12007
Tensor_740 <- Tensor_12013
Tensor_742 <- Tensor_12015
Tensor_748 <- Tensor_12021
Tensor_750 <- Tensor_12023
Tensor_772 <- Tensor_12035
Tensor_780 <- Tensor_12045
Tensor_782 <- Tensor_12047
Tensor_792 <- Tensor_11912
Tensor_800 <- Tensor_12059
Tensor_802 <- Tensor_12061
Tensor_814 <- Tensor_12079
Tensor_816 <- Tensor_12083
Tensor_817 <- Tensor_12084
Tensor_818 <- Tensor_12085
Tensor_820 <- Tensor_12087
Tensor_822 <- Tensor_12088
Tensor_837 <- Tensor_12098
Tensor_839 <- Tensor_12100
Tensor_847 <- Tensor_12107
Tensor_849 <- Tensor_12109
Tensor_850 <- Tensor_12110
Tensor_851 <- Tensor_12111
Tensor_855 <- Tensor_12115
Tensor_857 <- Tensor_12116
Tensor_863 <- Tensor_12121
Tensor_865 <- Tensor_12123
Tensor_869 <- Tensor_12127
Tensor_871 <- Tensor_12129
Tensor_872 <- Tensor_12130
Tensor_873 <- Tensor_12131
Tensor_875 <- Tensor_12133
Tensor_877 <- Tensor_12135
Tensor_890 <- Tensor_12148
Tensor_892 <- Tensor_12150
Tensor_900 <- Tensor_12158
Tensor_902 <- Tensor_12160
Tensor_903 <- Tensor_12161
Tensor_904 <- Tensor_12162
Tensor_906 <- Tensor_12164
Tensor_908 <- Tensor_12166
Tensor_914 <- Tensor_12172
Tensor_916 <- Tensor_12174
Tensor_922 <- Tensor_12180
Tensor_924 <- Tensor_12182
Tensor_946 <- Tensor_12194
Tensor_954 <- Tensor_12204
Tensor_956 <- Tensor_12206
Tensor_966 <- Tensor_11912
Tensor_974 <- Tensor_12218
Tensor_976 <- Tensor_12220
Tensor_988 <- Tensor_12238
Tensor_990 <- Tensor_12242
Tensor_991 <- Tensor_12243
Tensor_992 <- Tensor_12244
Tensor_994 <- Tensor_12246
Tensor_996 <- Tensor_12247
Tensor_1011 <- Tensor_12257
Tensor_1013 <- Tensor_12259
Tensor_1021 <- Tensor_12266
Tensor_1023 <- Tensor_12268
Tensor_1024 <- Tensor_12269
Tensor_1025 <- Tensor_12270
Tensor_1029 <- Tensor_12274
Tensor_1031 <- Tensor_12275
Tensor_1037 <- Tensor_12280
Tensor_1039 <- Tensor_12282
Tensor_1043 <- Tensor_12286
Tensor_1045 <- Tensor_12288
Tensor_1046 <- Tensor_12289
Tensor_1047 <- Tensor_12290
Tensor_1049 <- Tensor_12292
Tensor_1051 <- Tensor_12294
Tensor_1064 <- Tensor_12307
Tensor_1066 <- Tensor_12309
Tensor_1074 <- Tensor_12317
Tensor_1076 <- Tensor_12319
Tensor_1077 <- Tensor_12320
Tensor_1078 <- Tensor_12321
Tensor_1080 <- Tensor_12323
Tensor_1082 <- Tensor_12325
Tensor_1088 <- Tensor_12331
Tensor_1090 <- Tensor_12333
Tensor_1096 <- Tensor_12339
Tensor_1098 <- Tensor_12341
Tensor_1120 <- Tensor_12353
Tensor_1128 <- Tensor_12363
Tensor_1130 <- Tensor_12365
Tensor_1140 <- Tensor_11912
Tensor_1148 <- Tensor_12377
Tensor_1150 <- Tensor_12379
Tensor_1162 <- Tensor_12397
Tensor_1164 <- Tensor_12401
Tensor_1165 <- Tensor_12402
Tensor_1166 <- Tensor_12403
Tensor_1168 <- Tensor_12405
Tensor_1170 <- Tensor_12406
Tensor_1185 <- Tensor_12416
Tensor_1187 <- Tensor_12418
Tensor_1195 <- Tensor_12425
Tensor_1197 <- Tensor_12427
Tensor_1198 <- Tensor_12428
Tensor_1199 <- Tensor_12429
Tensor_1203 <- Tensor_12433
Tensor_1205 <- Tensor_12434
Tensor_1211 <- Tensor_12439
Tensor_1213 <- Tensor_12441
Tensor_1217 <- Tensor_12445
Tensor_1219 <- Tensor_12447
Tensor_1220 <- Tensor_12448
Tensor_1221 <- Tensor_12449
Tensor_1223 <- Tensor_12451
Tensor_1225 <- Tensor_12453
Tensor_1238 <- Tensor_12466
Tensor_1240 <- Tensor_12468
Tensor_1248 <- Tensor_12476
Tensor_1250 <- Tensor_12478
Tensor_1251 <- Tensor_12479
Tensor_1252 <- Tensor_12480
Tensor_1254 <- Tensor_12482
Tensor_1256 <- Tensor_12484
Tensor_1262 <- Tensor_12490
Tensor_1264 <- Tensor_12492
Tensor_1270 <- Tensor_12498
Tensor_1272 <- Tensor_12500
Tensor_1294 <- Tensor_12512
Tensor_1302 <- Tensor_12522
Tensor_1304 <- Tensor_12524
Tensor_1314 <- Tensor_11912
Tensor_1322 <- Tensor_12536
Tensor_1324 <- Tensor_12538
Tensor_1336 <- Tensor_12556
Tensor_1338 <- Tensor_12560
Tensor_1339 <- Tensor_12561
Tensor_1340 <- Tensor_12562
Tensor_1342 <- Tensor_12564
Tensor_1344 <- Tensor_12565
Tensor_1359 <- Tensor_12575
Tensor_1361 <- Tensor_12577
Tensor_1369 <- Tensor_12584
Tensor_1371 <- Tensor_12586
Tensor_1372 <- Tensor_12587
Tensor_1373 <- Tensor_12588
Tensor_1377 <- Tensor_12592
Tensor_1379 <- Tensor_12593
Tensor_1385 <- Tensor_12598
Tensor_1387 <- Tensor_12600
Tensor_1391 <- Tensor_12604
Tensor_1393 <- Tensor_12606
Tensor_1394 <- Tensor_12607
Tensor_1395 <- Tensor_12608
Tensor_1397 <- Tensor_12610
Tensor_1399 <- Tensor_12612
Tensor_1412 <- Tensor_12625
Tensor_1414 <- Tensor_12627
Tensor_1422 <- Tensor_12635
Tensor_1424 <- Tensor_12637
Tensor_1425 <- Tensor_12638
Tensor_1426 <- Tensor_12639
Tensor_1428 <- Tensor_12641
Tensor_1430 <- Tensor_12643
Tensor_1436 <- Tensor_12649
Tensor_1438 <- Tensor_12651
Tensor_1444 <- Tensor_12657
Tensor_1446 <- Tensor_12659
Tensor_1468 <- Tensor_12671
Tensor_1476 <- Tensor_12681
Tensor_1478 <- Tensor_12683
Tensor_1488 <- Tensor_11912
Tensor_1496 <- Tensor_12695
Tensor_1498 <- Tensor_12697
Tensor_1510 <- Tensor_12715
Tensor_1512 <- Tensor_12719
Tensor_1513 <- Tensor_12720
Tensor_1514 <- Tensor_12721
Tensor_1516 <- Tensor_12723
Tensor_1518 <- Tensor_12724
Tensor_1533 <- Tensor_12734
Tensor_1535 <- Tensor_12736
Tensor_1543 <- Tensor_12743
Tensor_1545 <- Tensor_12745
Tensor_1546 <- Tensor_12746
Tensor_1547 <- Tensor_12747
Tensor_1551 <- Tensor_12751
Tensor_1553 <- Tensor_12752
Tensor_1559 <- Tensor_12757
Tensor_1561 <- Tensor_12759
Tensor_1565 <- Tensor_12763
Tensor_1567 <- Tensor_12765
Tensor_1568 <- Tensor_12766
Tensor_1569 <- Tensor_12767
Tensor_1571 <- Tensor_12769
Tensor_1573 <- Tensor_12771
Tensor_1586 <- Tensor_12784
Tensor_1588 <- Tensor_12786
Tensor_1596 <- Tensor_12794
Tensor_1598 <- Tensor_12796
Tensor_1599 <- Tensor_12797
Tensor_1600 <- Tensor_12798
Tensor_1602 <- Tensor_12800
Tensor_1604 <- Tensor_12802
Tensor_1610 <- Tensor_12808
Tensor_1612 <- Tensor_12810
Tensor_1618 <- Tensor_12816
Tensor_1620 <- Tensor_12818
Tensor_1642 <- Tensor_12830
Tensor_1650 <- Tensor_12840
Tensor_1652 <- Tensor_12842
Tensor_1662 <- Tensor_11912
Tensor_1670 <- Tensor_12854
Tensor_1672 <- Tensor_12856
Tensor_1684 <- Tensor_12874
Tensor_1686 <- Tensor_12878
Tensor_1687 <- Tensor_12879
Tensor_1688 <- Tensor_12880
Tensor_1690 <- Tensor_12882
Tensor_1692 <- Tensor_12883
Tensor_1707 <- Tensor_12893
Tensor_1709 <- Tensor_12895
Tensor_1717 <- Tensor_12902
Tensor_1719 <- Tensor_12904
Tensor_1720 <- Tensor_12905
Tensor_1721 <- Tensor_12906
Tensor_1725 <- Tensor_12910
Tensor_1727 <- Tensor_12911
Tensor_1733 <- Tensor_12916
Tensor_1735 <- Tensor_12918
Tensor_1739 <- Tensor_12922
Tensor_1741 <- Tensor_12924
Tensor_1742 <- Tensor_12925
Tensor_1743 <- Tensor_12926
Tensor_1745 <- Tensor_12928
Tensor_1747 <- Tensor_12930
Tensor_1760 <- Tensor_12943
Tensor_1762 <- Tensor_12945
Tensor_1770 <- Tensor_12953
Tensor_1772 <- Tensor_12955
Tensor_1773 <- Tensor_12956
Tensor_1774 <- Tensor_12957
Tensor_1776 <- Tensor_12959
Tensor_1778 <- Tensor_12961
Tensor_1784 <- Tensor_12967
Tensor_1786 <- Tensor_12969
Tensor_1792 <- Tensor_12975
Tensor_1794 <- Tensor_12977
Tensor_1816 <- Tensor_12989
Tensor_1824 <- Tensor_12999
Tensor_1826 <- Tensor_13001
Tensor_1836 <- Tensor_11912
Tensor_1844 <- Tensor_13013
Tensor_1846 <- Tensor_13015
Tensor_1858 <- Tensor_13033
Tensor_1860 <- Tensor_13037
Tensor_1861 <- Tensor_13038
Tensor_1862 <- Tensor_13039
Tensor_1864 <- Tensor_13041
Tensor_1866 <- Tensor_13042
Tensor_1881 <- Tensor_13052
Tensor_1883 <- Tensor_13054
Tensor_1891 <- Tensor_13061
Tensor_1893 <- Tensor_13063
Tensor_1894 <- Tensor_13064
Tensor_1895 <- Tensor_13065
Tensor_1899 <- Tensor_13069
Tensor_1901 <- Tensor_13070
Tensor_1907 <- Tensor_13075
Tensor_1909 <- Tensor_13077
Tensor_1913 <- Tensor_13081
Tensor_1915 <- Tensor_13083
Tensor_1916 <- Tensor_13084
Tensor_1917 <- Tensor_13085
Tensor_1919 <- Tensor_13087
Tensor_1921 <- Tensor_13089
Tensor_1934 <- Tensor_13102
Tensor_1936 <- Tensor_13104
Tensor_1944 <- Tensor_13112
Tensor_1946 <- Tensor_13114
Tensor_1947 <- Tensor_13115
Tensor_1948 <- Tensor_13116
Tensor_1950 <- Tensor_13118
Tensor_1952 <- Tensor_13120
Tensor_1958 <- Tensor_13126
Tensor_1960 <- Tensor_13128
Tensor_1966 <- Tensor_13134
Tensor_1968 <- Tensor_13136
Tensor_1990 <- Tensor_13148
Tensor_1998 <- Tensor_13158
Tensor_2000 <- Tensor_13160
Tensor_2010 <- Tensor_11912
Tensor_2018 <- Tensor_13172
Tensor_2020 <- Tensor_13174
Tensor_2032 <- Tensor_13192
Tensor_2034 <- Tensor_13196
Tensor_2035 <- Tensor_13197
Tensor_2036 <- Tensor_13198
Tensor_2038 <- Tensor_13200
Tensor_2040 <- Tensor_13201
Tensor_2055 <- Tensor_13211
Tensor_2057 <- Tensor_13213
Tensor_2065 <- Tensor_13220
Tensor_2067 <- Tensor_13222
Tensor_2068 <- Tensor_13223
Tensor_2069 <- Tensor_13224
Tensor_2073 <- Tensor_13228
Tensor_2075 <- Tensor_13229
Tensor_2081 <- Tensor_13234
Tensor_2083 <- Tensor_13236
Tensor_2087 <- Tensor_13240
Tensor_2089 <- Tensor_13242
Tensor_2090 <- Tensor_13243
Tensor_2091 <- Tensor_13244
Tensor_2093 <- Tensor_13246
Tensor_2095 <- Tensor_13248
Tensor_2108 <- Tensor_13261
Tensor_2110 <- Tensor_13263
Tensor_2118 <- Tensor_13271
Tensor_2120 <- Tensor_13273
Tensor_2121 <- Tensor_13274
Tensor_2122 <- Tensor_13275
Tensor_2124 <- Tensor_13277
Tensor_2126 <- Tensor_13279
Tensor_2132 <- Tensor_13285
Tensor_2134 <- Tensor_13287
Tensor_2140 <- Tensor_13293
Tensor_2142 <- Tensor_13295
Tensor_2164 <- Tensor_13307
Tensor_2172 <- Tensor_13317
Tensor_2174 <- Tensor_13319
Tensor_2184 <- Tensor_11912
Tensor_2192 <- Tensor_13331
Tensor_2194 <- Tensor_13333
Tensor_2206 <- Tensor_13351
Tensor_2208 <- Tensor_13355
Tensor_2209 <- Tensor_13356
Tensor_2210 <- Tensor_13357
Tensor_2212 <- Tensor_13359
Tensor_2214 <- Tensor_13360
Tensor_2229 <- Tensor_13370
Tensor_2231 <- Tensor_13372
Tensor_2239 <- Tensor_13379
Tensor_2241 <- Tensor_13381
Tensor_2242 <- Tensor_13382
Tensor_2243 <- Tensor_13383
Tensor_2247 <- Tensor_13387
Tensor_2249 <- Tensor_13388
Tensor_2255 <- Tensor_13393
Tensor_2257 <- Tensor_13395
Tensor_2261 <- Tensor_13399
Tensor_2263 <- Tensor_13401
Tensor_2264 <- Tensor_13402
Tensor_2265 <- Tensor_13403
Tensor_2267 <- Tensor_13405
Tensor_2269 <- Tensor_13407
Tensor_2282 <- Tensor_13420
Tensor_2284 <- Tensor_13422
Tensor_2292 <- Tensor_13430
Tensor_2294 <- Tensor_13432
Tensor_2295 <- Tensor_13433
Tensor_2296 <- Tensor_13434
Tensor_2298 <- Tensor_13436
Tensor_2300 <- Tensor_13438
Tensor_2306 <- Tensor_13444
Tensor_2308 <- Tensor_13446
Tensor_2314 <- Tensor_13452
Tensor_2316 <- Tensor_13454
Tensor_2338 <- Tensor_13466
Tensor_2346 <- Tensor_13476
Tensor_2348 <- Tensor_13478
Tensor_2358 <- Tensor_11912
Tensor_2366 <- Tensor_13490
Tensor_2368 <- Tensor_13492
Tensor_2380 <- Tensor_13510
Tensor_2382 <- Tensor_13514
Tensor_2383 <- Tensor_13515
Tensor_2384 <- Tensor_13516
Tensor_2386 <- Tensor_13518
Tensor_2388 <- Tensor_13519
Tensor_2403 <- Tensor_13529
Tensor_2405 <- Tensor_13531
Tensor_2413 <- Tensor_13538
Tensor_2415 <- Tensor_13540
Tensor_2416 <- Tensor_13541
Tensor_2417 <- Tensor_13542
Tensor_2421 <- Tensor_13546
Tensor_2423 <- Tensor_13547
Tensor_2429 <- Tensor_13552
Tensor_2431 <- Tensor_13554
Tensor_2435 <- Tensor_13558
Tensor_2437 <- Tensor_13560
Tensor_2438 <- Tensor_13561
Tensor_2439 <- Tensor_13562
Tensor_2441 <- Tensor_13564
Tensor_2443 <- Tensor_13566
Tensor_2456 <- Tensor_13579
Tensor_2458 <- Tensor_13581
Tensor_2466 <- Tensor_13589
Tensor_2468 <- Tensor_13591
Tensor_2469 <- Tensor_13592
Tensor_2470 <- Tensor_13593
Tensor_2472 <- Tensor_13595
Tensor_2474 <- Tensor_13597
Tensor_2480 <- Tensor_13603
Tensor_2482 <- Tensor_13605
Tensor_2488 <- Tensor_13611
Tensor_2490 <- Tensor_13613
Tensor_2512 <- Tensor_13625
Tensor_2520 <- Tensor_13635
Tensor_2522 <- Tensor_13637
Tensor_2532 <- Tensor_11912
Tensor_2540 <- Tensor_13649
Tensor_2542 <- Tensor_13651
Tensor_2554 <- Tensor_13669
Tensor_2556 <- Tensor_13673
Tensor_2557 <- Tensor_13674
Tensor_2558 <- Tensor_13675
Tensor_2560 <- Tensor_13677
Tensor_2562 <- Tensor_13678
Tensor_2577 <- Tensor_13688
Tensor_2579 <- Tensor_13690
Tensor_2587 <- Tensor_13697
Tensor_2589 <- Tensor_13699
Tensor_2590 <- Tensor_13700
Tensor_2591 <- Tensor_13701
Tensor_2595 <- Tensor_13705
Tensor_2597 <- Tensor_13706
Tensor_2603 <- Tensor_13711
Tensor_2605 <- Tensor_13713
Tensor_2609 <- Tensor_13717
Tensor_2611 <- Tensor_13719
Tensor_2612 <- Tensor_13720
Tensor_2613 <- Tensor_13721
Tensor_2615 <- Tensor_13723
Tensor_2617 <- Tensor_13725
Tensor_2630 <- Tensor_13738
Tensor_2632 <- Tensor_13740
Tensor_2644 <- Tensor_13752
Tensor_2646 <- Tensor_13754
Tensor_2647 <- Tensor_13755
Tensor_2648 <- Tensor_13756
Tensor_2650 <- Tensor_13758
Tensor_2652 <- Tensor_13760
Tensor_2658 <- Tensor_13766
Tensor_2662 <- Tensor_13770
Tensor_2668 <- Tensor_13781

== Translation ==
Source graph:
Tensor_24: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t2, o2], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11542: 3072 Bnn)
Tensor_25: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t2, o2], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11543: 3072 Bnn)
Tensor_28: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t2, o2], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11544: 3072 Bnn)
Tensor_29: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t2, o2], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11545: 3072 Bnn)
Tensor_36: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t2, o2], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11550: 12288 Bnn)
Tensor_37: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t2, o2], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11551: 12288 Bnn)
Tensor_44: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t2, o2], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11552: 3072 Bnn)
Tensor_45: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t2, o2], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11553: 3072 Bnn)
Tensor_64: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t2, o2], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11562: 3072 Bnn)
Tensor_65: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t2, o2], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11563: 3072 Bnn)
Tensor_68: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t2, o2], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11564: 3072 Bnn)
Tensor_69: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t2, o2], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11565: 3072 Bnn)
Tensor_76: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t2, o2], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11570: 12288 Bnn)
Tensor_77: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t2, o2], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11571: 12288 Bnn)
Tensor_84: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t2, o2], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11572: 3072 Bnn)
Tensor_85: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t2, o2], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11573: 3072 Bnn)
Tensor_104: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t2, o2], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11582: 3072 Bnn)
Tensor_105: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t2, o2], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11583: 3072 Bnn)
Tensor_108: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t2, o2], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11584: 3072 Bnn)
Tensor_109: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t2, o2], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11585: 3072 Bnn)
Tensor_116: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t2, o2], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11590: 12288 Bnn)
Tensor_117: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t2, o2], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11591: 12288 Bnn)
Tensor_124: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t2, o2], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11592: 3072 Bnn)
Tensor_125: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t2, o2], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11593: 3072 Bnn)
Tensor_144: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t2, o2], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11602: 3072 Bnn)
Tensor_145: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t2, o2], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11603: 3072 Bnn)
Tensor_148: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t2, o2], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11604: 3072 Bnn)
Tensor_149: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t2, o2], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11605: 3072 Bnn)
Tensor_156: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t2, o2], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11610: 12288 Bnn)
Tensor_157: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t2, o2], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11611: 12288 Bnn)
Tensor_164: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t2, o2], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11612: 3072 Bnn)
Tensor_165: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t2, o2], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11613: 3072 Bnn)
Tensor_184: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t2, o2], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11622: 3072 Bnn)
Tensor_185: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t2, o2], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11623: 3072 Bnn)
Tensor_188: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t2, o2], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11624: 3072 Bnn)
Tensor_189: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t2, o2], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11625: 3072 Bnn)
Tensor_196: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t2, o2], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11630: 12288 Bnn)
Tensor_197: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t2, o2], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11631: 12288 Bnn)
Tensor_204: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t2, o2], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11632: 3072 Bnn)
Tensor_205: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t2, o2], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11633: 3072 Bnn)
Tensor_224: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t2, o2], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11642: 3072 Bnn)
Tensor_225: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t2, o2], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11643: 3072 Bnn)
Tensor_228: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t2, o2], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11644: 3072 Bnn)
Tensor_229: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t2, o2], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11645: 3072 Bnn)
Tensor_236: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t2, o2], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11650: 12288 Bnn)
Tensor_237: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t2, o2], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11651: 12288 Bnn)
Tensor_244: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t2, o2], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11652: 3072 Bnn)
Tensor_245: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t2, o2], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11653: 3072 Bnn)
Tensor_264: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t2, o2], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11662: 3072 Bnn)
Tensor_265: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t2, o2], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11663: 3072 Bnn)
Tensor_268: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t2, o2], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11664: 3072 Bnn)
Tensor_269: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t2, o2], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11665: 3072 Bnn)
Tensor_276: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t2, o2], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11670: 12288 Bnn)
Tensor_277: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t2, o2], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11671: 12288 Bnn)
Tensor_284: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t2, o2], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11672: 3072 Bnn)
Tensor_285: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t2, o2], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11673: 3072 Bnn)
Tensor_304: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t2, o2], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11682: 3072 Bnn)
Tensor_305: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t2, o2], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11683: 3072 Bnn)
Tensor_308: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t2, o2], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11684: 3072 Bnn)
Tensor_309: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t2, o2], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11685: 3072 Bnn)
Tensor_316: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t2, o2], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11690: 12288 Bnn)
Tensor_317: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t2, o2], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11691: 12288 Bnn)
Tensor_324: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t2, o2], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11692: 3072 Bnn)
Tensor_325: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t2, o2], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11693: 3072 Bnn)
Tensor_344: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t2, o2], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11702: 3072 Bnn)
Tensor_345: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t2, o2], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11703: 3072 Bnn)
Tensor_348: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t2, o2], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11704: 3072 Bnn)
Tensor_349: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t2, o2], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11705: 3072 Bnn)
Tensor_356: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t2, o2], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11710: 12288 Bnn)
Tensor_357: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t2, o2], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11711: 12288 Bnn)
Tensor_364: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t2, o2], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11712: 3072 Bnn)
Tensor_365: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t2, o2], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11713: 3072 Bnn)
Tensor_384: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t2, o2], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11722: 3072 Bnn)
Tensor_385: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t2, o2], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11723: 3072 Bnn)
Tensor_388: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t2, o2], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11724: 3072 Bnn)
Tensor_389: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t2, o2], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11725: 3072 Bnn)
Tensor_396: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t2, o2], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11730: 12288 Bnn)
Tensor_397: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t2, o2], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11731: 12288 Bnn)
Tensor_404: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t2, o2], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11732: 3072 Bnn)
Tensor_405: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t2, o2], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11733: 3072 Bnn)
Tensor_424: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t2, o2], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11742: 3072 Bnn)
Tensor_425: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t2, o2], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11743: 3072 Bnn)
Tensor_428: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t2, o2], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11744: 3072 Bnn)
Tensor_429: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t2, o2], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11745: 3072 Bnn)
Tensor_436: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t2, o2], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11750: 12288 Bnn)
Tensor_437: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t2, o2], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11751: 12288 Bnn)
Tensor_444: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t2, o2], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11752: 3072 Bnn)
Tensor_445: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t2, o2], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11753: 3072 Bnn)
Tensor_464: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t2, o2], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11762: 3072 Bnn)
Tensor_465: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t2, o2], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11763: 3072 Bnn)
Tensor_468: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t2, o2], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11764: 3072 Bnn)
Tensor_469: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t2, o2], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11765: 3072 Bnn)
Tensor_476: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t2, o2], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11770: 12288 Bnn)
Tensor_477: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t2, o2], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11771: 12288 Bnn)
Tensor_484: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t2, o2], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11772: 3072 Bnn)
Tensor_485: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t2, o2], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11773: 3072 Bnn)
Tensor_488: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t2, o2], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11774: 3072 Bnn)
Tensor_489: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v2, t2, o2], attr<AutogradTensor> = (leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11775: 3072 Bnn)
Tensor_531: shape = [128, 3, 224, 224], stride = [150528, 50176, 224, 1], offset = 0, size = 19267584, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11817: 38535168 Bnn)
Tensor_533: shape = [768, 3, 16, 16], stride = [768, 256, 16, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11819: 1179648 Bnn)
Tensor_543: shape = [128, 1, 768], stride = [0, 768, 1], offset = 0, size = 768, datatype = float32, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11827: 3072 Bnn)
Tensor_545: shape = [128, 196, 768], stride = [150528, 768, 1], offset = 0, size = 19267584, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11831: 38535168 Bnn)
Tensor_558: shape = [1, 197], stride = [197, 1], offset = 0, size = 197, datatype = int64, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11844: 1576 Bnn)
Tensor_564: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11840: 38731776 Bnn)
Tensor_572: shape = [128, 197, 768], stride = [197, 1, 0], offset = 0, size = 25216, datatype = float32, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11836: 50432 Bnn)
Tensor_574: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11860: 38731776 Bnn)
Tensor_576: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11862: 38731776 Bnn)
Tensor_577: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11863: 50432 Bnn)
Tensor_578: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11864: 50432 Bnn)
Tensor_580: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11866: 19365888 Bnn)
Tensor_582: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11868: 1179648 Bnn)
Tensor_588: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11874: 19365888 Bnn)
Tensor_590: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11876: 1179648 Bnn)
Tensor_596: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11882: 19365888 Bnn)
Tensor_598: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11884: 1179648 Bnn)
Tensor_620: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11896: 19365888 Bnn)
Tensor_628: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11898: 19365888 Bnn)
Tensor_630: shape = [1536, 64, 197], stride = [12608, 1, 64], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11890: 19365888 Bnn)
Tensor_640: shape = [128, 12, 197, 197], stride = [197, 0, 0, 1], offset = 0, size = 25216, datatype = boolean, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11852: 12608 Bnn)
Tensor_648: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11922: 119221248 Bnn)
Tensor_650: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11914: 59610624 Bnn)
Tensor_662: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11942: 77463552 Bnn)
Tensor_664: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11940: 38731776 Bnn)
Tensor_665: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11947: 50432 Bnn)
Tensor_666: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11948: 50432 Bnn)
Tensor_668: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11902: 19365888 Bnn)
Tensor_670: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11951: 1179648 Bnn)
Tensor_684: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11959: 38731776 Bnn)
Tensor_686: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11936: 38731776 Bnn)
Tensor_687: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11962: 50432 Bnn)
Tensor_688: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11963: 50432 Bnn)
Tensor_692: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11954: 19365888 Bnn)
Tensor_694: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11968: 4718592 Bnn)
Tensor_700: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11973: 154927104 Bnn)
Tensor_702: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11975: 154927104 Bnn)
Tensor_706: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11979: 154927104 Bnn)
Tensor_708: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11981: 154927104 Bnn)
Tensor_709: shape = [25216], stride = [1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11982: 50432 Bnn)
Tensor_710: shape = [25216], stride = [1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11983: 50432 Bnn)
Tensor_712: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11971: 77463552 Bnn)
Tensor_714: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11987: 4718592 Bnn)
Tensor_726: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11999: 38731776 Bnn)
Tensor_728: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11997: 38731776 Bnn)
Tensor_729: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12002: 50432 Bnn)
Tensor_730: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12003: 50432 Bnn)
Tensor_732: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11995: 19365888 Bnn)
Tensor_734: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12007: 1179648 Bnn)
Tensor_740: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12013: 19365888 Bnn)
Tensor_742: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12015: 1179648 Bnn)
Tensor_748: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12021: 19365888 Bnn)
Tensor_750: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12023: 1179648 Bnn)
Tensor_772: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12035: 19365888 Bnn)
Tensor_780: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12037: 19365888 Bnn)
Tensor_782: shape = [1536, 64, 197], stride = [12608, 1, 64], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12029: 19365888 Bnn)
Tensor_792: shape = [128, 12, 197, 197], stride = [197, 0, 0, 1], offset = 0, size = 25216, datatype = boolean, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11852: 12608 Bnn)
Tensor_800: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12059: 119221248 Bnn)
Tensor_802: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12051: 59610624 Bnn)
Tensor_814: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12079: 77463552 Bnn)
Tensor_816: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12077: 38731776 Bnn)
Tensor_817: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12084: 50432 Bnn)
Tensor_818: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12085: 50432 Bnn)
Tensor_820: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12041: 19365888 Bnn)
Tensor_822: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12088: 1179648 Bnn)
Tensor_837: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12098: 19365888 Bnn)
Tensor_839: shape = [128, 197, 768], stride = [1, 0, 0], offset = 0, size = 128, datatype = float16, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12094: 128 Bnn)
Tensor_847: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12107: 38731776 Bnn)
Tensor_849: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12073: 38731776 Bnn)
Tensor_850: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12110: 50432 Bnn)
Tensor_851: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12111: 50432 Bnn)
Tensor_855: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12102: 19365888 Bnn)
Tensor_857: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12116: 4718592 Bnn)
Tensor_863: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12121: 154927104 Bnn)
Tensor_865: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12123: 154927104 Bnn)
Tensor_869: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12127: 154927104 Bnn)
Tensor_871: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12129: 154927104 Bnn)
Tensor_872: shape = [25216], stride = [1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12130: 50432 Bnn)
Tensor_873: shape = [25216], stride = [1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12131: 50432 Bnn)
Tensor_875: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12119: 77463552 Bnn)
Tensor_877: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12135: 4718592 Bnn)
Tensor_890: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12146: 19365888 Bnn)
Tensor_892: shape = [128, 197, 768], stride = [1, 0, 0], offset = 0, size = 128, datatype = float16, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12142: 128 Bnn)
Tensor_900: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12158: 38731776 Bnn)
Tensor_902: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12156: 38731776 Bnn)
Tensor_903: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12161: 50432 Bnn)
Tensor_904: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12162: 50432 Bnn)
Tensor_906: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12152: 19365888 Bnn)
Tensor_908: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12166: 1179648 Bnn)
Tensor_914: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12172: 19365888 Bnn)
Tensor_916: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12174: 1179648 Bnn)
Tensor_922: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12180: 19365888 Bnn)
Tensor_924: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12182: 1179648 Bnn)
Tensor_946: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12194: 19365888 Bnn)
Tensor_954: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12196: 19365888 Bnn)
Tensor_956: shape = [1536, 64, 197], stride = [12608, 1, 64], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12188: 19365888 Bnn)
Tensor_966: shape = [128, 12, 197, 197], stride = [197, 0, 0, 1], offset = 0, size = 25216, datatype = boolean, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11852: 12608 Bnn)
Tensor_974: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12218: 119221248 Bnn)
Tensor_976: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12210: 59610624 Bnn)
Tensor_988: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12238: 77463552 Bnn)
Tensor_990: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12236: 38731776 Bnn)
Tensor_991: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12243: 50432 Bnn)
Tensor_992: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12244: 50432 Bnn)
Tensor_994: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12200: 19365888 Bnn)
Tensor_996: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12247: 1179648 Bnn)
Tensor_1011: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12257: 19365888 Bnn)
Tensor_1013: shape = [128, 197, 768], stride = [1, 0, 0], offset = 0, size = 128, datatype = float16, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12253: 128 Bnn)
Tensor_1021: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12266: 38731776 Bnn)
Tensor_1023: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12232: 38731776 Bnn)
Tensor_1024: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12269: 50432 Bnn)
Tensor_1025: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12270: 50432 Bnn)
Tensor_1029: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12261: 19365888 Bnn)
Tensor_1031: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12275: 4718592 Bnn)
Tensor_1037: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12280: 154927104 Bnn)
Tensor_1039: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12282: 154927104 Bnn)
Tensor_1043: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12286: 154927104 Bnn)
Tensor_1045: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12288: 154927104 Bnn)
Tensor_1046: shape = [25216], stride = [1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12289: 50432 Bnn)
Tensor_1047: shape = [25216], stride = [1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12290: 50432 Bnn)
Tensor_1049: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12278: 77463552 Bnn)
Tensor_1051: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12294: 4718592 Bnn)
Tensor_1064: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12305: 19365888 Bnn)
Tensor_1066: shape = [128, 197, 768], stride = [1, 0, 0], offset = 0, size = 128, datatype = float16, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12301: 128 Bnn)
Tensor_1074: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12317: 38731776 Bnn)
Tensor_1076: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12315: 38731776 Bnn)
Tensor_1077: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12320: 50432 Bnn)
Tensor_1078: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12321: 50432 Bnn)
Tensor_1080: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12311: 19365888 Bnn)
Tensor_1082: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12325: 1179648 Bnn)
Tensor_1088: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12331: 19365888 Bnn)
Tensor_1090: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12333: 1179648 Bnn)
Tensor_1096: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12339: 19365888 Bnn)
Tensor_1098: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12341: 1179648 Bnn)
Tensor_1120: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12353: 19365888 Bnn)
Tensor_1128: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12355: 19365888 Bnn)
Tensor_1130: shape = [1536, 64, 197], stride = [12608, 1, 64], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12347: 19365888 Bnn)
Tensor_1140: shape = [128, 12, 197, 197], stride = [197, 0, 0, 1], offset = 0, size = 25216, datatype = boolean, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11852: 12608 Bnn)
Tensor_1148: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12377: 119221248 Bnn)
Tensor_1150: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12369: 59610624 Bnn)
Tensor_1162: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12397: 77463552 Bnn)
Tensor_1164: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12395: 38731776 Bnn)
Tensor_1165: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12402: 50432 Bnn)
Tensor_1166: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12403: 50432 Bnn)
Tensor_1168: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12359: 19365888 Bnn)
Tensor_1170: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12406: 1179648 Bnn)
Tensor_1185: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12416: 19365888 Bnn)
Tensor_1187: shape = [128, 197, 768], stride = [1, 0, 0], offset = 0, size = 128, datatype = float16, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12412: 128 Bnn)
Tensor_1195: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12425: 38731776 Bnn)
Tensor_1197: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12391: 38731776 Bnn)
Tensor_1198: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12428: 50432 Bnn)
Tensor_1199: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12429: 50432 Bnn)
Tensor_1203: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12420: 19365888 Bnn)
Tensor_1205: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12434: 4718592 Bnn)
Tensor_1211: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12439: 154927104 Bnn)
Tensor_1213: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12441: 154927104 Bnn)
Tensor_1217: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12445: 154927104 Bnn)
Tensor_1219: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12447: 154927104 Bnn)
Tensor_1220: shape = [25216], stride = [1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12448: 50432 Bnn)
Tensor_1221: shape = [25216], stride = [1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12449: 50432 Bnn)
Tensor_1223: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12437: 77463552 Bnn)
Tensor_1225: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12453: 4718592 Bnn)
Tensor_1238: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12464: 19365888 Bnn)
Tensor_1240: shape = [128, 197, 768], stride = [1, 0, 0], offset = 0, size = 128, datatype = float16, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12460: 128 Bnn)
Tensor_1248: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12476: 38731776 Bnn)
Tensor_1250: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12474: 38731776 Bnn)
Tensor_1251: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12479: 50432 Bnn)
Tensor_1252: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12480: 50432 Bnn)
Tensor_1254: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12470: 19365888 Bnn)
Tensor_1256: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12484: 1179648 Bnn)
Tensor_1262: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12490: 19365888 Bnn)
Tensor_1264: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12492: 1179648 Bnn)
Tensor_1270: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12498: 19365888 Bnn)
Tensor_1272: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12500: 1179648 Bnn)
Tensor_1294: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12512: 19365888 Bnn)
Tensor_1302: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12514: 19365888 Bnn)
Tensor_1304: shape = [1536, 64, 197], stride = [12608, 1, 64], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12506: 19365888 Bnn)
Tensor_1314: shape = [128, 12, 197, 197], stride = [197, 0, 0, 1], offset = 0, size = 25216, datatype = boolean, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11852: 12608 Bnn)
Tensor_1322: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12536: 119221248 Bnn)
Tensor_1324: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12528: 59610624 Bnn)
Tensor_1336: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12556: 77463552 Bnn)
Tensor_1338: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12554: 38731776 Bnn)
Tensor_1339: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12561: 50432 Bnn)
Tensor_1340: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12562: 50432 Bnn)
Tensor_1342: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12518: 19365888 Bnn)
Tensor_1344: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12565: 1179648 Bnn)
Tensor_1359: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12575: 19365888 Bnn)
Tensor_1361: shape = [128, 197, 768], stride = [1, 0, 0], offset = 0, size = 128, datatype = float16, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12571: 128 Bnn)
Tensor_1369: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12584: 38731776 Bnn)
Tensor_1371: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12550: 38731776 Bnn)
Tensor_1372: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12587: 50432 Bnn)
Tensor_1373: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12588: 50432 Bnn)
Tensor_1377: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12579: 19365888 Bnn)
Tensor_1379: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12593: 4718592 Bnn)
Tensor_1385: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12598: 154927104 Bnn)
Tensor_1387: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12600: 154927104 Bnn)
Tensor_1391: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12604: 154927104 Bnn)
Tensor_1393: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12606: 154927104 Bnn)
Tensor_1394: shape = [25216], stride = [1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12607: 50432 Bnn)
Tensor_1395: shape = [25216], stride = [1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12608: 50432 Bnn)
Tensor_1397: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12596: 77463552 Bnn)
Tensor_1399: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12612: 4718592 Bnn)
Tensor_1412: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12623: 19365888 Bnn)
Tensor_1414: shape = [128, 197, 768], stride = [1, 0, 0], offset = 0, size = 128, datatype = float16, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12619: 128 Bnn)
Tensor_1422: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12635: 38731776 Bnn)
Tensor_1424: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12633: 38731776 Bnn)
Tensor_1425: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12638: 50432 Bnn)
Tensor_1426: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12639: 50432 Bnn)
Tensor_1428: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12629: 19365888 Bnn)
Tensor_1430: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12643: 1179648 Bnn)
Tensor_1436: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12649: 19365888 Bnn)
Tensor_1438: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12651: 1179648 Bnn)
Tensor_1444: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12657: 19365888 Bnn)
Tensor_1446: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12659: 1179648 Bnn)
Tensor_1468: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12671: 19365888 Bnn)
Tensor_1476: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12673: 19365888 Bnn)
Tensor_1478: shape = [1536, 64, 197], stride = [12608, 1, 64], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12665: 19365888 Bnn)
Tensor_1488: shape = [128, 12, 197, 197], stride = [197, 0, 0, 1], offset = 0, size = 25216, datatype = boolean, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11852: 12608 Bnn)
Tensor_1496: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12695: 119221248 Bnn)
Tensor_1498: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12687: 59610624 Bnn)
Tensor_1510: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12715: 77463552 Bnn)
Tensor_1512: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12713: 38731776 Bnn)
Tensor_1513: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12720: 50432 Bnn)
Tensor_1514: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12721: 50432 Bnn)
Tensor_1516: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12677: 19365888 Bnn)
Tensor_1518: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12724: 1179648 Bnn)
Tensor_1533: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12734: 19365888 Bnn)
Tensor_1535: shape = [128, 197, 768], stride = [1, 0, 0], offset = 0, size = 128, datatype = float16, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12730: 128 Bnn)
Tensor_1543: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12743: 38731776 Bnn)
Tensor_1545: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12709: 38731776 Bnn)
Tensor_1546: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12746: 50432 Bnn)
Tensor_1547: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12747: 50432 Bnn)
Tensor_1551: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12738: 19365888 Bnn)
Tensor_1553: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12752: 4718592 Bnn)
Tensor_1559: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12757: 154927104 Bnn)
Tensor_1561: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12759: 154927104 Bnn)
Tensor_1565: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12763: 154927104 Bnn)
Tensor_1567: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12765: 154927104 Bnn)
Tensor_1568: shape = [25216], stride = [1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12766: 50432 Bnn)
Tensor_1569: shape = [25216], stride = [1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12767: 50432 Bnn)
Tensor_1571: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12755: 77463552 Bnn)
Tensor_1573: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12771: 4718592 Bnn)
Tensor_1586: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12782: 19365888 Bnn)
Tensor_1588: shape = [128, 197, 768], stride = [1, 0, 0], offset = 0, size = 128, datatype = float16, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12778: 128 Bnn)
Tensor_1596: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12794: 38731776 Bnn)
Tensor_1598: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12792: 38731776 Bnn)
Tensor_1599: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12797: 50432 Bnn)
Tensor_1600: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12798: 50432 Bnn)
Tensor_1602: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12788: 19365888 Bnn)
Tensor_1604: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12802: 1179648 Bnn)
Tensor_1610: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12808: 19365888 Bnn)
Tensor_1612: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12810: 1179648 Bnn)
Tensor_1618: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12816: 19365888 Bnn)
Tensor_1620: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12818: 1179648 Bnn)
Tensor_1642: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12830: 19365888 Bnn)
Tensor_1650: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12832: 19365888 Bnn)
Tensor_1652: shape = [1536, 64, 197], stride = [12608, 1, 64], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12824: 19365888 Bnn)
Tensor_1662: shape = [128, 12, 197, 197], stride = [197, 0, 0, 1], offset = 0, size = 25216, datatype = boolean, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11852: 12608 Bnn)
Tensor_1670: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12854: 119221248 Bnn)
Tensor_1672: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12846: 59610624 Bnn)
Tensor_1684: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12874: 77463552 Bnn)
Tensor_1686: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12872: 38731776 Bnn)
Tensor_1687: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12879: 50432 Bnn)
Tensor_1688: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12880: 50432 Bnn)
Tensor_1690: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12836: 19365888 Bnn)
Tensor_1692: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12883: 1179648 Bnn)
Tensor_1707: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12893: 19365888 Bnn)
Tensor_1709: shape = [128, 197, 768], stride = [1, 0, 0], offset = 0, size = 128, datatype = float16, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12889: 128 Bnn)
Tensor_1717: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12902: 38731776 Bnn)
Tensor_1719: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12868: 38731776 Bnn)
Tensor_1720: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12905: 50432 Bnn)
Tensor_1721: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12906: 50432 Bnn)
Tensor_1725: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12897: 19365888 Bnn)
Tensor_1727: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12911: 4718592 Bnn)
Tensor_1733: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12916: 154927104 Bnn)
Tensor_1735: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12918: 154927104 Bnn)
Tensor_1739: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12922: 154927104 Bnn)
Tensor_1741: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12924: 154927104 Bnn)
Tensor_1742: shape = [25216], stride = [1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12925: 50432 Bnn)
Tensor_1743: shape = [25216], stride = [1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12926: 50432 Bnn)
Tensor_1745: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12914: 77463552 Bnn)
Tensor_1747: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12930: 4718592 Bnn)
Tensor_1760: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12941: 19365888 Bnn)
Tensor_1762: shape = [128, 197, 768], stride = [1, 0, 0], offset = 0, size = 128, datatype = float16, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12937: 128 Bnn)
Tensor_1770: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12953: 38731776 Bnn)
Tensor_1772: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12951: 38731776 Bnn)
Tensor_1773: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12956: 50432 Bnn)
Tensor_1774: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12957: 50432 Bnn)
Tensor_1776: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12947: 19365888 Bnn)
Tensor_1778: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12961: 1179648 Bnn)
Tensor_1784: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12967: 19365888 Bnn)
Tensor_1786: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12969: 1179648 Bnn)
Tensor_1792: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12975: 19365888 Bnn)
Tensor_1794: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12977: 1179648 Bnn)
Tensor_1816: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12989: 19365888 Bnn)
Tensor_1824: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12991: 19365888 Bnn)
Tensor_1826: shape = [1536, 64, 197], stride = [12608, 1, 64], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12983: 19365888 Bnn)
Tensor_1836: shape = [128, 12, 197, 197], stride = [197, 0, 0, 1], offset = 0, size = 25216, datatype = boolean, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11852: 12608 Bnn)
Tensor_1844: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13013: 119221248 Bnn)
Tensor_1846: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13005: 59610624 Bnn)
Tensor_1858: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13033: 77463552 Bnn)
Tensor_1860: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13031: 38731776 Bnn)
Tensor_1861: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13038: 50432 Bnn)
Tensor_1862: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13039: 50432 Bnn)
Tensor_1864: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12995: 19365888 Bnn)
Tensor_1866: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13042: 1179648 Bnn)
Tensor_1881: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13052: 19365888 Bnn)
Tensor_1883: shape = [128, 197, 768], stride = [1, 0, 0], offset = 0, size = 128, datatype = float16, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13048: 128 Bnn)
Tensor_1891: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13061: 38731776 Bnn)
Tensor_1893: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13027: 38731776 Bnn)
Tensor_1894: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13064: 50432 Bnn)
Tensor_1895: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13065: 50432 Bnn)
Tensor_1899: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13056: 19365888 Bnn)
Tensor_1901: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13070: 4718592 Bnn)
Tensor_1907: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13075: 154927104 Bnn)
Tensor_1909: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13077: 154927104 Bnn)
Tensor_1913: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13081: 154927104 Bnn)
Tensor_1915: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13083: 154927104 Bnn)
Tensor_1916: shape = [25216], stride = [1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13084: 50432 Bnn)
Tensor_1917: shape = [25216], stride = [1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13085: 50432 Bnn)
Tensor_1919: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13073: 77463552 Bnn)
Tensor_1921: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13089: 4718592 Bnn)
Tensor_1934: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13100: 19365888 Bnn)
Tensor_1936: shape = [128, 197, 768], stride = [1, 0, 0], offset = 0, size = 128, datatype = float16, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13096: 128 Bnn)
Tensor_1944: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13112: 38731776 Bnn)
Tensor_1946: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13110: 38731776 Bnn)
Tensor_1947: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13115: 50432 Bnn)
Tensor_1948: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13116: 50432 Bnn)
Tensor_1950: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13106: 19365888 Bnn)
Tensor_1952: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13120: 1179648 Bnn)
Tensor_1958: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13126: 19365888 Bnn)
Tensor_1960: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13128: 1179648 Bnn)
Tensor_1966: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13134: 19365888 Bnn)
Tensor_1968: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13136: 1179648 Bnn)
Tensor_1990: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13148: 19365888 Bnn)
Tensor_1998: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13150: 19365888 Bnn)
Tensor_2000: shape = [1536, 64, 197], stride = [12608, 1, 64], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13142: 19365888 Bnn)
Tensor_2010: shape = [128, 12, 197, 197], stride = [197, 0, 0, 1], offset = 0, size = 25216, datatype = boolean, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11852: 12608 Bnn)
Tensor_2018: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13172: 119221248 Bnn)
Tensor_2020: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13164: 59610624 Bnn)
Tensor_2032: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13192: 77463552 Bnn)
Tensor_2034: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13190: 38731776 Bnn)
Tensor_2035: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13197: 50432 Bnn)
Tensor_2036: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13198: 50432 Bnn)
Tensor_2038: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13154: 19365888 Bnn)
Tensor_2040: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13201: 1179648 Bnn)
Tensor_2055: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13211: 19365888 Bnn)
Tensor_2057: shape = [128, 197, 768], stride = [1, 0, 0], offset = 0, size = 128, datatype = float16, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13207: 128 Bnn)
Tensor_2065: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13220: 38731776 Bnn)
Tensor_2067: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13186: 38731776 Bnn)
Tensor_2068: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13223: 50432 Bnn)
Tensor_2069: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13224: 50432 Bnn)
Tensor_2073: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13215: 19365888 Bnn)
Tensor_2075: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13229: 4718592 Bnn)
Tensor_2081: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13234: 154927104 Bnn)
Tensor_2083: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13236: 154927104 Bnn)
Tensor_2087: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13240: 154927104 Bnn)
Tensor_2089: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13242: 154927104 Bnn)
Tensor_2090: shape = [25216], stride = [1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13243: 50432 Bnn)
Tensor_2091: shape = [25216], stride = [1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13244: 50432 Bnn)
Tensor_2093: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13232: 77463552 Bnn)
Tensor_2095: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13248: 4718592 Bnn)
Tensor_2108: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13259: 19365888 Bnn)
Tensor_2110: shape = [128, 197, 768], stride = [1, 0, 0], offset = 0, size = 128, datatype = float16, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13255: 128 Bnn)
Tensor_2118: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13271: 38731776 Bnn)
Tensor_2120: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13269: 38731776 Bnn)
Tensor_2121: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13274: 50432 Bnn)
Tensor_2122: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13275: 50432 Bnn)
Tensor_2124: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13265: 19365888 Bnn)
Tensor_2126: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13279: 1179648 Bnn)
Tensor_2132: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13285: 19365888 Bnn)
Tensor_2134: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13287: 1179648 Bnn)
Tensor_2140: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13293: 19365888 Bnn)
Tensor_2142: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13295: 1179648 Bnn)
Tensor_2164: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13307: 19365888 Bnn)
Tensor_2172: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13309: 19365888 Bnn)
Tensor_2174: shape = [1536, 64, 197], stride = [12608, 1, 64], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13301: 19365888 Bnn)
Tensor_2184: shape = [128, 12, 197, 197], stride = [197, 0, 0, 1], offset = 0, size = 25216, datatype = boolean, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11852: 12608 Bnn)
Tensor_2192: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13331: 119221248 Bnn)
Tensor_2194: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13323: 59610624 Bnn)
Tensor_2206: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13351: 77463552 Bnn)
Tensor_2208: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13349: 38731776 Bnn)
Tensor_2209: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13356: 50432 Bnn)
Tensor_2210: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13357: 50432 Bnn)
Tensor_2212: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13313: 19365888 Bnn)
Tensor_2214: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13360: 1179648 Bnn)
Tensor_2229: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13370: 19365888 Bnn)
Tensor_2231: shape = [128, 197, 768], stride = [1, 0, 0], offset = 0, size = 128, datatype = float16, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13366: 128 Bnn)
Tensor_2239: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13379: 38731776 Bnn)
Tensor_2241: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13345: 38731776 Bnn)
Tensor_2242: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13382: 50432 Bnn)
Tensor_2243: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13383: 50432 Bnn)
Tensor_2247: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13374: 19365888 Bnn)
Tensor_2249: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13388: 4718592 Bnn)
Tensor_2255: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13393: 154927104 Bnn)
Tensor_2257: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13395: 154927104 Bnn)
Tensor_2261: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13399: 154927104 Bnn)
Tensor_2263: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13401: 154927104 Bnn)
Tensor_2264: shape = [25216], stride = [1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13402: 50432 Bnn)
Tensor_2265: shape = [25216], stride = [1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13403: 50432 Bnn)
Tensor_2267: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13391: 77463552 Bnn)
Tensor_2269: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13407: 4718592 Bnn)
Tensor_2282: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13418: 19365888 Bnn)
Tensor_2284: shape = [128, 197, 768], stride = [1, 0, 0], offset = 0, size = 128, datatype = float16, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13414: 128 Bnn)
Tensor_2292: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13430: 38731776 Bnn)
Tensor_2294: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13428: 38731776 Bnn)
Tensor_2295: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13433: 50432 Bnn)
Tensor_2296: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13434: 50432 Bnn)
Tensor_2298: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13424: 19365888 Bnn)
Tensor_2300: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13438: 1179648 Bnn)
Tensor_2306: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13444: 19365888 Bnn)
Tensor_2308: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13446: 1179648 Bnn)
Tensor_2314: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13452: 19365888 Bnn)
Tensor_2316: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13454: 1179648 Bnn)
Tensor_2338: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13466: 19365888 Bnn)
Tensor_2346: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13468: 19365888 Bnn)
Tensor_2348: shape = [1536, 64, 197], stride = [12608, 1, 64], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13460: 19365888 Bnn)
Tensor_2358: shape = [128, 12, 197, 197], stride = [197, 0, 0, 1], offset = 0, size = 25216, datatype = boolean, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11852: 12608 Bnn)
Tensor_2366: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13490: 119221248 Bnn)
Tensor_2368: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13482: 59610624 Bnn)
Tensor_2380: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13510: 77463552 Bnn)
Tensor_2382: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13508: 38731776 Bnn)
Tensor_2383: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13515: 50432 Bnn)
Tensor_2384: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13516: 50432 Bnn)
Tensor_2386: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13472: 19365888 Bnn)
Tensor_2388: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13519: 1179648 Bnn)
Tensor_2403: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13529: 19365888 Bnn)
Tensor_2405: shape = [128, 197, 768], stride = [1, 0, 0], offset = 0, size = 128, datatype = float16, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13525: 128 Bnn)
Tensor_2413: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13538: 38731776 Bnn)
Tensor_2415: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13504: 38731776 Bnn)
Tensor_2416: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13541: 50432 Bnn)
Tensor_2417: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13542: 50432 Bnn)
Tensor_2421: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13533: 19365888 Bnn)
Tensor_2423: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13547: 4718592 Bnn)
Tensor_2429: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13552: 154927104 Bnn)
Tensor_2431: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13554: 154927104 Bnn)
Tensor_2435: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13558: 154927104 Bnn)
Tensor_2437: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13560: 154927104 Bnn)
Tensor_2438: shape = [25216], stride = [1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13561: 50432 Bnn)
Tensor_2439: shape = [25216], stride = [1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13562: 50432 Bnn)
Tensor_2441: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13550: 77463552 Bnn)
Tensor_2443: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13566: 4718592 Bnn)
Tensor_2456: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13577: 19365888 Bnn)
Tensor_2458: shape = [128, 197, 768], stride = [1, 0, 0], offset = 0, size = 128, datatype = float16, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13573: 128 Bnn)
Tensor_2466: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13589: 38731776 Bnn)
Tensor_2468: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13587: 38731776 Bnn)
Tensor_2469: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13592: 50432 Bnn)
Tensor_2470: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13593: 50432 Bnn)
Tensor_2472: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13583: 19365888 Bnn)
Tensor_2474: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13597: 1179648 Bnn)
Tensor_2480: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13603: 19365888 Bnn)
Tensor_2482: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13605: 1179648 Bnn)
Tensor_2488: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13611: 19365888 Bnn)
Tensor_2490: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13613: 1179648 Bnn)
Tensor_2512: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13625: 19365888 Bnn)
Tensor_2520: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13627: 19365888 Bnn)
Tensor_2522: shape = [1536, 64, 197], stride = [12608, 1, 64], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13619: 19365888 Bnn)
Tensor_2532: shape = [128, 12, 197, 197], stride = [197, 0, 0, 1], offset = 0, size = 25216, datatype = boolean, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11852: 12608 Bnn)
Tensor_2540: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13649: 119221248 Bnn)
Tensor_2542: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13641: 59610624 Bnn)
Tensor_2554: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13669: 77463552 Bnn)
Tensor_2556: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13667: 38731776 Bnn)
Tensor_2557: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13674: 50432 Bnn)
Tensor_2558: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13675: 50432 Bnn)
Tensor_2560: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13631: 19365888 Bnn)
Tensor_2562: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13678: 1179648 Bnn)
Tensor_2577: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13688: 19365888 Bnn)
Tensor_2579: shape = [128, 197, 768], stride = [1, 0, 0], offset = 0, size = 128, datatype = float16, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13684: 128 Bnn)
Tensor_2587: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13697: 38731776 Bnn)
Tensor_2589: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13663: 38731776 Bnn)
Tensor_2590: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13700: 50432 Bnn)
Tensor_2591: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13701: 50432 Bnn)
Tensor_2595: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13692: 19365888 Bnn)
Tensor_2597: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13706: 4718592 Bnn)
Tensor_2603: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13711: 154927104 Bnn)
Tensor_2605: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13713: 154927104 Bnn)
Tensor_2609: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13717: 154927104 Bnn)
Tensor_2611: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13719: 154927104 Bnn)
Tensor_2612: shape = [25216], stride = [1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13720: 50432 Bnn)
Tensor_2613: shape = [25216], stride = [1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13721: 50432 Bnn)
Tensor_2615: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13709: 77463552 Bnn)
Tensor_2617: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13725: 4718592 Bnn)
Tensor_2630: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13736: 19365888 Bnn)
Tensor_2632: shape = [128, 197, 768], stride = [1, 0, 0], offset = 0, size = 128, datatype = float16, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13732: 128 Bnn)
Tensor_2644: shape = [128, 768], stride = [768, 1], offset = 0, size = 98304, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13752: 196608 Bnn)
Tensor_2646: shape = [128, 768], stride = [768, 1], offset = 0, size = 98304, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13754: 196608 Bnn)
Tensor_2647: shape = [128], stride = [1], offset = 0, size = 128, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13755: 256 Bnn)
Tensor_2648: shape = [128], stride = [1], offset = 0, size = 128, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13756: 256 Bnn)
Tensor_2650: shape = [128, 768], stride = [768, 1], offset = 0, size = 98304, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13758: 98304 Bnn)
Tensor_2652: shape = [1000, 768], stride = [768, 1], offset = 0, size = 768000, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13760: 1536000 Bnn)
Tensor_2658: shape = [128, 1000], stride = [1000, 1], offset = 0, size = 128000, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11802: 512000 Bnn)
Tensor_2662: shape = [128, 1000], stride = [1000, 1], offset = 0, size = 128000, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13770: 256000 Bnn)
Tensor_2668: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11795: 4 Bnn)
Operation_13782: operator = ElemwiseUnaryForward, inputs = [Tensor_2668], outputs = [Tensor_13783], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_DIV scal1_float: 1]
Tensor_13783: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_13784: shape = [1], stride = [1], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_13785: operator = ElemwiseUnaryForward, inputs = [Tensor_13784], outputs = [Tensor_13786], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_FILL scal1_float: 65536]
Tensor_13786: shape = [1], stride = [1], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_13787: operator = ReshapeForward, inputs = [Tensor_13783], outputs = [Tensor_13788], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1 stride: 0]
Tensor_13788: shape = [1], stride = [0], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_13789: operator = ElemwiseBinaryForward, inputs = [Tensor_13788, Tensor_13786], outputs = [Tensor_13790], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_MUL]
Tensor_13790: shape = [1], stride = [1], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (!leaf requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_13791: shape = [1], stride = [1], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_13792: operator = ElemwiseBinaryBackward, inputs = [Tensor_13791, Tensor_13788, Tensor_13786, nullptr], outputs = [Tensor_13793, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_MUL]
Tensor_13793: shape = [1], stride = [1], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_13794: operator = SumForward, inputs = [Tensor_13793], outputs = [Tensor_13795], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dims: 0]
Tensor_13795: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_13796: operator = ReshapeForward, inputs = [Tensor_13795], outputs = [Tensor_13797], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1 stride: 0]
Tensor_13797: shape = [1], stride = [0], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_13798: operator = ReshapeBackward, inputs = [Tensor_13797], outputs = [Tensor_13799], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1 stride: 0]
Tensor_13799: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_13800: operator = ElemwiseUnaryBackward, inputs = [Tensor_13799, nullptr, nullptr], outputs = [Tensor_13801], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_DIV scal1_float: 1]
Tensor_13801: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_13802: operator = MeanBackward, inputs = [Tensor_13801], outputs = [Tensor_13803], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dims: 0 divisor: 128]
Tensor_13803: shape = [128], stride = [1], offset = 0, size = 128, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_13804: operator = SumBackward, inputs = [Tensor_13803, nullptr], outputs = [Tensor_13805], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dims: 1]
Tensor_13805: shape = [128, 1000], stride = [1000, 1], offset = 0, size = 128000, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_13806: operator = ElemwiseBinaryBackward, inputs = [Tensor_13805, Tensor_2658, Tensor_2662, nullptr], outputs = [nullptr, Tensor_13807], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_MUL]
Tensor_13807: shape = [128, 1000], stride = [1000, 1], offset = 0, size = 128000, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_13808: operator = SoftmaxBackward, inputs = [Tensor_13807, Tensor_2662], outputs = [Tensor_13809], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [method: SOFTMAX_LOG dim: 1]
Tensor_13809: shape = [128, 1000], stride = [1000, 1], offset = 0, size = 128000, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_13810: operator = TypecastBackward, inputs = [Tensor_13809], outputs = [Tensor_13811], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_13811: shape = [128, 1000], stride = [1000, 1], offset = 0, size = 128000, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_13812: operator = LinearBackward, inputs = [Tensor_13811, Tensor_2650, Tensor_2652], outputs = [Tensor_13813, Tensor_13814, Tensor_13815], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_13813: shape = [128, 768], stride = [768, 1], offset = 0, size = 98304, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_13814: shape = [1000, 768], stride = [768, 1], offset = 0, size = 768000, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_13815: shape = [1000], stride = [1], offset = 0, size = 1000, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_13816: operator = TypecastBackward, inputs = [Tensor_13813], outputs = [Tensor_13817], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_13817: shape = [128, 768], stride = [768, 1], offset = 0, size = 98304, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_13818: operator = LayernormBackward, inputs = [Tensor_13817, Tensor_2644, Tensor_2646, Tensor_488, Tensor_489, Tensor_2647, Tensor_2648], outputs = [Tensor_13819, Tensor_13820, Tensor_13821], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_13819: shape = [128, 768], stride = [768, 1], offset = 0, size = 98304, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_13820: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_13821: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_13822: operator = MeanBackward, inputs = [Tensor_13819], outputs = [Tensor_13823], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dims: 1 divisor: 196]
Tensor_13823: shape = [128, 196, 768], stride = [151296, 768, 1], offset = 768, size = 19365888, datatype = float32, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_13824: operator = ReshapeBackward, inputs = [Tensor_13823], outputs = [Tensor_13825], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 196 shape: 768 stride: 151296 stride: 768 stride: 1 offset: 768]
Tensor_13825: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_13826: operator = ElemwiseBinaryBackward, inputs = [Tensor_13825, nullptr, nullptr, nullptr], outputs = [Tensor_13827, Tensor_13828], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_13827: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_13828: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_13829: operator = ElemwiseUnaryBackward, inputs = [Tensor_13827, nullptr, nullptr], outputs = [Tensor_13830], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 1]
Tensor_13830: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_13831: operator = TypecastBackward, inputs = [Tensor_13828], outputs = [Tensor_13832], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_13832: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_13833: operator = ElemwiseBinaryBackward, inputs = [Tensor_13832, Tensor_2630, Tensor_2632, nullptr], outputs = [Tensor_13834, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_MUL]
Tensor_13834: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_13835: operator = ElemwiseUnaryBackward, inputs = [Tensor_13834, nullptr, nullptr], outputs = [Tensor_13836], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_DIV scal1_float: 0.85]
Tensor_13836: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_13837: operator = ReshapeBackward, inputs = [Tensor_13836], outputs = [Tensor_13838], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 151296 stride: 768 stride: 1]
Tensor_13838: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_13839: operator = LinearBackward, inputs = [Tensor_13838, Tensor_2615, Tensor_2617], outputs = [Tensor_13840, Tensor_13841, Tensor_13842], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_13840: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_13841: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_13842: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_13843: operator = TypecastBackward, inputs = [Tensor_13840], outputs = [Tensor_13844], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_13844: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_13845: operator = LayernormBackward, inputs = [Tensor_13844, Tensor_2609, Tensor_2611, Tensor_476, Tensor_477, Tensor_2612, Tensor_2613], outputs = [Tensor_13846, Tensor_13847, Tensor_13848], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_13846: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_13847: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_13848: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_13849: operator = TypecastBackward, inputs = [Tensor_13846], outputs = [Tensor_13850], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_13850: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_13851: operator = TypecastBackward, inputs = [Tensor_13850], outputs = [Tensor_13852], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_13852: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_13853: operator = ElemwiseUnaryBackward, inputs = [Tensor_13852, Tensor_2603, Tensor_2605], outputs = [Tensor_13854], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_GELU]
Tensor_13854: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_13855: operator = TypecastBackward, inputs = [Tensor_13854], outputs = [Tensor_13856], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_13856: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_13857: operator = LinearBackward, inputs = [Tensor_13856, Tensor_2595, Tensor_2597], outputs = [Tensor_13858, Tensor_13859, Tensor_13860], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_13858: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_13859: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_13860: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_13861: operator = TypecastBackward, inputs = [Tensor_13858], outputs = [Tensor_13862], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_13862: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_13863: operator = ReshapeBackward, inputs = [Tensor_13862], outputs = [Tensor_13864], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 25216 shape: 768 stride: 768 stride: 1]
Tensor_13864: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_13865: operator = LayernormBackward, inputs = [Tensor_13864, Tensor_2587, Tensor_2589, Tensor_484, Tensor_485, Tensor_2590, Tensor_2591], outputs = [Tensor_13866, Tensor_13869, Tensor_13870], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_13866: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_13867: operator = ElemwiseBinaryForward, inputs = [Tensor_13830, Tensor_13866], outputs = [Tensor_13868], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_13868: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_13869: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_13870: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_13871: operator = ElemwiseBinaryBackward, inputs = [Tensor_13868, nullptr, nullptr, nullptr], outputs = [Tensor_13872, Tensor_13873], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_13872: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_13873: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_13874: operator = ElemwiseUnaryBackward, inputs = [Tensor_13872, nullptr, nullptr], outputs = [Tensor_13875], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 1]
Tensor_13875: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_13876: operator = TypecastBackward, inputs = [Tensor_13873], outputs = [Tensor_13877], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_13877: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_13878: operator = ElemwiseBinaryBackward, inputs = [Tensor_13877, Tensor_2577, Tensor_2579, nullptr], outputs = [Tensor_13879, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_MUL]
Tensor_13879: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_13880: operator = ElemwiseUnaryBackward, inputs = [Tensor_13879, nullptr, nullptr], outputs = [Tensor_13881], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_DIV scal1_float: 0.85]
Tensor_13881: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_13882: operator = LinearBackward, inputs = [Tensor_13881, Tensor_2560, Tensor_2562], outputs = [Tensor_13883, Tensor_13884, Tensor_13885], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_13883: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_13884: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_13885: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_13886: operator = TypecastBackward, inputs = [Tensor_13883], outputs = [Tensor_13887], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_13887: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_13888: operator = LayernormBackward, inputs = [Tensor_13887, Tensor_2554, Tensor_2556, Tensor_464, Tensor_465, Tensor_2557, Tensor_2558], outputs = [Tensor_13889, Tensor_13890, Tensor_13891], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_13889: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_13890: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_13891: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_13892: operator = TypecastBackward, inputs = [Tensor_13889], outputs = [Tensor_13893], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_13893: shape = [128, 197, 768], stride = [768, 98304, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_13894: operator = ReshapeBackward, inputs = [Tensor_13893], outputs = [Tensor_13895], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 768 stride: 98304 stride: 1]
Tensor_13895: shape = [197, 128, 768], stride = [98304, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_13896: operator = ReshapeBackward, inputs = [Tensor_13895], outputs = [Tensor_13897], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 197 shape: 128 shape: 768 stride: 98304 stride: 768 stride: 1]
Tensor_13897: shape = [197, 1536, 64], stride = [98304, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_13898: operator = ContiguousBackward, inputs = [Tensor_13897], outputs = [Tensor_13899], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_13899: shape = [197, 1536, 64], stride = [64, 12608, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_13900: operator = ReshapeBackward, inputs = [Tensor_13899], outputs = [Tensor_13901], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 197 shape: 1536 shape: 64 stride: 64 stride: 12608 stride: 1]
Tensor_13901: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_13902: operator = BatchedMatmulBackward, inputs = [Tensor_13901, Tensor_2542, Tensor_2520], outputs = [Tensor_13903, Tensor_13904], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_13903: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_13904: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_13905: operator = TypecastBackward, inputs = [Tensor_13903], outputs = [Tensor_13906], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_13906: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_13907: operator = ReshapeBackward, inputs = [Tensor_13904], outputs = [Tensor_13908], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_13908: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_13909: operator = SoftmaxBackward, inputs = [Tensor_13906, Tensor_2540], outputs = [Tensor_13910], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dim: 2]
Tensor_13910: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_13911: operator = ContiguousBackward, inputs = [Tensor_13908], outputs = [Tensor_13912], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_13912: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_13913: operator = TypecastBackward, inputs = [Tensor_13910], outputs = [Tensor_13914], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_13914: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_13915: operator = ReshapeBackward, inputs = [Tensor_13912], outputs = [Tensor_13916], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 64 stride: 151296 stride: 64 stride: 768 stride: 1]
Tensor_13916: shape = [128, 197, 12, 64], stride = [151296, 768, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_13917: operator = ReshapeBackward, inputs = [Tensor_13914], outputs = [Tensor_13918], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 197 stride: 38809 stride: 197 stride: 1]
Tensor_13918: shape = [128, 12, 197, 197], stride = [465708, 38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_13919: operator = ReshapeBackward, inputs = [Tensor_13916], outputs = [Tensor_13920], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 12 shape: 64 stride: 151296 stride: 768 stride: 64 stride: 1]
Tensor_13920: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_13921: operator = MaskedFillBackward, inputs = [Tensor_13918, Tensor_2532], outputs = [Tensor_13922], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [value_float: -inf]
Tensor_13922: shape = [128, 12, 197, 197], stride = [465708, 38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_13923: operator = LinearBackward, inputs = [Tensor_13920, Tensor_2488, Tensor_2490], outputs = [Tensor_13924, Tensor_13925, Tensor_13926], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_13924: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_13925: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_13926: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_13927: operator = ReshapeBackward, inputs = [Tensor_13922], outputs = [Tensor_13928], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 197 stride: 465708 stride: 38809 stride: 197 stride: 1]
Tensor_13928: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_13929: operator = TypecastBackward, inputs = [Tensor_13924], outputs = [Tensor_13930], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_13930: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_13931: operator = BatchedMatmulBackward, inputs = [Tensor_13928, Tensor_2512, Tensor_2522], outputs = [Tensor_13932, Tensor_13933], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_13932: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_13933: shape = [1536, 64, 197], stride = [12608, 1, 64], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_13934: operator = ReshapeBackward, inputs = [Tensor_13932], outputs = [Tensor_13935], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_13935: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_13936: operator = ReshapeBackward, inputs = [Tensor_13933], outputs = [Tensor_13937], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 64 shape: 197 stride: 12608 stride: 1 stride: 64]
Tensor_13937: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_13938: operator = ContiguousBackward, inputs = [Tensor_13935], outputs = [Tensor_13939], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_13939: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_13940: operator = ReshapeBackward, inputs = [Tensor_13937], outputs = [Tensor_13941], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_13941: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_13942: operator = ReshapeBackward, inputs = [Tensor_13939], outputs = [Tensor_13943], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 64 stride: 151296 stride: 64 stride: 768 stride: 1]
Tensor_13943: shape = [128, 197, 12, 64], stride = [151296, 768, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_13944: operator = ContiguousBackward, inputs = [Tensor_13941], outputs = [Tensor_13945], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_13945: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_13946: operator = ReshapeBackward, inputs = [Tensor_13943], outputs = [Tensor_13947], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 12 shape: 64 stride: 151296 stride: 768 stride: 64 stride: 1]
Tensor_13947: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_13948: operator = ReshapeBackward, inputs = [Tensor_13945], outputs = [Tensor_13949], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 64 stride: 151296 stride: 64 stride: 768 stride: 1]
Tensor_13949: shape = [128, 197, 12, 64], stride = [151296, 768, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_13950: operator = ElemwiseUnaryBackward, inputs = [Tensor_13947, nullptr, nullptr], outputs = [Tensor_13951], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 0.125]
Tensor_13951: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_13952: operator = ReshapeBackward, inputs = [Tensor_13949], outputs = [Tensor_13953], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 12 shape: 64 stride: 151296 stride: 768 stride: 64 stride: 1]
Tensor_13953: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_13954: operator = LinearBackward, inputs = [Tensor_13951, Tensor_2472, Tensor_2474], outputs = [Tensor_13955, Tensor_13956, Tensor_13957], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_13955: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_13956: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_13957: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_13958: operator = LinearBackward, inputs = [Tensor_13953, Tensor_2480, Tensor_2482], outputs = [Tensor_13959, Tensor_13960, Tensor_13961], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_13959: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_13960: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_13961: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_13962: operator = TypecastBackward, inputs = [Tensor_13955], outputs = [Tensor_13963], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_13963: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_13964: operator = ElemwiseBinaryForward, inputs = [Tensor_13930, Tensor_13963], outputs = [Tensor_13965], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_13965: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_13966: operator = TypecastBackward, inputs = [Tensor_13959], outputs = [Tensor_13967], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_13967: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_13968: operator = ElemwiseBinaryForward, inputs = [Tensor_13965, Tensor_13967], outputs = [Tensor_13969], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_13969: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_13970: operator = LayernormBackward, inputs = [Tensor_13969, Tensor_2466, Tensor_2468, Tensor_468, Tensor_469, Tensor_2469, Tensor_2470], outputs = [Tensor_13971, Tensor_13974, Tensor_13975], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_13971: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_13972: operator = ElemwiseBinaryForward, inputs = [Tensor_13875, Tensor_13971], outputs = [Tensor_13973], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_13973: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_13974: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_13975: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_13976: operator = ElemwiseBinaryBackward, inputs = [Tensor_13973, nullptr, nullptr, nullptr], outputs = [Tensor_13977, Tensor_13978], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_13977: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_13978: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_13979: operator = ElemwiseUnaryBackward, inputs = [Tensor_13977, nullptr, nullptr], outputs = [Tensor_13980], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 1]
Tensor_13980: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_13981: operator = TypecastBackward, inputs = [Tensor_13978], outputs = [Tensor_13982], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_13982: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_13983: operator = ElemwiseBinaryBackward, inputs = [Tensor_13982, Tensor_2456, Tensor_2458, nullptr], outputs = [Tensor_13984, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_MUL]
Tensor_13984: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_13985: operator = ElemwiseUnaryBackward, inputs = [Tensor_13984, nullptr, nullptr], outputs = [Tensor_13986], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_DIV scal1_float: 0.863636374]
Tensor_13986: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_13987: operator = ReshapeBackward, inputs = [Tensor_13986], outputs = [Tensor_13988], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 151296 stride: 768 stride: 1]
Tensor_13988: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_13989: operator = LinearBackward, inputs = [Tensor_13988, Tensor_2441, Tensor_2443], outputs = [Tensor_13990, Tensor_13991, Tensor_13992], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_13990: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_13991: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_13992: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_13993: operator = TypecastBackward, inputs = [Tensor_13990], outputs = [Tensor_13994], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_13994: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_13995: operator = LayernormBackward, inputs = [Tensor_13994, Tensor_2435, Tensor_2437, Tensor_436, Tensor_437, Tensor_2438, Tensor_2439], outputs = [Tensor_13996, Tensor_13997, Tensor_13998], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_13996: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_13997: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_13998: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_13999: operator = TypecastBackward, inputs = [Tensor_13996], outputs = [Tensor_14000], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_14000: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14001: operator = TypecastBackward, inputs = [Tensor_14000], outputs = [Tensor_14002], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_14002: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14003: operator = ElemwiseUnaryBackward, inputs = [Tensor_14002, Tensor_2429, Tensor_2431], outputs = [Tensor_14004], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_GELU]
Tensor_14004: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14005: operator = TypecastBackward, inputs = [Tensor_14004], outputs = [Tensor_14006], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_14006: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14007: operator = LinearBackward, inputs = [Tensor_14006, Tensor_2421, Tensor_2423], outputs = [Tensor_14008, Tensor_14009, Tensor_14010], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_14008: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14009: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14010: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14011: operator = TypecastBackward, inputs = [Tensor_14008], outputs = [Tensor_14012], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_14012: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14013: operator = ReshapeBackward, inputs = [Tensor_14012], outputs = [Tensor_14014], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 25216 shape: 768 stride: 768 stride: 1]
Tensor_14014: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14015: operator = LayernormBackward, inputs = [Tensor_14014, Tensor_2413, Tensor_2415, Tensor_444, Tensor_445, Tensor_2416, Tensor_2417], outputs = [Tensor_14016, Tensor_14019, Tensor_14020], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_14016: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14017: operator = ElemwiseBinaryForward, inputs = [Tensor_13980, Tensor_14016], outputs = [Tensor_14018], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_14018: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14019: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14020: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14021: operator = ElemwiseBinaryBackward, inputs = [Tensor_14018, nullptr, nullptr, nullptr], outputs = [Tensor_14022, Tensor_14023], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_14022: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14023: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14024: operator = ElemwiseUnaryBackward, inputs = [Tensor_14022, nullptr, nullptr], outputs = [Tensor_14025], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 1]
Tensor_14025: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14026: operator = TypecastBackward, inputs = [Tensor_14023], outputs = [Tensor_14027], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_14027: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14028: operator = ElemwiseBinaryBackward, inputs = [Tensor_14027, Tensor_2403, Tensor_2405, nullptr], outputs = [Tensor_14029, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_MUL]
Tensor_14029: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14030: operator = ElemwiseUnaryBackward, inputs = [Tensor_14029, nullptr, nullptr], outputs = [Tensor_14031], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_DIV scal1_float: 0.863636374]
Tensor_14031: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14032: operator = LinearBackward, inputs = [Tensor_14031, Tensor_2386, Tensor_2388], outputs = [Tensor_14033, Tensor_14034, Tensor_14035], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_14033: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14034: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14035: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14036: operator = TypecastBackward, inputs = [Tensor_14033], outputs = [Tensor_14037], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_14037: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14038: operator = LayernormBackward, inputs = [Tensor_14037, Tensor_2380, Tensor_2382, Tensor_424, Tensor_425, Tensor_2383, Tensor_2384], outputs = [Tensor_14039, Tensor_14040, Tensor_14041], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_14039: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14040: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14041: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14042: operator = TypecastBackward, inputs = [Tensor_14039], outputs = [Tensor_14043], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_14043: shape = [128, 197, 768], stride = [768, 98304, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14044: operator = ReshapeBackward, inputs = [Tensor_14043], outputs = [Tensor_14045], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 768 stride: 98304 stride: 1]
Tensor_14045: shape = [197, 128, 768], stride = [98304, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14046: operator = ReshapeBackward, inputs = [Tensor_14045], outputs = [Tensor_14047], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 197 shape: 128 shape: 768 stride: 98304 stride: 768 stride: 1]
Tensor_14047: shape = [197, 1536, 64], stride = [98304, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14048: operator = ContiguousBackward, inputs = [Tensor_14047], outputs = [Tensor_14049], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_14049: shape = [197, 1536, 64], stride = [64, 12608, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14050: operator = ReshapeBackward, inputs = [Tensor_14049], outputs = [Tensor_14051], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 197 shape: 1536 shape: 64 stride: 64 stride: 12608 stride: 1]
Tensor_14051: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14052: operator = BatchedMatmulBackward, inputs = [Tensor_14051, Tensor_2368, Tensor_2346], outputs = [Tensor_14053, Tensor_14054], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_14053: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14054: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14055: operator = TypecastBackward, inputs = [Tensor_14053], outputs = [Tensor_14056], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_14056: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14057: operator = ReshapeBackward, inputs = [Tensor_14054], outputs = [Tensor_14058], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_14058: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14059: operator = SoftmaxBackward, inputs = [Tensor_14056, Tensor_2366], outputs = [Tensor_14060], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dim: 2]
Tensor_14060: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14061: operator = ContiguousBackward, inputs = [Tensor_14058], outputs = [Tensor_14062], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_14062: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14063: operator = TypecastBackward, inputs = [Tensor_14060], outputs = [Tensor_14064], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_14064: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14065: operator = ReshapeBackward, inputs = [Tensor_14062], outputs = [Tensor_14066], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 64 stride: 151296 stride: 64 stride: 768 stride: 1]
Tensor_14066: shape = [128, 197, 12, 64], stride = [151296, 768, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14067: operator = ReshapeBackward, inputs = [Tensor_14064], outputs = [Tensor_14068], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 197 stride: 38809 stride: 197 stride: 1]
Tensor_14068: shape = [128, 12, 197, 197], stride = [465708, 38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14069: operator = ReshapeBackward, inputs = [Tensor_14066], outputs = [Tensor_14070], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 12 shape: 64 stride: 151296 stride: 768 stride: 64 stride: 1]
Tensor_14070: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14071: operator = MaskedFillBackward, inputs = [Tensor_14068, Tensor_2358], outputs = [Tensor_14072], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [value_float: -inf]
Tensor_14072: shape = [128, 12, 197, 197], stride = [465708, 38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14073: operator = LinearBackward, inputs = [Tensor_14070, Tensor_2314, Tensor_2316], outputs = [Tensor_14074, Tensor_14075, Tensor_14076], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_14074: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14075: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14076: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14077: operator = ReshapeBackward, inputs = [Tensor_14072], outputs = [Tensor_14078], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 197 stride: 465708 stride: 38809 stride: 197 stride: 1]
Tensor_14078: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14079: operator = TypecastBackward, inputs = [Tensor_14074], outputs = [Tensor_14080], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_14080: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14081: operator = BatchedMatmulBackward, inputs = [Tensor_14078, Tensor_2338, Tensor_2348], outputs = [Tensor_14082, Tensor_14083], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_14082: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14083: shape = [1536, 64, 197], stride = [12608, 1, 64], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14084: operator = ReshapeBackward, inputs = [Tensor_14082], outputs = [Tensor_14085], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_14085: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14086: operator = ReshapeBackward, inputs = [Tensor_14083], outputs = [Tensor_14087], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 64 shape: 197 stride: 12608 stride: 1 stride: 64]
Tensor_14087: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14088: operator = ContiguousBackward, inputs = [Tensor_14085], outputs = [Tensor_14089], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_14089: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14090: operator = ReshapeBackward, inputs = [Tensor_14087], outputs = [Tensor_14091], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_14091: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14092: operator = ReshapeBackward, inputs = [Tensor_14089], outputs = [Tensor_14093], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 64 stride: 151296 stride: 64 stride: 768 stride: 1]
Tensor_14093: shape = [128, 197, 12, 64], stride = [151296, 768, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14094: operator = ContiguousBackward, inputs = [Tensor_14091], outputs = [Tensor_14095], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_14095: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14096: operator = ReshapeBackward, inputs = [Tensor_14093], outputs = [Tensor_14097], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 12 shape: 64 stride: 151296 stride: 768 stride: 64 stride: 1]
Tensor_14097: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14098: operator = ReshapeBackward, inputs = [Tensor_14095], outputs = [Tensor_14099], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 64 stride: 151296 stride: 64 stride: 768 stride: 1]
Tensor_14099: shape = [128, 197, 12, 64], stride = [151296, 768, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14100: operator = ElemwiseUnaryBackward, inputs = [Tensor_14097, nullptr, nullptr], outputs = [Tensor_14101], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 0.125]
Tensor_14101: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14102: operator = ReshapeBackward, inputs = [Tensor_14099], outputs = [Tensor_14103], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 12 shape: 64 stride: 151296 stride: 768 stride: 64 stride: 1]
Tensor_14103: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14104: operator = LinearBackward, inputs = [Tensor_14101, Tensor_2298, Tensor_2300], outputs = [Tensor_14105, Tensor_14106, Tensor_14107], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_14105: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14106: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14107: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14108: operator = LinearBackward, inputs = [Tensor_14103, Tensor_2306, Tensor_2308], outputs = [Tensor_14109, Tensor_14110, Tensor_14111], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_14109: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14110: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14111: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14112: operator = TypecastBackward, inputs = [Tensor_14105], outputs = [Tensor_14113], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_14113: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14114: operator = ElemwiseBinaryForward, inputs = [Tensor_14080, Tensor_14113], outputs = [Tensor_14115], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_14115: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14116: operator = TypecastBackward, inputs = [Tensor_14109], outputs = [Tensor_14117], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_14117: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14118: operator = ElemwiseBinaryForward, inputs = [Tensor_14115, Tensor_14117], outputs = [Tensor_14119], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_14119: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14120: operator = LayernormBackward, inputs = [Tensor_14119, Tensor_2292, Tensor_2294, Tensor_428, Tensor_429, Tensor_2295, Tensor_2296], outputs = [Tensor_14121, Tensor_14124, Tensor_14125], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_14121: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14122: operator = ElemwiseBinaryForward, inputs = [Tensor_14025, Tensor_14121], outputs = [Tensor_14123], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_14123: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14124: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14125: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14126: operator = ElemwiseBinaryBackward, inputs = [Tensor_14123, nullptr, nullptr, nullptr], outputs = [Tensor_14127, Tensor_14128], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_14127: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14128: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14129: operator = ElemwiseUnaryBackward, inputs = [Tensor_14127, nullptr, nullptr], outputs = [Tensor_14130], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 1]
Tensor_14130: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14131: operator = TypecastBackward, inputs = [Tensor_14128], outputs = [Tensor_14132], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_14132: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14133: operator = ElemwiseBinaryBackward, inputs = [Tensor_14132, Tensor_2282, Tensor_2284, nullptr], outputs = [Tensor_14134, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_MUL]
Tensor_14134: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14135: operator = ElemwiseUnaryBackward, inputs = [Tensor_14134, nullptr, nullptr], outputs = [Tensor_14136], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_DIV scal1_float: 0.877272725]
Tensor_14136: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14137: operator = ReshapeBackward, inputs = [Tensor_14136], outputs = [Tensor_14138], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 151296 stride: 768 stride: 1]
Tensor_14138: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14139: operator = LinearBackward, inputs = [Tensor_14138, Tensor_2267, Tensor_2269], outputs = [Tensor_14140, Tensor_14141, Tensor_14142], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_14140: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14141: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14142: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14143: operator = TypecastBackward, inputs = [Tensor_14140], outputs = [Tensor_14144], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_14144: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14145: operator = LayernormBackward, inputs = [Tensor_14144, Tensor_2261, Tensor_2263, Tensor_396, Tensor_397, Tensor_2264, Tensor_2265], outputs = [Tensor_14146, Tensor_14147, Tensor_14148], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_14146: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14147: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14148: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14149: operator = TypecastBackward, inputs = [Tensor_14146], outputs = [Tensor_14150], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_14150: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14151: operator = TypecastBackward, inputs = [Tensor_14150], outputs = [Tensor_14152], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_14152: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14153: operator = ElemwiseUnaryBackward, inputs = [Tensor_14152, Tensor_2255, Tensor_2257], outputs = [Tensor_14154], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_GELU]
Tensor_14154: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14155: operator = TypecastBackward, inputs = [Tensor_14154], outputs = [Tensor_14156], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_14156: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14157: operator = LinearBackward, inputs = [Tensor_14156, Tensor_2247, Tensor_2249], outputs = [Tensor_14158, Tensor_14159, Tensor_14160], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_14158: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14159: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14160: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14161: operator = TypecastBackward, inputs = [Tensor_14158], outputs = [Tensor_14162], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_14162: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14163: operator = ReshapeBackward, inputs = [Tensor_14162], outputs = [Tensor_14164], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 25216 shape: 768 stride: 768 stride: 1]
Tensor_14164: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14165: operator = LayernormBackward, inputs = [Tensor_14164, Tensor_2239, Tensor_2241, Tensor_404, Tensor_405, Tensor_2242, Tensor_2243], outputs = [Tensor_14166, Tensor_14169, Tensor_14170], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_14166: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14167: operator = ElemwiseBinaryForward, inputs = [Tensor_14130, Tensor_14166], outputs = [Tensor_14168], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_14168: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14169: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14170: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14171: operator = ElemwiseBinaryBackward, inputs = [Tensor_14168, nullptr, nullptr, nullptr], outputs = [Tensor_14172, Tensor_14173], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_14172: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14173: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14174: operator = ElemwiseUnaryBackward, inputs = [Tensor_14172, nullptr, nullptr], outputs = [Tensor_14175], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 1]
Tensor_14175: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14176: operator = TypecastBackward, inputs = [Tensor_14173], outputs = [Tensor_14177], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_14177: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14178: operator = ElemwiseBinaryBackward, inputs = [Tensor_14177, Tensor_2229, Tensor_2231, nullptr], outputs = [Tensor_14179, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_MUL]
Tensor_14179: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14180: operator = ElemwiseUnaryBackward, inputs = [Tensor_14179, nullptr, nullptr], outputs = [Tensor_14181], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_DIV scal1_float: 0.877272725]
Tensor_14181: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14182: operator = LinearBackward, inputs = [Tensor_14181, Tensor_2212, Tensor_2214], outputs = [Tensor_14183, Tensor_14184, Tensor_14185], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_14183: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14184: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14185: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14186: operator = TypecastBackward, inputs = [Tensor_14183], outputs = [Tensor_14187], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_14187: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14188: operator = LayernormBackward, inputs = [Tensor_14187, Tensor_2206, Tensor_2208, Tensor_384, Tensor_385, Tensor_2209, Tensor_2210], outputs = [Tensor_14189, Tensor_14190, Tensor_14191], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_14189: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14190: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14191: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14192: operator = TypecastBackward, inputs = [Tensor_14189], outputs = [Tensor_14193], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_14193: shape = [128, 197, 768], stride = [768, 98304, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14194: operator = ReshapeBackward, inputs = [Tensor_14193], outputs = [Tensor_14195], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 768 stride: 98304 stride: 1]
Tensor_14195: shape = [197, 128, 768], stride = [98304, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14196: operator = ReshapeBackward, inputs = [Tensor_14195], outputs = [Tensor_14197], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 197 shape: 128 shape: 768 stride: 98304 stride: 768 stride: 1]
Tensor_14197: shape = [197, 1536, 64], stride = [98304, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14198: operator = ContiguousBackward, inputs = [Tensor_14197], outputs = [Tensor_14199], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_14199: shape = [197, 1536, 64], stride = [64, 12608, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14200: operator = ReshapeBackward, inputs = [Tensor_14199], outputs = [Tensor_14201], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 197 shape: 1536 shape: 64 stride: 64 stride: 12608 stride: 1]
Tensor_14201: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14202: operator = BatchedMatmulBackward, inputs = [Tensor_14201, Tensor_2194, Tensor_2172], outputs = [Tensor_14203, Tensor_14204], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_14203: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14204: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14205: operator = TypecastBackward, inputs = [Tensor_14203], outputs = [Tensor_14206], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_14206: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14207: operator = ReshapeBackward, inputs = [Tensor_14204], outputs = [Tensor_14208], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_14208: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14209: operator = SoftmaxBackward, inputs = [Tensor_14206, Tensor_2192], outputs = [Tensor_14210], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dim: 2]
Tensor_14210: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14211: operator = ContiguousBackward, inputs = [Tensor_14208], outputs = [Tensor_14212], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_14212: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14213: operator = TypecastBackward, inputs = [Tensor_14210], outputs = [Tensor_14214], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_14214: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14215: operator = ReshapeBackward, inputs = [Tensor_14212], outputs = [Tensor_14216], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 64 stride: 151296 stride: 64 stride: 768 stride: 1]
Tensor_14216: shape = [128, 197, 12, 64], stride = [151296, 768, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14217: operator = ReshapeBackward, inputs = [Tensor_14214], outputs = [Tensor_14218], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 197 stride: 38809 stride: 197 stride: 1]
Tensor_14218: shape = [128, 12, 197, 197], stride = [465708, 38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14219: operator = ReshapeBackward, inputs = [Tensor_14216], outputs = [Tensor_14220], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 12 shape: 64 stride: 151296 stride: 768 stride: 64 stride: 1]
Tensor_14220: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14221: operator = MaskedFillBackward, inputs = [Tensor_14218, Tensor_2184], outputs = [Tensor_14222], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [value_float: -inf]
Tensor_14222: shape = [128, 12, 197, 197], stride = [465708, 38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14223: operator = LinearBackward, inputs = [Tensor_14220, Tensor_2140, Tensor_2142], outputs = [Tensor_14224, Tensor_14225, Tensor_14226], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_14224: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14225: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14226: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14227: operator = ReshapeBackward, inputs = [Tensor_14222], outputs = [Tensor_14228], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 197 stride: 465708 stride: 38809 stride: 197 stride: 1]
Tensor_14228: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14229: operator = TypecastBackward, inputs = [Tensor_14224], outputs = [Tensor_14230], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_14230: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14231: operator = BatchedMatmulBackward, inputs = [Tensor_14228, Tensor_2164, Tensor_2174], outputs = [Tensor_14232, Tensor_14233], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_14232: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14233: shape = [1536, 64, 197], stride = [12608, 1, 64], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14234: operator = ReshapeBackward, inputs = [Tensor_14232], outputs = [Tensor_14235], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_14235: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14236: operator = ReshapeBackward, inputs = [Tensor_14233], outputs = [Tensor_14237], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 64 shape: 197 stride: 12608 stride: 1 stride: 64]
Tensor_14237: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14238: operator = ContiguousBackward, inputs = [Tensor_14235], outputs = [Tensor_14239], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_14239: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14240: operator = ReshapeBackward, inputs = [Tensor_14237], outputs = [Tensor_14241], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_14241: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14242: operator = ReshapeBackward, inputs = [Tensor_14239], outputs = [Tensor_14243], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 64 stride: 151296 stride: 64 stride: 768 stride: 1]
Tensor_14243: shape = [128, 197, 12, 64], stride = [151296, 768, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14244: operator = ContiguousBackward, inputs = [Tensor_14241], outputs = [Tensor_14245], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_14245: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14246: operator = ReshapeBackward, inputs = [Tensor_14243], outputs = [Tensor_14247], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 12 shape: 64 stride: 151296 stride: 768 stride: 64 stride: 1]
Tensor_14247: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14248: operator = ReshapeBackward, inputs = [Tensor_14245], outputs = [Tensor_14249], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 64 stride: 151296 stride: 64 stride: 768 stride: 1]
Tensor_14249: shape = [128, 197, 12, 64], stride = [151296, 768, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14250: operator = ElemwiseUnaryBackward, inputs = [Tensor_14247, nullptr, nullptr], outputs = [Tensor_14251], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 0.125]
Tensor_14251: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14252: operator = ReshapeBackward, inputs = [Tensor_14249], outputs = [Tensor_14253], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 12 shape: 64 stride: 151296 stride: 768 stride: 64 stride: 1]
Tensor_14253: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14254: operator = LinearBackward, inputs = [Tensor_14251, Tensor_2124, Tensor_2126], outputs = [Tensor_14255, Tensor_14256, Tensor_14257], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_14255: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14256: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14257: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14258: operator = LinearBackward, inputs = [Tensor_14253, Tensor_2132, Tensor_2134], outputs = [Tensor_14259, Tensor_14260, Tensor_14261], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_14259: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14260: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14261: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14262: operator = TypecastBackward, inputs = [Tensor_14255], outputs = [Tensor_14263], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_14263: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14264: operator = ElemwiseBinaryForward, inputs = [Tensor_14230, Tensor_14263], outputs = [Tensor_14265], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_14265: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14266: operator = TypecastBackward, inputs = [Tensor_14259], outputs = [Tensor_14267], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_14267: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14268: operator = ElemwiseBinaryForward, inputs = [Tensor_14265, Tensor_14267], outputs = [Tensor_14269], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_14269: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14270: operator = LayernormBackward, inputs = [Tensor_14269, Tensor_2118, Tensor_2120, Tensor_388, Tensor_389, Tensor_2121, Tensor_2122], outputs = [Tensor_14271, Tensor_14274, Tensor_14275], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_14271: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14272: operator = ElemwiseBinaryForward, inputs = [Tensor_14175, Tensor_14271], outputs = [Tensor_14273], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_14273: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14274: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14275: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14276: operator = ElemwiseBinaryBackward, inputs = [Tensor_14273, nullptr, nullptr, nullptr], outputs = [Tensor_14277, Tensor_14278], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_14277: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14278: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14279: operator = ElemwiseUnaryBackward, inputs = [Tensor_14277, nullptr, nullptr], outputs = [Tensor_14280], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 1]
Tensor_14280: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14281: operator = TypecastBackward, inputs = [Tensor_14278], outputs = [Tensor_14282], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_14282: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14283: operator = ElemwiseBinaryBackward, inputs = [Tensor_14282, Tensor_2108, Tensor_2110, nullptr], outputs = [Tensor_14284, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_MUL]
Tensor_14284: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14285: operator = ElemwiseUnaryBackward, inputs = [Tensor_14284, nullptr, nullptr], outputs = [Tensor_14286], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_DIV scal1_float: 0.890909076]
Tensor_14286: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14287: operator = ReshapeBackward, inputs = [Tensor_14286], outputs = [Tensor_14288], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 151296 stride: 768 stride: 1]
Tensor_14288: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14289: operator = LinearBackward, inputs = [Tensor_14288, Tensor_2093, Tensor_2095], outputs = [Tensor_14290, Tensor_14291, Tensor_14292], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_14290: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14291: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14292: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14293: operator = TypecastBackward, inputs = [Tensor_14290], outputs = [Tensor_14294], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_14294: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14295: operator = LayernormBackward, inputs = [Tensor_14294, Tensor_2087, Tensor_2089, Tensor_356, Tensor_357, Tensor_2090, Tensor_2091], outputs = [Tensor_14296, Tensor_14297, Tensor_14298], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_14296: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14297: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14298: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14299: operator = TypecastBackward, inputs = [Tensor_14296], outputs = [Tensor_14300], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_14300: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14301: operator = TypecastBackward, inputs = [Tensor_14300], outputs = [Tensor_14302], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_14302: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14303: operator = ElemwiseUnaryBackward, inputs = [Tensor_14302, Tensor_2081, Tensor_2083], outputs = [Tensor_14304], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_GELU]
Tensor_14304: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14305: operator = TypecastBackward, inputs = [Tensor_14304], outputs = [Tensor_14306], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_14306: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14307: operator = LinearBackward, inputs = [Tensor_14306, Tensor_2073, Tensor_2075], outputs = [Tensor_14308, Tensor_14309, Tensor_14310], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_14308: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14309: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14310: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14311: operator = TypecastBackward, inputs = [Tensor_14308], outputs = [Tensor_14312], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_14312: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14313: operator = ReshapeBackward, inputs = [Tensor_14312], outputs = [Tensor_14314], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 25216 shape: 768 stride: 768 stride: 1]
Tensor_14314: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14315: operator = LayernormBackward, inputs = [Tensor_14314, Tensor_2065, Tensor_2067, Tensor_364, Tensor_365, Tensor_2068, Tensor_2069], outputs = [Tensor_14316, Tensor_14319, Tensor_14320], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_14316: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14317: operator = ElemwiseBinaryForward, inputs = [Tensor_14280, Tensor_14316], outputs = [Tensor_14318], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_14318: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14319: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14320: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14321: operator = ElemwiseBinaryBackward, inputs = [Tensor_14318, nullptr, nullptr, nullptr], outputs = [Tensor_14322, Tensor_14323], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_14322: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14323: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14324: operator = ElemwiseUnaryBackward, inputs = [Tensor_14322, nullptr, nullptr], outputs = [Tensor_14325], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 1]
Tensor_14325: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14326: operator = TypecastBackward, inputs = [Tensor_14323], outputs = [Tensor_14327], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_14327: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14328: operator = ElemwiseBinaryBackward, inputs = [Tensor_14327, Tensor_2055, Tensor_2057, nullptr], outputs = [Tensor_14329, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_MUL]
Tensor_14329: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14330: operator = ElemwiseUnaryBackward, inputs = [Tensor_14329, nullptr, nullptr], outputs = [Tensor_14331], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_DIV scal1_float: 0.890909076]
Tensor_14331: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14332: operator = LinearBackward, inputs = [Tensor_14331, Tensor_2038, Tensor_2040], outputs = [Tensor_14333, Tensor_14334, Tensor_14335], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_14333: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14334: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14335: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14336: operator = TypecastBackward, inputs = [Tensor_14333], outputs = [Tensor_14337], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_14337: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14338: operator = LayernormBackward, inputs = [Tensor_14337, Tensor_2032, Tensor_2034, Tensor_344, Tensor_345, Tensor_2035, Tensor_2036], outputs = [Tensor_14339, Tensor_14340, Tensor_14341], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_14339: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14340: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14341: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14342: operator = TypecastBackward, inputs = [Tensor_14339], outputs = [Tensor_14343], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_14343: shape = [128, 197, 768], stride = [768, 98304, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14344: operator = ReshapeBackward, inputs = [Tensor_14343], outputs = [Tensor_14345], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 768 stride: 98304 stride: 1]
Tensor_14345: shape = [197, 128, 768], stride = [98304, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14346: operator = ReshapeBackward, inputs = [Tensor_14345], outputs = [Tensor_14347], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 197 shape: 128 shape: 768 stride: 98304 stride: 768 stride: 1]
Tensor_14347: shape = [197, 1536, 64], stride = [98304, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14348: operator = ContiguousBackward, inputs = [Tensor_14347], outputs = [Tensor_14349], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_14349: shape = [197, 1536, 64], stride = [64, 12608, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14350: operator = ReshapeBackward, inputs = [Tensor_14349], outputs = [Tensor_14351], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 197 shape: 1536 shape: 64 stride: 64 stride: 12608 stride: 1]
Tensor_14351: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14352: operator = BatchedMatmulBackward, inputs = [Tensor_14351, Tensor_2020, Tensor_1998], outputs = [Tensor_14353, Tensor_14354], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_14353: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14354: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14355: operator = TypecastBackward, inputs = [Tensor_14353], outputs = [Tensor_14356], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_14356: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14357: operator = ReshapeBackward, inputs = [Tensor_14354], outputs = [Tensor_14358], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_14358: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14359: operator = SoftmaxBackward, inputs = [Tensor_14356, Tensor_2018], outputs = [Tensor_14360], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dim: 2]
Tensor_14360: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14361: operator = ContiguousBackward, inputs = [Tensor_14358], outputs = [Tensor_14362], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_14362: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14363: operator = TypecastBackward, inputs = [Tensor_14360], outputs = [Tensor_14364], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_14364: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14365: operator = ReshapeBackward, inputs = [Tensor_14362], outputs = [Tensor_14366], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 64 stride: 151296 stride: 64 stride: 768 stride: 1]
Tensor_14366: shape = [128, 197, 12, 64], stride = [151296, 768, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14367: operator = ReshapeBackward, inputs = [Tensor_14364], outputs = [Tensor_14368], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 197 stride: 38809 stride: 197 stride: 1]
Tensor_14368: shape = [128, 12, 197, 197], stride = [465708, 38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14369: operator = ReshapeBackward, inputs = [Tensor_14366], outputs = [Tensor_14370], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 12 shape: 64 stride: 151296 stride: 768 stride: 64 stride: 1]
Tensor_14370: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14371: operator = MaskedFillBackward, inputs = [Tensor_14368, Tensor_2010], outputs = [Tensor_14372], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [value_float: -inf]
Tensor_14372: shape = [128, 12, 197, 197], stride = [465708, 38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14373: operator = LinearBackward, inputs = [Tensor_14370, Tensor_1966, Tensor_1968], outputs = [Tensor_14374, Tensor_14375, Tensor_14376], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_14374: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14375: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14376: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14377: operator = ReshapeBackward, inputs = [Tensor_14372], outputs = [Tensor_14378], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 197 stride: 465708 stride: 38809 stride: 197 stride: 1]
Tensor_14378: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14379: operator = TypecastBackward, inputs = [Tensor_14374], outputs = [Tensor_14380], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_14380: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14381: operator = BatchedMatmulBackward, inputs = [Tensor_14378, Tensor_1990, Tensor_2000], outputs = [Tensor_14382, Tensor_14383], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_14382: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14383: shape = [1536, 64, 197], stride = [12608, 1, 64], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14384: operator = ReshapeBackward, inputs = [Tensor_14382], outputs = [Tensor_14385], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_14385: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14386: operator = ReshapeBackward, inputs = [Tensor_14383], outputs = [Tensor_14387], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 64 shape: 197 stride: 12608 stride: 1 stride: 64]
Tensor_14387: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14388: operator = ContiguousBackward, inputs = [Tensor_14385], outputs = [Tensor_14389], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_14389: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14390: operator = ReshapeBackward, inputs = [Tensor_14387], outputs = [Tensor_14391], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_14391: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14392: operator = ReshapeBackward, inputs = [Tensor_14389], outputs = [Tensor_14393], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 64 stride: 151296 stride: 64 stride: 768 stride: 1]
Tensor_14393: shape = [128, 197, 12, 64], stride = [151296, 768, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14394: operator = ContiguousBackward, inputs = [Tensor_14391], outputs = [Tensor_14395], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_14395: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14396: operator = ReshapeBackward, inputs = [Tensor_14393], outputs = [Tensor_14397], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 12 shape: 64 stride: 151296 stride: 768 stride: 64 stride: 1]
Tensor_14397: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14398: operator = ReshapeBackward, inputs = [Tensor_14395], outputs = [Tensor_14399], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 64 stride: 151296 stride: 64 stride: 768 stride: 1]
Tensor_14399: shape = [128, 197, 12, 64], stride = [151296, 768, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14400: operator = ElemwiseUnaryBackward, inputs = [Tensor_14397, nullptr, nullptr], outputs = [Tensor_14401], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 0.125]
Tensor_14401: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14402: operator = ReshapeBackward, inputs = [Tensor_14399], outputs = [Tensor_14403], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 12 shape: 64 stride: 151296 stride: 768 stride: 64 stride: 1]
Tensor_14403: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14404: operator = LinearBackward, inputs = [Tensor_14401, Tensor_1950, Tensor_1952], outputs = [Tensor_14405, Tensor_14406, Tensor_14407], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_14405: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14406: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14407: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14408: operator = LinearBackward, inputs = [Tensor_14403, Tensor_1958, Tensor_1960], outputs = [Tensor_14409, Tensor_14410, Tensor_14411], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_14409: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14410: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14411: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14412: operator = TypecastBackward, inputs = [Tensor_14405], outputs = [Tensor_14413], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_14413: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14414: operator = ElemwiseBinaryForward, inputs = [Tensor_14380, Tensor_14413], outputs = [Tensor_14415], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_14415: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14416: operator = TypecastBackward, inputs = [Tensor_14409], outputs = [Tensor_14417], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_14417: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14418: operator = ElemwiseBinaryForward, inputs = [Tensor_14415, Tensor_14417], outputs = [Tensor_14419], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_14419: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14420: operator = LayernormBackward, inputs = [Tensor_14419, Tensor_1944, Tensor_1946, Tensor_348, Tensor_349, Tensor_1947, Tensor_1948], outputs = [Tensor_14421, Tensor_14424, Tensor_14425], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_14421: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14422: operator = ElemwiseBinaryForward, inputs = [Tensor_14325, Tensor_14421], outputs = [Tensor_14423], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_14423: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14424: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14425: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14426: operator = ElemwiseBinaryBackward, inputs = [Tensor_14423, nullptr, nullptr, nullptr], outputs = [Tensor_14427, Tensor_14428], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_14427: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14428: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14429: operator = ElemwiseUnaryBackward, inputs = [Tensor_14427, nullptr, nullptr], outputs = [Tensor_14430], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 1]
Tensor_14430: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14431: operator = TypecastBackward, inputs = [Tensor_14428], outputs = [Tensor_14432], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_14432: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14433: operator = ElemwiseBinaryBackward, inputs = [Tensor_14432, Tensor_1934, Tensor_1936, nullptr], outputs = [Tensor_14434, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_MUL]
Tensor_14434: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14435: operator = ElemwiseUnaryBackward, inputs = [Tensor_14434, nullptr, nullptr], outputs = [Tensor_14436], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_DIV scal1_float: 0.904545426]
Tensor_14436: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14437: operator = ReshapeBackward, inputs = [Tensor_14436], outputs = [Tensor_14438], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 151296 stride: 768 stride: 1]
Tensor_14438: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14439: operator = LinearBackward, inputs = [Tensor_14438, Tensor_1919, Tensor_1921], outputs = [Tensor_14440, Tensor_14441, Tensor_14442], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_14440: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14441: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14442: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14443: operator = TypecastBackward, inputs = [Tensor_14440], outputs = [Tensor_14444], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_14444: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14445: operator = LayernormBackward, inputs = [Tensor_14444, Tensor_1913, Tensor_1915, Tensor_316, Tensor_317, Tensor_1916, Tensor_1917], outputs = [Tensor_14446, Tensor_14447, Tensor_14448], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_14446: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14447: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14448: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14449: operator = TypecastBackward, inputs = [Tensor_14446], outputs = [Tensor_14450], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_14450: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14451: operator = TypecastBackward, inputs = [Tensor_14450], outputs = [Tensor_14452], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_14452: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14453: operator = ElemwiseUnaryBackward, inputs = [Tensor_14452, Tensor_1907, Tensor_1909], outputs = [Tensor_14454], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_GELU]
Tensor_14454: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14455: operator = TypecastBackward, inputs = [Tensor_14454], outputs = [Tensor_14456], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_14456: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14457: operator = LinearBackward, inputs = [Tensor_14456, Tensor_1899, Tensor_1901], outputs = [Tensor_14458, Tensor_14459, Tensor_14460], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_14458: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14459: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14460: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14461: operator = TypecastBackward, inputs = [Tensor_14458], outputs = [Tensor_14462], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_14462: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14463: operator = ReshapeBackward, inputs = [Tensor_14462], outputs = [Tensor_14464], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 25216 shape: 768 stride: 768 stride: 1]
Tensor_14464: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14465: operator = LayernormBackward, inputs = [Tensor_14464, Tensor_1891, Tensor_1893, Tensor_324, Tensor_325, Tensor_1894, Tensor_1895], outputs = [Tensor_14466, Tensor_14469, Tensor_14470], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_14466: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14467: operator = ElemwiseBinaryForward, inputs = [Tensor_14430, Tensor_14466], outputs = [Tensor_14468], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_14468: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14469: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14470: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14471: operator = ElemwiseBinaryBackward, inputs = [Tensor_14468, nullptr, nullptr, nullptr], outputs = [Tensor_14472, Tensor_14473], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_14472: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14473: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14474: operator = ElemwiseUnaryBackward, inputs = [Tensor_14472, nullptr, nullptr], outputs = [Tensor_14475], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 1]
Tensor_14475: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14476: operator = TypecastBackward, inputs = [Tensor_14473], outputs = [Tensor_14477], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_14477: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14478: operator = ElemwiseBinaryBackward, inputs = [Tensor_14477, Tensor_1881, Tensor_1883, nullptr], outputs = [Tensor_14479, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_MUL]
Tensor_14479: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14480: operator = ElemwiseUnaryBackward, inputs = [Tensor_14479, nullptr, nullptr], outputs = [Tensor_14481], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_DIV scal1_float: 0.904545426]
Tensor_14481: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14482: operator = LinearBackward, inputs = [Tensor_14481, Tensor_1864, Tensor_1866], outputs = [Tensor_14483, Tensor_14484, Tensor_14485], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_14483: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14484: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14485: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14486: operator = TypecastBackward, inputs = [Tensor_14483], outputs = [Tensor_14487], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_14487: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14488: operator = LayernormBackward, inputs = [Tensor_14487, Tensor_1858, Tensor_1860, Tensor_304, Tensor_305, Tensor_1861, Tensor_1862], outputs = [Tensor_14489, Tensor_14490, Tensor_14491], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_14489: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14490: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14491: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14492: operator = TypecastBackward, inputs = [Tensor_14489], outputs = [Tensor_14493], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_14493: shape = [128, 197, 768], stride = [768, 98304, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14494: operator = ReshapeBackward, inputs = [Tensor_14493], outputs = [Tensor_14495], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 768 stride: 98304 stride: 1]
Tensor_14495: shape = [197, 128, 768], stride = [98304, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14496: operator = ReshapeBackward, inputs = [Tensor_14495], outputs = [Tensor_14497], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 197 shape: 128 shape: 768 stride: 98304 stride: 768 stride: 1]
Tensor_14497: shape = [197, 1536, 64], stride = [98304, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14498: operator = ContiguousBackward, inputs = [Tensor_14497], outputs = [Tensor_14499], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_14499: shape = [197, 1536, 64], stride = [64, 12608, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14500: operator = ReshapeBackward, inputs = [Tensor_14499], outputs = [Tensor_14501], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 197 shape: 1536 shape: 64 stride: 64 stride: 12608 stride: 1]
Tensor_14501: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14502: operator = BatchedMatmulBackward, inputs = [Tensor_14501, Tensor_1846, Tensor_1824], outputs = [Tensor_14503, Tensor_14504], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_14503: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14504: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14505: operator = TypecastBackward, inputs = [Tensor_14503], outputs = [Tensor_14506], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_14506: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14507: operator = ReshapeBackward, inputs = [Tensor_14504], outputs = [Tensor_14508], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_14508: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14509: operator = SoftmaxBackward, inputs = [Tensor_14506, Tensor_1844], outputs = [Tensor_14510], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dim: 2]
Tensor_14510: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14511: operator = ContiguousBackward, inputs = [Tensor_14508], outputs = [Tensor_14512], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_14512: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14513: operator = TypecastBackward, inputs = [Tensor_14510], outputs = [Tensor_14514], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_14514: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14515: operator = ReshapeBackward, inputs = [Tensor_14512], outputs = [Tensor_14516], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 64 stride: 151296 stride: 64 stride: 768 stride: 1]
Tensor_14516: shape = [128, 197, 12, 64], stride = [151296, 768, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14517: operator = ReshapeBackward, inputs = [Tensor_14514], outputs = [Tensor_14518], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 197 stride: 38809 stride: 197 stride: 1]
Tensor_14518: shape = [128, 12, 197, 197], stride = [465708, 38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14519: operator = ReshapeBackward, inputs = [Tensor_14516], outputs = [Tensor_14520], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 12 shape: 64 stride: 151296 stride: 768 stride: 64 stride: 1]
Tensor_14520: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14521: operator = MaskedFillBackward, inputs = [Tensor_14518, Tensor_1836], outputs = [Tensor_14522], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [value_float: -inf]
Tensor_14522: shape = [128, 12, 197, 197], stride = [465708, 38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14523: operator = LinearBackward, inputs = [Tensor_14520, Tensor_1792, Tensor_1794], outputs = [Tensor_14524, Tensor_14525, Tensor_14526], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_14524: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14525: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14526: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14527: operator = ReshapeBackward, inputs = [Tensor_14522], outputs = [Tensor_14528], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 197 stride: 465708 stride: 38809 stride: 197 stride: 1]
Tensor_14528: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14529: operator = TypecastBackward, inputs = [Tensor_14524], outputs = [Tensor_14530], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_14530: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14531: operator = BatchedMatmulBackward, inputs = [Tensor_14528, Tensor_1816, Tensor_1826], outputs = [Tensor_14532, Tensor_14533], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_14532: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14533: shape = [1536, 64, 197], stride = [12608, 1, 64], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14534: operator = ReshapeBackward, inputs = [Tensor_14532], outputs = [Tensor_14535], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_14535: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14536: operator = ReshapeBackward, inputs = [Tensor_14533], outputs = [Tensor_14537], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 64 shape: 197 stride: 12608 stride: 1 stride: 64]
Tensor_14537: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14538: operator = ContiguousBackward, inputs = [Tensor_14535], outputs = [Tensor_14539], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_14539: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14540: operator = ReshapeBackward, inputs = [Tensor_14537], outputs = [Tensor_14541], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_14541: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14542: operator = ReshapeBackward, inputs = [Tensor_14539], outputs = [Tensor_14543], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 64 stride: 151296 stride: 64 stride: 768 stride: 1]
Tensor_14543: shape = [128, 197, 12, 64], stride = [151296, 768, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14544: operator = ContiguousBackward, inputs = [Tensor_14541], outputs = [Tensor_14545], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_14545: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14546: operator = ReshapeBackward, inputs = [Tensor_14543], outputs = [Tensor_14547], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 12 shape: 64 stride: 151296 stride: 768 stride: 64 stride: 1]
Tensor_14547: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14548: operator = ReshapeBackward, inputs = [Tensor_14545], outputs = [Tensor_14549], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 64 stride: 151296 stride: 64 stride: 768 stride: 1]
Tensor_14549: shape = [128, 197, 12, 64], stride = [151296, 768, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14550: operator = ElemwiseUnaryBackward, inputs = [Tensor_14547, nullptr, nullptr], outputs = [Tensor_14551], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 0.125]
Tensor_14551: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14552: operator = ReshapeBackward, inputs = [Tensor_14549], outputs = [Tensor_14553], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 12 shape: 64 stride: 151296 stride: 768 stride: 64 stride: 1]
Tensor_14553: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14554: operator = LinearBackward, inputs = [Tensor_14551, Tensor_1776, Tensor_1778], outputs = [Tensor_14555, Tensor_14556, Tensor_14557], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_14555: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14556: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14557: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14558: operator = LinearBackward, inputs = [Tensor_14553, Tensor_1784, Tensor_1786], outputs = [Tensor_14559, Tensor_14560, Tensor_14561], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_14559: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14560: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14561: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14562: operator = TypecastBackward, inputs = [Tensor_14555], outputs = [Tensor_14563], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_14563: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14564: operator = ElemwiseBinaryForward, inputs = [Tensor_14530, Tensor_14563], outputs = [Tensor_14565], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_14565: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14566: operator = TypecastBackward, inputs = [Tensor_14559], outputs = [Tensor_14567], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_14567: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14568: operator = ElemwiseBinaryForward, inputs = [Tensor_14565, Tensor_14567], outputs = [Tensor_14569], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_14569: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14570: operator = LayernormBackward, inputs = [Tensor_14569, Tensor_1770, Tensor_1772, Tensor_308, Tensor_309, Tensor_1773, Tensor_1774], outputs = [Tensor_14571, Tensor_14574, Tensor_14575], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_14571: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14572: operator = ElemwiseBinaryForward, inputs = [Tensor_14475, Tensor_14571], outputs = [Tensor_14573], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_14573: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14574: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14575: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14576: operator = ElemwiseBinaryBackward, inputs = [Tensor_14573, nullptr, nullptr, nullptr], outputs = [Tensor_14577, Tensor_14578], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_14577: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14578: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14579: operator = ElemwiseUnaryBackward, inputs = [Tensor_14577, nullptr, nullptr], outputs = [Tensor_14580], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 1]
Tensor_14580: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14581: operator = TypecastBackward, inputs = [Tensor_14578], outputs = [Tensor_14582], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_14582: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14583: operator = ElemwiseBinaryBackward, inputs = [Tensor_14582, Tensor_1760, Tensor_1762, nullptr], outputs = [Tensor_14584, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_MUL]
Tensor_14584: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14585: operator = ElemwiseUnaryBackward, inputs = [Tensor_14584, nullptr, nullptr], outputs = [Tensor_14586], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_DIV scal1_float: 0.918181837]
Tensor_14586: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14587: operator = ReshapeBackward, inputs = [Tensor_14586], outputs = [Tensor_14588], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 151296 stride: 768 stride: 1]
Tensor_14588: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14589: operator = LinearBackward, inputs = [Tensor_14588, Tensor_1745, Tensor_1747], outputs = [Tensor_14590, Tensor_14591, Tensor_14592], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_14590: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14591: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14592: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14593: operator = TypecastBackward, inputs = [Tensor_14590], outputs = [Tensor_14594], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_14594: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14595: operator = LayernormBackward, inputs = [Tensor_14594, Tensor_1739, Tensor_1741, Tensor_276, Tensor_277, Tensor_1742, Tensor_1743], outputs = [Tensor_14596, Tensor_14597, Tensor_14598], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_14596: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14597: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14598: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14599: operator = TypecastBackward, inputs = [Tensor_14596], outputs = [Tensor_14600], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_14600: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14601: operator = TypecastBackward, inputs = [Tensor_14600], outputs = [Tensor_14602], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_14602: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14603: operator = ElemwiseUnaryBackward, inputs = [Tensor_14602, Tensor_1733, Tensor_1735], outputs = [Tensor_14604], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_GELU]
Tensor_14604: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14605: operator = TypecastBackward, inputs = [Tensor_14604], outputs = [Tensor_14606], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_14606: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14607: operator = LinearBackward, inputs = [Tensor_14606, Tensor_1725, Tensor_1727], outputs = [Tensor_14608, Tensor_14609, Tensor_14610], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_14608: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14609: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14610: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14611: operator = TypecastBackward, inputs = [Tensor_14608], outputs = [Tensor_14612], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_14612: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14613: operator = ReshapeBackward, inputs = [Tensor_14612], outputs = [Tensor_14614], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 25216 shape: 768 stride: 768 stride: 1]
Tensor_14614: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14615: operator = LayernormBackward, inputs = [Tensor_14614, Tensor_1717, Tensor_1719, Tensor_284, Tensor_285, Tensor_1720, Tensor_1721], outputs = [Tensor_14616, Tensor_14619, Tensor_14620], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_14616: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14617: operator = ElemwiseBinaryForward, inputs = [Tensor_14580, Tensor_14616], outputs = [Tensor_14618], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_14618: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14619: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14620: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14621: operator = ElemwiseBinaryBackward, inputs = [Tensor_14618, nullptr, nullptr, nullptr], outputs = [Tensor_14622, Tensor_14623], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_14622: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14623: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14624: operator = ElemwiseUnaryBackward, inputs = [Tensor_14622, nullptr, nullptr], outputs = [Tensor_14625], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 1]
Tensor_14625: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14626: operator = TypecastBackward, inputs = [Tensor_14623], outputs = [Tensor_14627], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_14627: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14628: operator = ElemwiseBinaryBackward, inputs = [Tensor_14627, Tensor_1707, Tensor_1709, nullptr], outputs = [Tensor_14629, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_MUL]
Tensor_14629: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14630: operator = ElemwiseUnaryBackward, inputs = [Tensor_14629, nullptr, nullptr], outputs = [Tensor_14631], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_DIV scal1_float: 0.918181837]
Tensor_14631: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14632: operator = LinearBackward, inputs = [Tensor_14631, Tensor_1690, Tensor_1692], outputs = [Tensor_14633, Tensor_14634, Tensor_14635], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_14633: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14634: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14635: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14636: operator = TypecastBackward, inputs = [Tensor_14633], outputs = [Tensor_14637], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_14637: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14638: operator = LayernormBackward, inputs = [Tensor_14637, Tensor_1684, Tensor_1686, Tensor_264, Tensor_265, Tensor_1687, Tensor_1688], outputs = [Tensor_14639, Tensor_14640, Tensor_14641], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_14639: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14640: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14641: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14642: operator = TypecastBackward, inputs = [Tensor_14639], outputs = [Tensor_14643], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_14643: shape = [128, 197, 768], stride = [768, 98304, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14644: operator = ReshapeBackward, inputs = [Tensor_14643], outputs = [Tensor_14645], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 768 stride: 98304 stride: 1]
Tensor_14645: shape = [197, 128, 768], stride = [98304, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14646: operator = ReshapeBackward, inputs = [Tensor_14645], outputs = [Tensor_14647], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 197 shape: 128 shape: 768 stride: 98304 stride: 768 stride: 1]
Tensor_14647: shape = [197, 1536, 64], stride = [98304, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14648: operator = ContiguousBackward, inputs = [Tensor_14647], outputs = [Tensor_14649], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_14649: shape = [197, 1536, 64], stride = [64, 12608, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14650: operator = ReshapeBackward, inputs = [Tensor_14649], outputs = [Tensor_14651], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 197 shape: 1536 shape: 64 stride: 64 stride: 12608 stride: 1]
Tensor_14651: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14652: operator = BatchedMatmulBackward, inputs = [Tensor_14651, Tensor_1672, Tensor_1650], outputs = [Tensor_14653, Tensor_14654], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_14653: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14654: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14655: operator = TypecastBackward, inputs = [Tensor_14653], outputs = [Tensor_14656], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_14656: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14657: operator = ReshapeBackward, inputs = [Tensor_14654], outputs = [Tensor_14658], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_14658: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14659: operator = SoftmaxBackward, inputs = [Tensor_14656, Tensor_1670], outputs = [Tensor_14660], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dim: 2]
Tensor_14660: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14661: operator = ContiguousBackward, inputs = [Tensor_14658], outputs = [Tensor_14662], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_14662: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14663: operator = TypecastBackward, inputs = [Tensor_14660], outputs = [Tensor_14664], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_14664: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14665: operator = ReshapeBackward, inputs = [Tensor_14662], outputs = [Tensor_14666], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 64 stride: 151296 stride: 64 stride: 768 stride: 1]
Tensor_14666: shape = [128, 197, 12, 64], stride = [151296, 768, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14667: operator = ReshapeBackward, inputs = [Tensor_14664], outputs = [Tensor_14668], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 197 stride: 38809 stride: 197 stride: 1]
Tensor_14668: shape = [128, 12, 197, 197], stride = [465708, 38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14669: operator = ReshapeBackward, inputs = [Tensor_14666], outputs = [Tensor_14670], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 12 shape: 64 stride: 151296 stride: 768 stride: 64 stride: 1]
Tensor_14670: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14671: operator = MaskedFillBackward, inputs = [Tensor_14668, Tensor_1662], outputs = [Tensor_14672], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [value_float: -inf]
Tensor_14672: shape = [128, 12, 197, 197], stride = [465708, 38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14673: operator = LinearBackward, inputs = [Tensor_14670, Tensor_1618, Tensor_1620], outputs = [Tensor_14674, Tensor_14675, Tensor_14676], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_14674: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14675: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14676: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14677: operator = ReshapeBackward, inputs = [Tensor_14672], outputs = [Tensor_14678], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 197 stride: 465708 stride: 38809 stride: 197 stride: 1]
Tensor_14678: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14679: operator = TypecastBackward, inputs = [Tensor_14674], outputs = [Tensor_14680], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_14680: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14681: operator = BatchedMatmulBackward, inputs = [Tensor_14678, Tensor_1642, Tensor_1652], outputs = [Tensor_14682, Tensor_14683], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_14682: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14683: shape = [1536, 64, 197], stride = [12608, 1, 64], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14684: operator = ReshapeBackward, inputs = [Tensor_14682], outputs = [Tensor_14685], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_14685: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14686: operator = ReshapeBackward, inputs = [Tensor_14683], outputs = [Tensor_14687], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 64 shape: 197 stride: 12608 stride: 1 stride: 64]
Tensor_14687: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14688: operator = ContiguousBackward, inputs = [Tensor_14685], outputs = [Tensor_14689], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_14689: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14690: operator = ReshapeBackward, inputs = [Tensor_14687], outputs = [Tensor_14691], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_14691: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14692: operator = ReshapeBackward, inputs = [Tensor_14689], outputs = [Tensor_14693], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 64 stride: 151296 stride: 64 stride: 768 stride: 1]
Tensor_14693: shape = [128, 197, 12, 64], stride = [151296, 768, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14694: operator = ContiguousBackward, inputs = [Tensor_14691], outputs = [Tensor_14695], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_14695: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14696: operator = ReshapeBackward, inputs = [Tensor_14693], outputs = [Tensor_14697], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 12 shape: 64 stride: 151296 stride: 768 stride: 64 stride: 1]
Tensor_14697: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14698: operator = ReshapeBackward, inputs = [Tensor_14695], outputs = [Tensor_14699], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 64 stride: 151296 stride: 64 stride: 768 stride: 1]
Tensor_14699: shape = [128, 197, 12, 64], stride = [151296, 768, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14700: operator = ElemwiseUnaryBackward, inputs = [Tensor_14697, nullptr, nullptr], outputs = [Tensor_14701], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 0.125]
Tensor_14701: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14702: operator = ReshapeBackward, inputs = [Tensor_14699], outputs = [Tensor_14703], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 12 shape: 64 stride: 151296 stride: 768 stride: 64 stride: 1]
Tensor_14703: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14704: operator = LinearBackward, inputs = [Tensor_14701, Tensor_1602, Tensor_1604], outputs = [Tensor_14705, Tensor_14706, Tensor_14707], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_14705: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14706: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14707: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14708: operator = LinearBackward, inputs = [Tensor_14703, Tensor_1610, Tensor_1612], outputs = [Tensor_14709, Tensor_14710, Tensor_14711], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_14709: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14710: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14711: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14712: operator = TypecastBackward, inputs = [Tensor_14705], outputs = [Tensor_14713], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_14713: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14714: operator = ElemwiseBinaryForward, inputs = [Tensor_14680, Tensor_14713], outputs = [Tensor_14715], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_14715: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14716: operator = TypecastBackward, inputs = [Tensor_14709], outputs = [Tensor_14717], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_14717: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14718: operator = ElemwiseBinaryForward, inputs = [Tensor_14715, Tensor_14717], outputs = [Tensor_14719], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_14719: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14720: operator = LayernormBackward, inputs = [Tensor_14719, Tensor_1596, Tensor_1598, Tensor_268, Tensor_269, Tensor_1599, Tensor_1600], outputs = [Tensor_14721, Tensor_14724, Tensor_14725], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_14721: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14722: operator = ElemwiseBinaryForward, inputs = [Tensor_14625, Tensor_14721], outputs = [Tensor_14723], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_14723: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14724: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14725: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14726: operator = ElemwiseBinaryBackward, inputs = [Tensor_14723, nullptr, nullptr, nullptr], outputs = [Tensor_14727, Tensor_14728], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_14727: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14728: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14729: operator = ElemwiseUnaryBackward, inputs = [Tensor_14727, nullptr, nullptr], outputs = [Tensor_14730], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 1]
Tensor_14730: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14731: operator = TypecastBackward, inputs = [Tensor_14728], outputs = [Tensor_14732], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_14732: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14733: operator = ElemwiseBinaryBackward, inputs = [Tensor_14732, Tensor_1586, Tensor_1588, nullptr], outputs = [Tensor_14734, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_MUL]
Tensor_14734: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14735: operator = ElemwiseUnaryBackward, inputs = [Tensor_14734, nullptr, nullptr], outputs = [Tensor_14736], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_DIV scal1_float: 0.931818187]
Tensor_14736: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14737: operator = ReshapeBackward, inputs = [Tensor_14736], outputs = [Tensor_14738], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 151296 stride: 768 stride: 1]
Tensor_14738: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14739: operator = LinearBackward, inputs = [Tensor_14738, Tensor_1571, Tensor_1573], outputs = [Tensor_14740, Tensor_14741, Tensor_14742], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_14740: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14741: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14742: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14743: operator = TypecastBackward, inputs = [Tensor_14740], outputs = [Tensor_14744], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_14744: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14745: operator = LayernormBackward, inputs = [Tensor_14744, Tensor_1565, Tensor_1567, Tensor_236, Tensor_237, Tensor_1568, Tensor_1569], outputs = [Tensor_14746, Tensor_14747, Tensor_14748], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_14746: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14747: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14748: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14749: operator = TypecastBackward, inputs = [Tensor_14746], outputs = [Tensor_14750], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_14750: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14751: operator = TypecastBackward, inputs = [Tensor_14750], outputs = [Tensor_14752], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_14752: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14753: operator = ElemwiseUnaryBackward, inputs = [Tensor_14752, Tensor_1559, Tensor_1561], outputs = [Tensor_14754], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_GELU]
Tensor_14754: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14755: operator = TypecastBackward, inputs = [Tensor_14754], outputs = [Tensor_14756], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_14756: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14757: operator = LinearBackward, inputs = [Tensor_14756, Tensor_1551, Tensor_1553], outputs = [Tensor_14758, Tensor_14759, Tensor_14760], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_14758: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14759: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14760: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14761: operator = TypecastBackward, inputs = [Tensor_14758], outputs = [Tensor_14762], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_14762: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14763: operator = ReshapeBackward, inputs = [Tensor_14762], outputs = [Tensor_14764], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 25216 shape: 768 stride: 768 stride: 1]
Tensor_14764: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14765: operator = LayernormBackward, inputs = [Tensor_14764, Tensor_1543, Tensor_1545, Tensor_244, Tensor_245, Tensor_1546, Tensor_1547], outputs = [Tensor_14766, Tensor_14769, Tensor_14770], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_14766: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14767: operator = ElemwiseBinaryForward, inputs = [Tensor_14730, Tensor_14766], outputs = [Tensor_14768], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_14768: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14769: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14770: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14771: operator = ElemwiseBinaryBackward, inputs = [Tensor_14768, nullptr, nullptr, nullptr], outputs = [Tensor_14772, Tensor_14773], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_14772: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14773: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14774: operator = ElemwiseUnaryBackward, inputs = [Tensor_14772, nullptr, nullptr], outputs = [Tensor_14775], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 1]
Tensor_14775: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14776: operator = TypecastBackward, inputs = [Tensor_14773], outputs = [Tensor_14777], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_14777: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14778: operator = ElemwiseBinaryBackward, inputs = [Tensor_14777, Tensor_1533, Tensor_1535, nullptr], outputs = [Tensor_14779, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_MUL]
Tensor_14779: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14780: operator = ElemwiseUnaryBackward, inputs = [Tensor_14779, nullptr, nullptr], outputs = [Tensor_14781], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_DIV scal1_float: 0.931818187]
Tensor_14781: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14782: operator = LinearBackward, inputs = [Tensor_14781, Tensor_1516, Tensor_1518], outputs = [Tensor_14783, Tensor_14784, Tensor_14785], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_14783: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14784: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14785: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14786: operator = TypecastBackward, inputs = [Tensor_14783], outputs = [Tensor_14787], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_14787: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14788: operator = LayernormBackward, inputs = [Tensor_14787, Tensor_1510, Tensor_1512, Tensor_224, Tensor_225, Tensor_1513, Tensor_1514], outputs = [Tensor_14789, Tensor_14790, Tensor_14791], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_14789: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14790: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14791: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14792: operator = TypecastBackward, inputs = [Tensor_14789], outputs = [Tensor_14793], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_14793: shape = [128, 197, 768], stride = [768, 98304, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14794: operator = ReshapeBackward, inputs = [Tensor_14793], outputs = [Tensor_14795], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 768 stride: 98304 stride: 1]
Tensor_14795: shape = [197, 128, 768], stride = [98304, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14796: operator = ReshapeBackward, inputs = [Tensor_14795], outputs = [Tensor_14797], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 197 shape: 128 shape: 768 stride: 98304 stride: 768 stride: 1]
Tensor_14797: shape = [197, 1536, 64], stride = [98304, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14798: operator = ContiguousBackward, inputs = [Tensor_14797], outputs = [Tensor_14799], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_14799: shape = [197, 1536, 64], stride = [64, 12608, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14800: operator = ReshapeBackward, inputs = [Tensor_14799], outputs = [Tensor_14801], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 197 shape: 1536 shape: 64 stride: 64 stride: 12608 stride: 1]
Tensor_14801: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14802: operator = BatchedMatmulBackward, inputs = [Tensor_14801, Tensor_1498, Tensor_1476], outputs = [Tensor_14803, Tensor_14804], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_14803: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14804: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14805: operator = TypecastBackward, inputs = [Tensor_14803], outputs = [Tensor_14806], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_14806: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14807: operator = ReshapeBackward, inputs = [Tensor_14804], outputs = [Tensor_14808], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_14808: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14809: operator = SoftmaxBackward, inputs = [Tensor_14806, Tensor_1496], outputs = [Tensor_14810], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dim: 2]
Tensor_14810: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14811: operator = ContiguousBackward, inputs = [Tensor_14808], outputs = [Tensor_14812], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_14812: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14813: operator = TypecastBackward, inputs = [Tensor_14810], outputs = [Tensor_14814], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_14814: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14815: operator = ReshapeBackward, inputs = [Tensor_14812], outputs = [Tensor_14816], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 64 stride: 151296 stride: 64 stride: 768 stride: 1]
Tensor_14816: shape = [128, 197, 12, 64], stride = [151296, 768, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14817: operator = ReshapeBackward, inputs = [Tensor_14814], outputs = [Tensor_14818], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 197 stride: 38809 stride: 197 stride: 1]
Tensor_14818: shape = [128, 12, 197, 197], stride = [465708, 38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14819: operator = ReshapeBackward, inputs = [Tensor_14816], outputs = [Tensor_14820], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 12 shape: 64 stride: 151296 stride: 768 stride: 64 stride: 1]
Tensor_14820: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14821: operator = MaskedFillBackward, inputs = [Tensor_14818, Tensor_1488], outputs = [Tensor_14822], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [value_float: -inf]
Tensor_14822: shape = [128, 12, 197, 197], stride = [465708, 38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14823: operator = LinearBackward, inputs = [Tensor_14820, Tensor_1444, Tensor_1446], outputs = [Tensor_14824, Tensor_14825, Tensor_14826], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_14824: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14825: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14826: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14827: operator = ReshapeBackward, inputs = [Tensor_14822], outputs = [Tensor_14828], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 197 stride: 465708 stride: 38809 stride: 197 stride: 1]
Tensor_14828: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14829: operator = TypecastBackward, inputs = [Tensor_14824], outputs = [Tensor_14830], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_14830: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14831: operator = BatchedMatmulBackward, inputs = [Tensor_14828, Tensor_1468, Tensor_1478], outputs = [Tensor_14832, Tensor_14833], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_14832: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14833: shape = [1536, 64, 197], stride = [12608, 1, 64], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14834: operator = ReshapeBackward, inputs = [Tensor_14832], outputs = [Tensor_14835], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_14835: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14836: operator = ReshapeBackward, inputs = [Tensor_14833], outputs = [Tensor_14837], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 64 shape: 197 stride: 12608 stride: 1 stride: 64]
Tensor_14837: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14838: operator = ContiguousBackward, inputs = [Tensor_14835], outputs = [Tensor_14839], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_14839: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14840: operator = ReshapeBackward, inputs = [Tensor_14837], outputs = [Tensor_14841], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_14841: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14842: operator = ReshapeBackward, inputs = [Tensor_14839], outputs = [Tensor_14843], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 64 stride: 151296 stride: 64 stride: 768 stride: 1]
Tensor_14843: shape = [128, 197, 12, 64], stride = [151296, 768, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14844: operator = ContiguousBackward, inputs = [Tensor_14841], outputs = [Tensor_14845], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_14845: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14846: operator = ReshapeBackward, inputs = [Tensor_14843], outputs = [Tensor_14847], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 12 shape: 64 stride: 151296 stride: 768 stride: 64 stride: 1]
Tensor_14847: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14848: operator = ReshapeBackward, inputs = [Tensor_14845], outputs = [Tensor_14849], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 64 stride: 151296 stride: 64 stride: 768 stride: 1]
Tensor_14849: shape = [128, 197, 12, 64], stride = [151296, 768, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14850: operator = ElemwiseUnaryBackward, inputs = [Tensor_14847, nullptr, nullptr], outputs = [Tensor_14851], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 0.125]
Tensor_14851: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14852: operator = ReshapeBackward, inputs = [Tensor_14849], outputs = [Tensor_14853], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 12 shape: 64 stride: 151296 stride: 768 stride: 64 stride: 1]
Tensor_14853: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14854: operator = LinearBackward, inputs = [Tensor_14851, Tensor_1428, Tensor_1430], outputs = [Tensor_14855, Tensor_14856, Tensor_14857], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_14855: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14856: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14857: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14858: operator = LinearBackward, inputs = [Tensor_14853, Tensor_1436, Tensor_1438], outputs = [Tensor_14859, Tensor_14860, Tensor_14861], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_14859: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14860: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14861: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14862: operator = TypecastBackward, inputs = [Tensor_14855], outputs = [Tensor_14863], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_14863: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14864: operator = ElemwiseBinaryForward, inputs = [Tensor_14830, Tensor_14863], outputs = [Tensor_14865], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_14865: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14866: operator = TypecastBackward, inputs = [Tensor_14859], outputs = [Tensor_14867], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_14867: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14868: operator = ElemwiseBinaryForward, inputs = [Tensor_14865, Tensor_14867], outputs = [Tensor_14869], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_14869: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14870: operator = LayernormBackward, inputs = [Tensor_14869, Tensor_1422, Tensor_1424, Tensor_228, Tensor_229, Tensor_1425, Tensor_1426], outputs = [Tensor_14871, Tensor_14874, Tensor_14875], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_14871: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14872: operator = ElemwiseBinaryForward, inputs = [Tensor_14775, Tensor_14871], outputs = [Tensor_14873], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_14873: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14874: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14875: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14876: operator = ElemwiseBinaryBackward, inputs = [Tensor_14873, nullptr, nullptr, nullptr], outputs = [Tensor_14877, Tensor_14878], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_14877: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14878: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14879: operator = ElemwiseUnaryBackward, inputs = [Tensor_14877, nullptr, nullptr], outputs = [Tensor_14880], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 1]
Tensor_14880: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14881: operator = TypecastBackward, inputs = [Tensor_14878], outputs = [Tensor_14882], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_14882: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14883: operator = ElemwiseBinaryBackward, inputs = [Tensor_14882, Tensor_1412, Tensor_1414, nullptr], outputs = [Tensor_14884, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_MUL]
Tensor_14884: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14885: operator = ElemwiseUnaryBackward, inputs = [Tensor_14884, nullptr, nullptr], outputs = [Tensor_14886], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_DIV scal1_float: 0.945454538]
Tensor_14886: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14887: operator = ReshapeBackward, inputs = [Tensor_14886], outputs = [Tensor_14888], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 151296 stride: 768 stride: 1]
Tensor_14888: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14889: operator = LinearBackward, inputs = [Tensor_14888, Tensor_1397, Tensor_1399], outputs = [Tensor_14890, Tensor_14891, Tensor_14892], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_14890: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14891: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14892: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14893: operator = TypecastBackward, inputs = [Tensor_14890], outputs = [Tensor_14894], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_14894: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14895: operator = LayernormBackward, inputs = [Tensor_14894, Tensor_1391, Tensor_1393, Tensor_196, Tensor_197, Tensor_1394, Tensor_1395], outputs = [Tensor_14896, Tensor_14897, Tensor_14898], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_14896: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14897: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14898: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14899: operator = TypecastBackward, inputs = [Tensor_14896], outputs = [Tensor_14900], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_14900: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14901: operator = TypecastBackward, inputs = [Tensor_14900], outputs = [Tensor_14902], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_14902: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14903: operator = ElemwiseUnaryBackward, inputs = [Tensor_14902, Tensor_1385, Tensor_1387], outputs = [Tensor_14904], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_GELU]
Tensor_14904: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14905: operator = TypecastBackward, inputs = [Tensor_14904], outputs = [Tensor_14906], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_14906: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14907: operator = LinearBackward, inputs = [Tensor_14906, Tensor_1377, Tensor_1379], outputs = [Tensor_14908, Tensor_14909, Tensor_14910], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_14908: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14909: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14910: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14911: operator = TypecastBackward, inputs = [Tensor_14908], outputs = [Tensor_14912], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_14912: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14913: operator = ReshapeBackward, inputs = [Tensor_14912], outputs = [Tensor_14914], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 25216 shape: 768 stride: 768 stride: 1]
Tensor_14914: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14915: operator = LayernormBackward, inputs = [Tensor_14914, Tensor_1369, Tensor_1371, Tensor_204, Tensor_205, Tensor_1372, Tensor_1373], outputs = [Tensor_14916, Tensor_14919, Tensor_14920], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_14916: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14917: operator = ElemwiseBinaryForward, inputs = [Tensor_14880, Tensor_14916], outputs = [Tensor_14918], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_14918: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14919: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14920: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14921: operator = ElemwiseBinaryBackward, inputs = [Tensor_14918, nullptr, nullptr, nullptr], outputs = [Tensor_14922, Tensor_14923], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_14922: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14923: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14924: operator = ElemwiseUnaryBackward, inputs = [Tensor_14922, nullptr, nullptr], outputs = [Tensor_14925], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 1]
Tensor_14925: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14926: operator = TypecastBackward, inputs = [Tensor_14923], outputs = [Tensor_14927], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_14927: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14928: operator = ElemwiseBinaryBackward, inputs = [Tensor_14927, Tensor_1359, Tensor_1361, nullptr], outputs = [Tensor_14929, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_MUL]
Tensor_14929: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14930: operator = ElemwiseUnaryBackward, inputs = [Tensor_14929, nullptr, nullptr], outputs = [Tensor_14931], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_DIV scal1_float: 0.945454538]
Tensor_14931: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14932: operator = LinearBackward, inputs = [Tensor_14931, Tensor_1342, Tensor_1344], outputs = [Tensor_14933, Tensor_14934, Tensor_14935], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_14933: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14934: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14935: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14936: operator = TypecastBackward, inputs = [Tensor_14933], outputs = [Tensor_14937], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_14937: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14938: operator = LayernormBackward, inputs = [Tensor_14937, Tensor_1336, Tensor_1338, Tensor_184, Tensor_185, Tensor_1339, Tensor_1340], outputs = [Tensor_14939, Tensor_14940, Tensor_14941], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_14939: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14940: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14941: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14942: operator = TypecastBackward, inputs = [Tensor_14939], outputs = [Tensor_14943], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_14943: shape = [128, 197, 768], stride = [768, 98304, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14944: operator = ReshapeBackward, inputs = [Tensor_14943], outputs = [Tensor_14945], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 768 stride: 98304 stride: 1]
Tensor_14945: shape = [197, 128, 768], stride = [98304, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14946: operator = ReshapeBackward, inputs = [Tensor_14945], outputs = [Tensor_14947], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 197 shape: 128 shape: 768 stride: 98304 stride: 768 stride: 1]
Tensor_14947: shape = [197, 1536, 64], stride = [98304, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14948: operator = ContiguousBackward, inputs = [Tensor_14947], outputs = [Tensor_14949], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_14949: shape = [197, 1536, 64], stride = [64, 12608, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14950: operator = ReshapeBackward, inputs = [Tensor_14949], outputs = [Tensor_14951], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 197 shape: 1536 shape: 64 stride: 64 stride: 12608 stride: 1]
Tensor_14951: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14952: operator = BatchedMatmulBackward, inputs = [Tensor_14951, Tensor_1324, Tensor_1302], outputs = [Tensor_14953, Tensor_14954], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_14953: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14954: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14955: operator = TypecastBackward, inputs = [Tensor_14953], outputs = [Tensor_14956], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_14956: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14957: operator = ReshapeBackward, inputs = [Tensor_14954], outputs = [Tensor_14958], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_14958: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14959: operator = SoftmaxBackward, inputs = [Tensor_14956, Tensor_1322], outputs = [Tensor_14960], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dim: 2]
Tensor_14960: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14961: operator = ContiguousBackward, inputs = [Tensor_14958], outputs = [Tensor_14962], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_14962: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14963: operator = TypecastBackward, inputs = [Tensor_14960], outputs = [Tensor_14964], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_14964: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14965: operator = ReshapeBackward, inputs = [Tensor_14962], outputs = [Tensor_14966], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 64 stride: 151296 stride: 64 stride: 768 stride: 1]
Tensor_14966: shape = [128, 197, 12, 64], stride = [151296, 768, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14967: operator = ReshapeBackward, inputs = [Tensor_14964], outputs = [Tensor_14968], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 197 stride: 38809 stride: 197 stride: 1]
Tensor_14968: shape = [128, 12, 197, 197], stride = [465708, 38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14969: operator = ReshapeBackward, inputs = [Tensor_14966], outputs = [Tensor_14970], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 12 shape: 64 stride: 151296 stride: 768 stride: 64 stride: 1]
Tensor_14970: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14971: operator = MaskedFillBackward, inputs = [Tensor_14968, Tensor_1314], outputs = [Tensor_14972], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [value_float: -inf]
Tensor_14972: shape = [128, 12, 197, 197], stride = [465708, 38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14973: operator = LinearBackward, inputs = [Tensor_14970, Tensor_1270, Tensor_1272], outputs = [Tensor_14974, Tensor_14975, Tensor_14976], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_14974: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14975: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14976: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14977: operator = ReshapeBackward, inputs = [Tensor_14972], outputs = [Tensor_14978], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 197 stride: 465708 stride: 38809 stride: 197 stride: 1]
Tensor_14978: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14979: operator = TypecastBackward, inputs = [Tensor_14974], outputs = [Tensor_14980], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_14980: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14981: operator = BatchedMatmulBackward, inputs = [Tensor_14978, Tensor_1294, Tensor_1304], outputs = [Tensor_14982, Tensor_14983], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_14982: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_14983: shape = [1536, 64, 197], stride = [12608, 1, 64], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14984: operator = ReshapeBackward, inputs = [Tensor_14982], outputs = [Tensor_14985], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_14985: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14986: operator = ReshapeBackward, inputs = [Tensor_14983], outputs = [Tensor_14987], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 64 shape: 197 stride: 12608 stride: 1 stride: 64]
Tensor_14987: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14988: operator = ContiguousBackward, inputs = [Tensor_14985], outputs = [Tensor_14989], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_14989: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14990: operator = ReshapeBackward, inputs = [Tensor_14987], outputs = [Tensor_14991], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_14991: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14992: operator = ReshapeBackward, inputs = [Tensor_14989], outputs = [Tensor_14993], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 64 stride: 151296 stride: 64 stride: 768 stride: 1]
Tensor_14993: shape = [128, 197, 12, 64], stride = [151296, 768, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14994: operator = ContiguousBackward, inputs = [Tensor_14991], outputs = [Tensor_14995], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_14995: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14996: operator = ReshapeBackward, inputs = [Tensor_14993], outputs = [Tensor_14997], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 12 shape: 64 stride: 151296 stride: 768 stride: 64 stride: 1]
Tensor_14997: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_14998: operator = ReshapeBackward, inputs = [Tensor_14995], outputs = [Tensor_14999], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 64 stride: 151296 stride: 64 stride: 768 stride: 1]
Tensor_14999: shape = [128, 197, 12, 64], stride = [151296, 768, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15000: operator = ElemwiseUnaryBackward, inputs = [Tensor_14997, nullptr, nullptr], outputs = [Tensor_15001], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 0.125]
Tensor_15001: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15002: operator = ReshapeBackward, inputs = [Tensor_14999], outputs = [Tensor_15003], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 12 shape: 64 stride: 151296 stride: 768 stride: 64 stride: 1]
Tensor_15003: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15004: operator = LinearBackward, inputs = [Tensor_15001, Tensor_1254, Tensor_1256], outputs = [Tensor_15005, Tensor_15006, Tensor_15007], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_15005: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_15006: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_15007: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15008: operator = LinearBackward, inputs = [Tensor_15003, Tensor_1262, Tensor_1264], outputs = [Tensor_15009, Tensor_15010, Tensor_15011], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_15009: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_15010: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_15011: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15012: operator = TypecastBackward, inputs = [Tensor_15005], outputs = [Tensor_15013], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15013: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15014: operator = ElemwiseBinaryForward, inputs = [Tensor_14980, Tensor_15013], outputs = [Tensor_15015], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_15015: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15016: operator = TypecastBackward, inputs = [Tensor_15009], outputs = [Tensor_15017], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15017: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15018: operator = ElemwiseBinaryForward, inputs = [Tensor_15015, Tensor_15017], outputs = [Tensor_15019], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_15019: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15020: operator = LayernormBackward, inputs = [Tensor_15019, Tensor_1248, Tensor_1250, Tensor_188, Tensor_189, Tensor_1251, Tensor_1252], outputs = [Tensor_15021, Tensor_15024, Tensor_15025], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_15021: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15022: operator = ElemwiseBinaryForward, inputs = [Tensor_14925, Tensor_15021], outputs = [Tensor_15023], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_15023: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_15024: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_15025: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15026: operator = ElemwiseBinaryBackward, inputs = [Tensor_15023, nullptr, nullptr, nullptr], outputs = [Tensor_15027, Tensor_15028], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_15027: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_15028: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15029: operator = ElemwiseUnaryBackward, inputs = [Tensor_15027, nullptr, nullptr], outputs = [Tensor_15030], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 1]
Tensor_15030: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15031: operator = TypecastBackward, inputs = [Tensor_15028], outputs = [Tensor_15032], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_15032: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15033: operator = ElemwiseBinaryBackward, inputs = [Tensor_15032, Tensor_1238, Tensor_1240, nullptr], outputs = [Tensor_15034, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_MUL]
Tensor_15034: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15035: operator = ElemwiseUnaryBackward, inputs = [Tensor_15034, nullptr, nullptr], outputs = [Tensor_15036], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_DIV scal1_float: 0.959090889]
Tensor_15036: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15037: operator = ReshapeBackward, inputs = [Tensor_15036], outputs = [Tensor_15038], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 151296 stride: 768 stride: 1]
Tensor_15038: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15039: operator = LinearBackward, inputs = [Tensor_15038, Tensor_1223, Tensor_1225], outputs = [Tensor_15040, Tensor_15041, Tensor_15042], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_15040: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_15041: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_15042: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15043: operator = TypecastBackward, inputs = [Tensor_15040], outputs = [Tensor_15044], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15044: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15045: operator = LayernormBackward, inputs = [Tensor_15044, Tensor_1217, Tensor_1219, Tensor_156, Tensor_157, Tensor_1220, Tensor_1221], outputs = [Tensor_15046, Tensor_15047, Tensor_15048], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_15046: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_15047: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_15048: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15049: operator = TypecastBackward, inputs = [Tensor_15046], outputs = [Tensor_15050], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_15050: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15051: operator = TypecastBackward, inputs = [Tensor_15050], outputs = [Tensor_15052], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15052: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15053: operator = ElemwiseUnaryBackward, inputs = [Tensor_15052, Tensor_1211, Tensor_1213], outputs = [Tensor_15054], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_GELU]
Tensor_15054: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15055: operator = TypecastBackward, inputs = [Tensor_15054], outputs = [Tensor_15056], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_15056: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15057: operator = LinearBackward, inputs = [Tensor_15056, Tensor_1203, Tensor_1205], outputs = [Tensor_15058, Tensor_15059, Tensor_15060], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_15058: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_15059: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_15060: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15061: operator = TypecastBackward, inputs = [Tensor_15058], outputs = [Tensor_15062], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15062: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15063: operator = ReshapeBackward, inputs = [Tensor_15062], outputs = [Tensor_15064], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 25216 shape: 768 stride: 768 stride: 1]
Tensor_15064: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15065: operator = LayernormBackward, inputs = [Tensor_15064, Tensor_1195, Tensor_1197, Tensor_164, Tensor_165, Tensor_1198, Tensor_1199], outputs = [Tensor_15066, Tensor_15069, Tensor_15070], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_15066: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15067: operator = ElemwiseBinaryForward, inputs = [Tensor_15030, Tensor_15066], outputs = [Tensor_15068], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_15068: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_15069: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_15070: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15071: operator = ElemwiseBinaryBackward, inputs = [Tensor_15068, nullptr, nullptr, nullptr], outputs = [Tensor_15072, Tensor_15073], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_15072: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_15073: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15074: operator = ElemwiseUnaryBackward, inputs = [Tensor_15072, nullptr, nullptr], outputs = [Tensor_15075], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 1]
Tensor_15075: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15076: operator = TypecastBackward, inputs = [Tensor_15073], outputs = [Tensor_15077], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_15077: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15078: operator = ElemwiseBinaryBackward, inputs = [Tensor_15077, Tensor_1185, Tensor_1187, nullptr], outputs = [Tensor_15079, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_MUL]
Tensor_15079: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15080: operator = ElemwiseUnaryBackward, inputs = [Tensor_15079, nullptr, nullptr], outputs = [Tensor_15081], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_DIV scal1_float: 0.959090889]
Tensor_15081: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15082: operator = LinearBackward, inputs = [Tensor_15081, Tensor_1168, Tensor_1170], outputs = [Tensor_15083, Tensor_15084, Tensor_15085], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_15083: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_15084: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_15085: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15086: operator = TypecastBackward, inputs = [Tensor_15083], outputs = [Tensor_15087], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15087: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15088: operator = LayernormBackward, inputs = [Tensor_15087, Tensor_1162, Tensor_1164, Tensor_144, Tensor_145, Tensor_1165, Tensor_1166], outputs = [Tensor_15089, Tensor_15090, Tensor_15091], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_15089: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_15090: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_15091: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15092: operator = TypecastBackward, inputs = [Tensor_15089], outputs = [Tensor_15093], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_15093: shape = [128, 197, 768], stride = [768, 98304, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15094: operator = ReshapeBackward, inputs = [Tensor_15093], outputs = [Tensor_15095], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 768 stride: 98304 stride: 1]
Tensor_15095: shape = [197, 128, 768], stride = [98304, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15096: operator = ReshapeBackward, inputs = [Tensor_15095], outputs = [Tensor_15097], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 197 shape: 128 shape: 768 stride: 98304 stride: 768 stride: 1]
Tensor_15097: shape = [197, 1536, 64], stride = [98304, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15098: operator = ContiguousBackward, inputs = [Tensor_15097], outputs = [Tensor_15099], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_15099: shape = [197, 1536, 64], stride = [64, 12608, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15100: operator = ReshapeBackward, inputs = [Tensor_15099], outputs = [Tensor_15101], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 197 shape: 1536 shape: 64 stride: 64 stride: 12608 stride: 1]
Tensor_15101: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15102: operator = BatchedMatmulBackward, inputs = [Tensor_15101, Tensor_1150, Tensor_1128], outputs = [Tensor_15103, Tensor_15104], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_15103: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_15104: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15105: operator = TypecastBackward, inputs = [Tensor_15103], outputs = [Tensor_15106], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15106: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15107: operator = ReshapeBackward, inputs = [Tensor_15104], outputs = [Tensor_15108], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_15108: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15109: operator = SoftmaxBackward, inputs = [Tensor_15106, Tensor_1148], outputs = [Tensor_15110], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dim: 2]
Tensor_15110: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15111: operator = ContiguousBackward, inputs = [Tensor_15108], outputs = [Tensor_15112], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_15112: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15113: operator = TypecastBackward, inputs = [Tensor_15110], outputs = [Tensor_15114], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_15114: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15115: operator = ReshapeBackward, inputs = [Tensor_15112], outputs = [Tensor_15116], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 64 stride: 151296 stride: 64 stride: 768 stride: 1]
Tensor_15116: shape = [128, 197, 12, 64], stride = [151296, 768, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15117: operator = ReshapeBackward, inputs = [Tensor_15114], outputs = [Tensor_15118], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 197 stride: 38809 stride: 197 stride: 1]
Tensor_15118: shape = [128, 12, 197, 197], stride = [465708, 38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15119: operator = ReshapeBackward, inputs = [Tensor_15116], outputs = [Tensor_15120], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 12 shape: 64 stride: 151296 stride: 768 stride: 64 stride: 1]
Tensor_15120: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15121: operator = MaskedFillBackward, inputs = [Tensor_15118, Tensor_1140], outputs = [Tensor_15122], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [value_float: -inf]
Tensor_15122: shape = [128, 12, 197, 197], stride = [465708, 38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15123: operator = LinearBackward, inputs = [Tensor_15120, Tensor_1096, Tensor_1098], outputs = [Tensor_15124, Tensor_15125, Tensor_15126], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_15124: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_15125: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_15126: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15127: operator = ReshapeBackward, inputs = [Tensor_15122], outputs = [Tensor_15128], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 197 stride: 465708 stride: 38809 stride: 197 stride: 1]
Tensor_15128: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15129: operator = TypecastBackward, inputs = [Tensor_15124], outputs = [Tensor_15130], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15130: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15131: operator = BatchedMatmulBackward, inputs = [Tensor_15128, Tensor_1120, Tensor_1130], outputs = [Tensor_15132, Tensor_15133], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_15132: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_15133: shape = [1536, 64, 197], stride = [12608, 1, 64], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15134: operator = ReshapeBackward, inputs = [Tensor_15132], outputs = [Tensor_15135], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_15135: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15136: operator = ReshapeBackward, inputs = [Tensor_15133], outputs = [Tensor_15137], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 64 shape: 197 stride: 12608 stride: 1 stride: 64]
Tensor_15137: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15138: operator = ContiguousBackward, inputs = [Tensor_15135], outputs = [Tensor_15139], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_15139: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15140: operator = ReshapeBackward, inputs = [Tensor_15137], outputs = [Tensor_15141], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_15141: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15142: operator = ReshapeBackward, inputs = [Tensor_15139], outputs = [Tensor_15143], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 64 stride: 151296 stride: 64 stride: 768 stride: 1]
Tensor_15143: shape = [128, 197, 12, 64], stride = [151296, 768, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15144: operator = ContiguousBackward, inputs = [Tensor_15141], outputs = [Tensor_15145], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_15145: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15146: operator = ReshapeBackward, inputs = [Tensor_15143], outputs = [Tensor_15147], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 12 shape: 64 stride: 151296 stride: 768 stride: 64 stride: 1]
Tensor_15147: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15148: operator = ReshapeBackward, inputs = [Tensor_15145], outputs = [Tensor_15149], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 64 stride: 151296 stride: 64 stride: 768 stride: 1]
Tensor_15149: shape = [128, 197, 12, 64], stride = [151296, 768, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15150: operator = ElemwiseUnaryBackward, inputs = [Tensor_15147, nullptr, nullptr], outputs = [Tensor_15151], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 0.125]
Tensor_15151: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15152: operator = ReshapeBackward, inputs = [Tensor_15149], outputs = [Tensor_15153], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 12 shape: 64 stride: 151296 stride: 768 stride: 64 stride: 1]
Tensor_15153: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15154: operator = LinearBackward, inputs = [Tensor_15151, Tensor_1080, Tensor_1082], outputs = [Tensor_15155, Tensor_15156, Tensor_15157], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_15155: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_15156: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_15157: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15158: operator = LinearBackward, inputs = [Tensor_15153, Tensor_1088, Tensor_1090], outputs = [Tensor_15159, Tensor_15160, Tensor_15161], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_15159: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_15160: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_15161: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15162: operator = TypecastBackward, inputs = [Tensor_15155], outputs = [Tensor_15163], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15163: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15164: operator = ElemwiseBinaryForward, inputs = [Tensor_15130, Tensor_15163], outputs = [Tensor_15165], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_15165: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15166: operator = TypecastBackward, inputs = [Tensor_15159], outputs = [Tensor_15167], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15167: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15168: operator = ElemwiseBinaryForward, inputs = [Tensor_15165, Tensor_15167], outputs = [Tensor_15169], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_15169: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15170: operator = LayernormBackward, inputs = [Tensor_15169, Tensor_1074, Tensor_1076, Tensor_148, Tensor_149, Tensor_1077, Tensor_1078], outputs = [Tensor_15171, Tensor_15174, Tensor_15175], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_15171: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15172: operator = ElemwiseBinaryForward, inputs = [Tensor_15075, Tensor_15171], outputs = [Tensor_15173], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_15173: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_15174: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_15175: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15176: operator = ElemwiseBinaryBackward, inputs = [Tensor_15173, nullptr, nullptr, nullptr], outputs = [Tensor_15177, Tensor_15178], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_15177: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_15178: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15179: operator = ElemwiseUnaryBackward, inputs = [Tensor_15177, nullptr, nullptr], outputs = [Tensor_15180], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 1]
Tensor_15180: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15181: operator = TypecastBackward, inputs = [Tensor_15178], outputs = [Tensor_15182], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_15182: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15183: operator = ElemwiseBinaryBackward, inputs = [Tensor_15182, Tensor_1064, Tensor_1066, nullptr], outputs = [Tensor_15184, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_MUL]
Tensor_15184: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15185: operator = ElemwiseUnaryBackward, inputs = [Tensor_15184, nullptr, nullptr], outputs = [Tensor_15186], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_DIV scal1_float: 0.972727299]
Tensor_15186: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15187: operator = ReshapeBackward, inputs = [Tensor_15186], outputs = [Tensor_15188], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 151296 stride: 768 stride: 1]
Tensor_15188: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15189: operator = LinearBackward, inputs = [Tensor_15188, Tensor_1049, Tensor_1051], outputs = [Tensor_15190, Tensor_15191, Tensor_15192], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_15190: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_15191: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_15192: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15193: operator = TypecastBackward, inputs = [Tensor_15190], outputs = [Tensor_15194], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15194: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15195: operator = LayernormBackward, inputs = [Tensor_15194, Tensor_1043, Tensor_1045, Tensor_116, Tensor_117, Tensor_1046, Tensor_1047], outputs = [Tensor_15196, Tensor_15197, Tensor_15198], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_15196: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_15197: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_15198: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15199: operator = TypecastBackward, inputs = [Tensor_15196], outputs = [Tensor_15200], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_15200: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15201: operator = TypecastBackward, inputs = [Tensor_15200], outputs = [Tensor_15202], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15202: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15203: operator = ElemwiseUnaryBackward, inputs = [Tensor_15202, Tensor_1037, Tensor_1039], outputs = [Tensor_15204], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_GELU]
Tensor_15204: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15205: operator = TypecastBackward, inputs = [Tensor_15204], outputs = [Tensor_15206], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_15206: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15207: operator = LinearBackward, inputs = [Tensor_15206, Tensor_1029, Tensor_1031], outputs = [Tensor_15208, Tensor_15209, Tensor_15210], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_15208: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_15209: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_15210: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15211: operator = TypecastBackward, inputs = [Tensor_15208], outputs = [Tensor_15212], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15212: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15213: operator = ReshapeBackward, inputs = [Tensor_15212], outputs = [Tensor_15214], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 25216 shape: 768 stride: 768 stride: 1]
Tensor_15214: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15215: operator = LayernormBackward, inputs = [Tensor_15214, Tensor_1021, Tensor_1023, Tensor_124, Tensor_125, Tensor_1024, Tensor_1025], outputs = [Tensor_15216, Tensor_15219, Tensor_15220], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_15216: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15217: operator = ElemwiseBinaryForward, inputs = [Tensor_15180, Tensor_15216], outputs = [Tensor_15218], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_15218: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_15219: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_15220: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15221: operator = ElemwiseBinaryBackward, inputs = [Tensor_15218, nullptr, nullptr, nullptr], outputs = [Tensor_15222, Tensor_15223], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_15222: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_15223: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15224: operator = ElemwiseUnaryBackward, inputs = [Tensor_15222, nullptr, nullptr], outputs = [Tensor_15225], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 1]
Tensor_15225: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15226: operator = TypecastBackward, inputs = [Tensor_15223], outputs = [Tensor_15227], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_15227: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15228: operator = ElemwiseBinaryBackward, inputs = [Tensor_15227, Tensor_1011, Tensor_1013, nullptr], outputs = [Tensor_15229, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_MUL]
Tensor_15229: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15230: operator = ElemwiseUnaryBackward, inputs = [Tensor_15229, nullptr, nullptr], outputs = [Tensor_15231], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_DIV scal1_float: 0.972727299]
Tensor_15231: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15232: operator = LinearBackward, inputs = [Tensor_15231, Tensor_994, Tensor_996], outputs = [Tensor_15233, Tensor_15234, Tensor_15235], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_15233: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_15234: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_15235: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15236: operator = TypecastBackward, inputs = [Tensor_15233], outputs = [Tensor_15237], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15237: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15238: operator = LayernormBackward, inputs = [Tensor_15237, Tensor_988, Tensor_990, Tensor_104, Tensor_105, Tensor_991, Tensor_992], outputs = [Tensor_15239, Tensor_15240, Tensor_15241], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_15239: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_15240: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_15241: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15242: operator = TypecastBackward, inputs = [Tensor_15239], outputs = [Tensor_15243], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_15243: shape = [128, 197, 768], stride = [768, 98304, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15244: operator = ReshapeBackward, inputs = [Tensor_15243], outputs = [Tensor_15245], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 768 stride: 98304 stride: 1]
Tensor_15245: shape = [197, 128, 768], stride = [98304, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15246: operator = ReshapeBackward, inputs = [Tensor_15245], outputs = [Tensor_15247], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 197 shape: 128 shape: 768 stride: 98304 stride: 768 stride: 1]
Tensor_15247: shape = [197, 1536, 64], stride = [98304, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15248: operator = ContiguousBackward, inputs = [Tensor_15247], outputs = [Tensor_15249], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_15249: shape = [197, 1536, 64], stride = [64, 12608, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15250: operator = ReshapeBackward, inputs = [Tensor_15249], outputs = [Tensor_15251], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 197 shape: 1536 shape: 64 stride: 64 stride: 12608 stride: 1]
Tensor_15251: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15252: operator = BatchedMatmulBackward, inputs = [Tensor_15251, Tensor_976, Tensor_954], outputs = [Tensor_15253, Tensor_15254], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_15253: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_15254: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15255: operator = TypecastBackward, inputs = [Tensor_15253], outputs = [Tensor_15256], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15256: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15257: operator = ReshapeBackward, inputs = [Tensor_15254], outputs = [Tensor_15258], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_15258: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15259: operator = SoftmaxBackward, inputs = [Tensor_15256, Tensor_974], outputs = [Tensor_15260], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dim: 2]
Tensor_15260: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15261: operator = ContiguousBackward, inputs = [Tensor_15258], outputs = [Tensor_15262], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_15262: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15263: operator = TypecastBackward, inputs = [Tensor_15260], outputs = [Tensor_15264], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_15264: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15265: operator = ReshapeBackward, inputs = [Tensor_15262], outputs = [Tensor_15266], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 64 stride: 151296 stride: 64 stride: 768 stride: 1]
Tensor_15266: shape = [128, 197, 12, 64], stride = [151296, 768, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15267: operator = ReshapeBackward, inputs = [Tensor_15264], outputs = [Tensor_15268], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 197 stride: 38809 stride: 197 stride: 1]
Tensor_15268: shape = [128, 12, 197, 197], stride = [465708, 38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15269: operator = ReshapeBackward, inputs = [Tensor_15266], outputs = [Tensor_15270], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 12 shape: 64 stride: 151296 stride: 768 stride: 64 stride: 1]
Tensor_15270: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15271: operator = MaskedFillBackward, inputs = [Tensor_15268, Tensor_966], outputs = [Tensor_15272], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [value_float: -inf]
Tensor_15272: shape = [128, 12, 197, 197], stride = [465708, 38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15273: operator = LinearBackward, inputs = [Tensor_15270, Tensor_922, Tensor_924], outputs = [Tensor_15274, Tensor_15275, Tensor_15276], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_15274: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_15275: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_15276: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15277: operator = ReshapeBackward, inputs = [Tensor_15272], outputs = [Tensor_15278], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 197 stride: 465708 stride: 38809 stride: 197 stride: 1]
Tensor_15278: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15279: operator = TypecastBackward, inputs = [Tensor_15274], outputs = [Tensor_15280], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15280: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15281: operator = BatchedMatmulBackward, inputs = [Tensor_15278, Tensor_946, Tensor_956], outputs = [Tensor_15282, Tensor_15283], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_15282: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_15283: shape = [1536, 64, 197], stride = [12608, 1, 64], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15284: operator = ReshapeBackward, inputs = [Tensor_15282], outputs = [Tensor_15285], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_15285: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15286: operator = ReshapeBackward, inputs = [Tensor_15283], outputs = [Tensor_15287], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 64 shape: 197 stride: 12608 stride: 1 stride: 64]
Tensor_15287: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15288: operator = ContiguousBackward, inputs = [Tensor_15285], outputs = [Tensor_15289], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_15289: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15290: operator = ReshapeBackward, inputs = [Tensor_15287], outputs = [Tensor_15291], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_15291: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15292: operator = ReshapeBackward, inputs = [Tensor_15289], outputs = [Tensor_15293], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 64 stride: 151296 stride: 64 stride: 768 stride: 1]
Tensor_15293: shape = [128, 197, 12, 64], stride = [151296, 768, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15294: operator = ContiguousBackward, inputs = [Tensor_15291], outputs = [Tensor_15295], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_15295: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15296: operator = ReshapeBackward, inputs = [Tensor_15293], outputs = [Tensor_15297], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 12 shape: 64 stride: 151296 stride: 768 stride: 64 stride: 1]
Tensor_15297: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15298: operator = ReshapeBackward, inputs = [Tensor_15295], outputs = [Tensor_15299], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 64 stride: 151296 stride: 64 stride: 768 stride: 1]
Tensor_15299: shape = [128, 197, 12, 64], stride = [151296, 768, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15300: operator = ElemwiseUnaryBackward, inputs = [Tensor_15297, nullptr, nullptr], outputs = [Tensor_15301], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 0.125]
Tensor_15301: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15302: operator = ReshapeBackward, inputs = [Tensor_15299], outputs = [Tensor_15303], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 12 shape: 64 stride: 151296 stride: 768 stride: 64 stride: 1]
Tensor_15303: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15304: operator = LinearBackward, inputs = [Tensor_15301, Tensor_906, Tensor_908], outputs = [Tensor_15305, Tensor_15306, Tensor_15307], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_15305: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_15306: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_15307: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15308: operator = LinearBackward, inputs = [Tensor_15303, Tensor_914, Tensor_916], outputs = [Tensor_15309, Tensor_15310, Tensor_15311], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_15309: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_15310: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_15311: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15312: operator = TypecastBackward, inputs = [Tensor_15305], outputs = [Tensor_15313], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15313: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15314: operator = ElemwiseBinaryForward, inputs = [Tensor_15280, Tensor_15313], outputs = [Tensor_15315], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_15315: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15316: operator = TypecastBackward, inputs = [Tensor_15309], outputs = [Tensor_15317], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15317: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15318: operator = ElemwiseBinaryForward, inputs = [Tensor_15315, Tensor_15317], outputs = [Tensor_15319], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_15319: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15320: operator = LayernormBackward, inputs = [Tensor_15319, Tensor_900, Tensor_902, Tensor_108, Tensor_109, Tensor_903, Tensor_904], outputs = [Tensor_15321, Tensor_15324, Tensor_15325], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_15321: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15322: operator = ElemwiseBinaryForward, inputs = [Tensor_15225, Tensor_15321], outputs = [Tensor_15323], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_15323: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_15324: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_15325: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15326: operator = ElemwiseBinaryBackward, inputs = [Tensor_15323, nullptr, nullptr, nullptr], outputs = [Tensor_15327, Tensor_15328], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_15327: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_15328: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15329: operator = ElemwiseUnaryBackward, inputs = [Tensor_15327, nullptr, nullptr], outputs = [Tensor_15330], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 1]
Tensor_15330: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15331: operator = TypecastBackward, inputs = [Tensor_15328], outputs = [Tensor_15332], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_15332: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15333: operator = ElemwiseBinaryBackward, inputs = [Tensor_15332, Tensor_890, Tensor_892, nullptr], outputs = [Tensor_15334, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_MUL]
Tensor_15334: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15335: operator = ElemwiseUnaryBackward, inputs = [Tensor_15334, nullptr, nullptr], outputs = [Tensor_15336], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_DIV scal1_float: 0.986363649]
Tensor_15336: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15337: operator = ReshapeBackward, inputs = [Tensor_15336], outputs = [Tensor_15338], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 151296 stride: 768 stride: 1]
Tensor_15338: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15339: operator = LinearBackward, inputs = [Tensor_15338, Tensor_875, Tensor_877], outputs = [Tensor_15340, Tensor_15341, Tensor_15342], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_15340: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_15341: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_15342: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15343: operator = TypecastBackward, inputs = [Tensor_15340], outputs = [Tensor_15344], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15344: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15345: operator = LayernormBackward, inputs = [Tensor_15344, Tensor_869, Tensor_871, Tensor_76, Tensor_77, Tensor_872, Tensor_873], outputs = [Tensor_15346, Tensor_15347, Tensor_15348], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_15346: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_15347: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_15348: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15349: operator = TypecastBackward, inputs = [Tensor_15346], outputs = [Tensor_15350], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_15350: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15351: operator = TypecastBackward, inputs = [Tensor_15350], outputs = [Tensor_15352], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15352: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15353: operator = ElemwiseUnaryBackward, inputs = [Tensor_15352, Tensor_863, Tensor_865], outputs = [Tensor_15354], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_GELU]
Tensor_15354: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15355: operator = TypecastBackward, inputs = [Tensor_15354], outputs = [Tensor_15356], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_15356: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15357: operator = LinearBackward, inputs = [Tensor_15356, Tensor_855, Tensor_857], outputs = [Tensor_15358, Tensor_15359, Tensor_15360], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_15358: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_15359: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_15360: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15361: operator = TypecastBackward, inputs = [Tensor_15358], outputs = [Tensor_15362], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15362: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15363: operator = ReshapeBackward, inputs = [Tensor_15362], outputs = [Tensor_15364], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 25216 shape: 768 stride: 768 stride: 1]
Tensor_15364: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15365: operator = LayernormBackward, inputs = [Tensor_15364, Tensor_847, Tensor_849, Tensor_84, Tensor_85, Tensor_850, Tensor_851], outputs = [Tensor_15366, Tensor_15369, Tensor_15370], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_15366: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15367: operator = ElemwiseBinaryForward, inputs = [Tensor_15330, Tensor_15366], outputs = [Tensor_15368], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_15368: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_15369: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_15370: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15371: operator = ElemwiseBinaryBackward, inputs = [Tensor_15368, nullptr, nullptr, nullptr], outputs = [Tensor_15372, Tensor_15373], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_15372: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_15373: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15374: operator = ElemwiseUnaryBackward, inputs = [Tensor_15372, nullptr, nullptr], outputs = [Tensor_15375], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 1]
Tensor_15375: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15376: operator = TypecastBackward, inputs = [Tensor_15373], outputs = [Tensor_15377], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_15377: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15378: operator = ElemwiseBinaryBackward, inputs = [Tensor_15377, Tensor_837, Tensor_839, nullptr], outputs = [Tensor_15379, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_MUL]
Tensor_15379: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15380: operator = ElemwiseUnaryBackward, inputs = [Tensor_15379, nullptr, nullptr], outputs = [Tensor_15381], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_DIV scal1_float: 0.986363649]
Tensor_15381: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15382: operator = LinearBackward, inputs = [Tensor_15381, Tensor_820, Tensor_822], outputs = [Tensor_15383, Tensor_15384, Tensor_15385], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_15383: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_15384: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_15385: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15386: operator = TypecastBackward, inputs = [Tensor_15383], outputs = [Tensor_15387], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15387: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15388: operator = LayernormBackward, inputs = [Tensor_15387, Tensor_814, Tensor_816, Tensor_64, Tensor_65, Tensor_817, Tensor_818], outputs = [Tensor_15389, Tensor_15390, Tensor_15391], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_15389: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_15390: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_15391: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15392: operator = TypecastBackward, inputs = [Tensor_15389], outputs = [Tensor_15393], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_15393: shape = [128, 197, 768], stride = [768, 98304, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15394: operator = ReshapeBackward, inputs = [Tensor_15393], outputs = [Tensor_15395], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 768 stride: 98304 stride: 1]
Tensor_15395: shape = [197, 128, 768], stride = [98304, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15396: operator = ReshapeBackward, inputs = [Tensor_15395], outputs = [Tensor_15397], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 197 shape: 128 shape: 768 stride: 98304 stride: 768 stride: 1]
Tensor_15397: shape = [197, 1536, 64], stride = [98304, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15398: operator = ContiguousBackward, inputs = [Tensor_15397], outputs = [Tensor_15399], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_15399: shape = [197, 1536, 64], stride = [64, 12608, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15400: operator = ReshapeBackward, inputs = [Tensor_15399], outputs = [Tensor_15401], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 197 shape: 1536 shape: 64 stride: 64 stride: 12608 stride: 1]
Tensor_15401: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15402: operator = BatchedMatmulBackward, inputs = [Tensor_15401, Tensor_802, Tensor_780], outputs = [Tensor_15403, Tensor_15404], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_15403: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_15404: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15405: operator = TypecastBackward, inputs = [Tensor_15403], outputs = [Tensor_15406], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15406: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15407: operator = ReshapeBackward, inputs = [Tensor_15404], outputs = [Tensor_15408], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_15408: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15409: operator = SoftmaxBackward, inputs = [Tensor_15406, Tensor_800], outputs = [Tensor_15410], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dim: 2]
Tensor_15410: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15411: operator = ContiguousBackward, inputs = [Tensor_15408], outputs = [Tensor_15412], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_15412: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15413: operator = TypecastBackward, inputs = [Tensor_15410], outputs = [Tensor_15414], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_15414: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15415: operator = ReshapeBackward, inputs = [Tensor_15412], outputs = [Tensor_15416], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 64 stride: 151296 stride: 64 stride: 768 stride: 1]
Tensor_15416: shape = [128, 197, 12, 64], stride = [151296, 768, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15417: operator = ReshapeBackward, inputs = [Tensor_15414], outputs = [Tensor_15418], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 197 stride: 38809 stride: 197 stride: 1]
Tensor_15418: shape = [128, 12, 197, 197], stride = [465708, 38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15419: operator = ReshapeBackward, inputs = [Tensor_15416], outputs = [Tensor_15420], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 12 shape: 64 stride: 151296 stride: 768 stride: 64 stride: 1]
Tensor_15420: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15421: operator = MaskedFillBackward, inputs = [Tensor_15418, Tensor_792], outputs = [Tensor_15422], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [value_float: -inf]
Tensor_15422: shape = [128, 12, 197, 197], stride = [465708, 38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15423: operator = LinearBackward, inputs = [Tensor_15420, Tensor_748, Tensor_750], outputs = [Tensor_15424, Tensor_15425, Tensor_15426], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_15424: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_15425: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_15426: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15427: operator = ReshapeBackward, inputs = [Tensor_15422], outputs = [Tensor_15428], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 197 stride: 465708 stride: 38809 stride: 197 stride: 1]
Tensor_15428: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15429: operator = TypecastBackward, inputs = [Tensor_15424], outputs = [Tensor_15430], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15430: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15431: operator = BatchedMatmulBackward, inputs = [Tensor_15428, Tensor_772, Tensor_782], outputs = [Tensor_15432, Tensor_15433], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_15432: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_15433: shape = [1536, 64, 197], stride = [12608, 1, 64], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15434: operator = ReshapeBackward, inputs = [Tensor_15432], outputs = [Tensor_15435], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_15435: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15436: operator = ReshapeBackward, inputs = [Tensor_15433], outputs = [Tensor_15437], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 64 shape: 197 stride: 12608 stride: 1 stride: 64]
Tensor_15437: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15438: operator = ContiguousBackward, inputs = [Tensor_15435], outputs = [Tensor_15439], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_15439: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15440: operator = ReshapeBackward, inputs = [Tensor_15437], outputs = [Tensor_15441], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_15441: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15442: operator = ReshapeBackward, inputs = [Tensor_15439], outputs = [Tensor_15443], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 64 stride: 151296 stride: 64 stride: 768 stride: 1]
Tensor_15443: shape = [128, 197, 12, 64], stride = [151296, 768, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15444: operator = ContiguousBackward, inputs = [Tensor_15441], outputs = [Tensor_15445], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_15445: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15446: operator = ReshapeBackward, inputs = [Tensor_15443], outputs = [Tensor_15447], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 12 shape: 64 stride: 151296 stride: 768 stride: 64 stride: 1]
Tensor_15447: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15448: operator = ReshapeBackward, inputs = [Tensor_15445], outputs = [Tensor_15449], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 64 stride: 151296 stride: 64 stride: 768 stride: 1]
Tensor_15449: shape = [128, 197, 12, 64], stride = [151296, 768, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15450: operator = ElemwiseUnaryBackward, inputs = [Tensor_15447, nullptr, nullptr], outputs = [Tensor_15451], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 0.125]
Tensor_15451: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15452: operator = ReshapeBackward, inputs = [Tensor_15449], outputs = [Tensor_15453], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 12 shape: 64 stride: 151296 stride: 768 stride: 64 stride: 1]
Tensor_15453: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15454: operator = LinearBackward, inputs = [Tensor_15451, Tensor_732, Tensor_734], outputs = [Tensor_15455, Tensor_15456, Tensor_15457], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_15455: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_15456: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_15457: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15458: operator = LinearBackward, inputs = [Tensor_15453, Tensor_740, Tensor_742], outputs = [Tensor_15459, Tensor_15460, Tensor_15461], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_15459: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_15460: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_15461: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15462: operator = TypecastBackward, inputs = [Tensor_15455], outputs = [Tensor_15463], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15463: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15464: operator = ElemwiseBinaryForward, inputs = [Tensor_15430, Tensor_15463], outputs = [Tensor_15465], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_15465: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15466: operator = TypecastBackward, inputs = [Tensor_15459], outputs = [Tensor_15467], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15467: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15468: operator = ElemwiseBinaryForward, inputs = [Tensor_15465, Tensor_15467], outputs = [Tensor_15469], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_15469: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15470: operator = LayernormBackward, inputs = [Tensor_15469, Tensor_726, Tensor_728, Tensor_68, Tensor_69, Tensor_729, Tensor_730], outputs = [Tensor_15471, Tensor_15474, Tensor_15475], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_15471: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15472: operator = ElemwiseBinaryForward, inputs = [Tensor_15375, Tensor_15471], outputs = [Tensor_15473], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_15473: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_15474: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_15475: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15476: operator = ElemwiseBinaryBackward, inputs = [Tensor_15473, nullptr, nullptr, nullptr], outputs = [Tensor_15477, Tensor_15478], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_15477: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_15478: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15479: operator = ElemwiseUnaryBackward, inputs = [Tensor_15477, nullptr, nullptr], outputs = [Tensor_15480], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 1]
Tensor_15480: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15481: operator = TypecastBackward, inputs = [Tensor_15478], outputs = [Tensor_15482], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_15482: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15483: operator = ReshapeBackward, inputs = [Tensor_15482], outputs = [Tensor_15484], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 151296 stride: 768 stride: 1]
Tensor_15484: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15485: operator = LinearBackward, inputs = [Tensor_15484, Tensor_712, Tensor_714], outputs = [Tensor_15486, Tensor_15487, Tensor_15488], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_15486: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_15487: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_15488: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15489: operator = TypecastBackward, inputs = [Tensor_15486], outputs = [Tensor_15490], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15490: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15491: operator = LayernormBackward, inputs = [Tensor_15490, Tensor_706, Tensor_708, Tensor_36, Tensor_37, Tensor_709, Tensor_710], outputs = [Tensor_15492, Tensor_15493, Tensor_15494], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_15492: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_15493: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_15494: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15495: operator = TypecastBackward, inputs = [Tensor_15492], outputs = [Tensor_15496], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_15496: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15497: operator = TypecastBackward, inputs = [Tensor_15496], outputs = [Tensor_15498], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15498: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15499: operator = ElemwiseUnaryBackward, inputs = [Tensor_15498, Tensor_700, Tensor_702], outputs = [Tensor_15500], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_GELU]
Tensor_15500: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15501: operator = TypecastBackward, inputs = [Tensor_15500], outputs = [Tensor_15502], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_15502: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15503: operator = LinearBackward, inputs = [Tensor_15502, Tensor_692, Tensor_694], outputs = [Tensor_15504, Tensor_15505, Tensor_15506], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_15504: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_15505: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_15506: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15507: operator = TypecastBackward, inputs = [Tensor_15504], outputs = [Tensor_15508], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15508: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15509: operator = ReshapeBackward, inputs = [Tensor_15508], outputs = [Tensor_15510], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 25216 shape: 768 stride: 768 stride: 1]
Tensor_15510: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15511: operator = LayernormBackward, inputs = [Tensor_15510, Tensor_684, Tensor_686, Tensor_44, Tensor_45, Tensor_687, Tensor_688], outputs = [Tensor_15512, Tensor_15515, Tensor_15516], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_15512: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15513: operator = ElemwiseBinaryForward, inputs = [Tensor_15480, Tensor_15512], outputs = [Tensor_15514], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_15514: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_15515: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_15516: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15517: operator = ElemwiseBinaryBackward, inputs = [Tensor_15514, nullptr, nullptr, nullptr], outputs = [Tensor_15518, Tensor_15519], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_15518: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_15519: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15520: operator = ElemwiseUnaryBackward, inputs = [Tensor_15518, nullptr, nullptr], outputs = [Tensor_15521], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 1]
Tensor_15521: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15522: operator = TypecastBackward, inputs = [Tensor_15519], outputs = [Tensor_15523], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_15523: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15524: operator = LinearBackward, inputs = [Tensor_15523, Tensor_668, Tensor_670], outputs = [Tensor_15525, Tensor_15526, Tensor_15527], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_15525: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_15526: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_15527: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15528: operator = TypecastBackward, inputs = [Tensor_15525], outputs = [Tensor_15529], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15529: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15530: operator = LayernormBackward, inputs = [Tensor_15529, Tensor_662, Tensor_664, Tensor_24, Tensor_25, Tensor_665, Tensor_666], outputs = [Tensor_15531, Tensor_15532, Tensor_15533], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_15531: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_15532: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_15533: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15534: operator = TypecastBackward, inputs = [Tensor_15531], outputs = [Tensor_15535], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_15535: shape = [128, 197, 768], stride = [768, 98304, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15536: operator = ReshapeBackward, inputs = [Tensor_15535], outputs = [Tensor_15537], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 768 stride: 98304 stride: 1]
Tensor_15537: shape = [197, 128, 768], stride = [98304, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15538: operator = ReshapeBackward, inputs = [Tensor_15537], outputs = [Tensor_15539], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 197 shape: 128 shape: 768 stride: 98304 stride: 768 stride: 1]
Tensor_15539: shape = [197, 1536, 64], stride = [98304, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15540: operator = ContiguousBackward, inputs = [Tensor_15539], outputs = [Tensor_15541], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_15541: shape = [197, 1536, 64], stride = [64, 12608, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15542: operator = ReshapeBackward, inputs = [Tensor_15541], outputs = [Tensor_15543], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 197 shape: 1536 shape: 64 stride: 64 stride: 12608 stride: 1]
Tensor_15543: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15544: operator = BatchedMatmulBackward, inputs = [Tensor_15543, Tensor_650, Tensor_628], outputs = [Tensor_15545, Tensor_15546], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_15545: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_15546: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15547: operator = TypecastBackward, inputs = [Tensor_15545], outputs = [Tensor_15548], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15548: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15549: operator = ReshapeBackward, inputs = [Tensor_15546], outputs = [Tensor_15550], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_15550: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15551: operator = SoftmaxBackward, inputs = [Tensor_15548, Tensor_648], outputs = [Tensor_15552], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dim: 2]
Tensor_15552: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15553: operator = ContiguousBackward, inputs = [Tensor_15550], outputs = [Tensor_15554], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_15554: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15555: operator = TypecastBackward, inputs = [Tensor_15552], outputs = [Tensor_15556], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_15556: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15557: operator = ReshapeBackward, inputs = [Tensor_15554], outputs = [Tensor_15558], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 64 stride: 151296 stride: 64 stride: 768 stride: 1]
Tensor_15558: shape = [128, 197, 12, 64], stride = [151296, 768, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15559: operator = ReshapeBackward, inputs = [Tensor_15556], outputs = [Tensor_15560], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 197 stride: 38809 stride: 197 stride: 1]
Tensor_15560: shape = [128, 12, 197, 197], stride = [465708, 38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15561: operator = ReshapeBackward, inputs = [Tensor_15558], outputs = [Tensor_15562], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 12 shape: 64 stride: 151296 stride: 768 stride: 64 stride: 1]
Tensor_15562: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15563: operator = MaskedFillBackward, inputs = [Tensor_15560, Tensor_640], outputs = [Tensor_15564], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [value_float: -inf]
Tensor_15564: shape = [128, 12, 197, 197], stride = [465708, 38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15565: operator = LinearBackward, inputs = [Tensor_15562, Tensor_596, Tensor_598], outputs = [Tensor_15566, Tensor_15567, Tensor_15568], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_15566: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_15567: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_15568: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15569: operator = ReshapeBackward, inputs = [Tensor_15564], outputs = [Tensor_15570], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 197 stride: 465708 stride: 38809 stride: 197 stride: 1]
Tensor_15570: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15571: operator = TypecastBackward, inputs = [Tensor_15566], outputs = [Tensor_15572], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15572: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15573: operator = BatchedMatmulBackward, inputs = [Tensor_15570, Tensor_620, Tensor_630], outputs = [Tensor_15574, Tensor_15575], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_15574: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_15575: shape = [1536, 64, 197], stride = [12608, 1, 64], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15576: operator = ReshapeBackward, inputs = [Tensor_15574], outputs = [Tensor_15577], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_15577: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15578: operator = ReshapeBackward, inputs = [Tensor_15575], outputs = [Tensor_15579], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 64 shape: 197 stride: 12608 stride: 1 stride: 64]
Tensor_15579: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15580: operator = ContiguousBackward, inputs = [Tensor_15577], outputs = [Tensor_15581], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_15581: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15582: operator = ReshapeBackward, inputs = [Tensor_15579], outputs = [Tensor_15583], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_15583: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15584: operator = ReshapeBackward, inputs = [Tensor_15581], outputs = [Tensor_15585], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 64 stride: 151296 stride: 64 stride: 768 stride: 1]
Tensor_15585: shape = [128, 197, 12, 64], stride = [151296, 768, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15586: operator = ContiguousBackward, inputs = [Tensor_15583], outputs = [Tensor_15587], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_15587: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15588: operator = ReshapeBackward, inputs = [Tensor_15585], outputs = [Tensor_15589], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 12 shape: 64 stride: 151296 stride: 768 stride: 64 stride: 1]
Tensor_15589: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15590: operator = ReshapeBackward, inputs = [Tensor_15587], outputs = [Tensor_15591], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 64 stride: 151296 stride: 64 stride: 768 stride: 1]
Tensor_15591: shape = [128, 197, 12, 64], stride = [151296, 768, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15592: operator = ElemwiseUnaryBackward, inputs = [Tensor_15589, nullptr, nullptr], outputs = [Tensor_15593], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 0.125]
Tensor_15593: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15594: operator = ReshapeBackward, inputs = [Tensor_15591], outputs = [Tensor_15595], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 12 shape: 64 stride: 151296 stride: 768 stride: 64 stride: 1]
Tensor_15595: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15596: operator = LinearBackward, inputs = [Tensor_15593, Tensor_580, Tensor_582], outputs = [Tensor_15597, Tensor_15598, Tensor_15599], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_15597: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_15598: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_15599: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15600: operator = LinearBackward, inputs = [Tensor_15595, Tensor_588, Tensor_590], outputs = [Tensor_15601, Tensor_15602, Tensor_15603], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_15601: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_15602: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_15603: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15604: operator = TypecastBackward, inputs = [Tensor_15597], outputs = [Tensor_15605], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15605: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15606: operator = ElemwiseBinaryForward, inputs = [Tensor_15572, Tensor_15605], outputs = [Tensor_15607], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_15607: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15608: operator = TypecastBackward, inputs = [Tensor_15601], outputs = [Tensor_15609], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15609: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15610: operator = ElemwiseBinaryForward, inputs = [Tensor_15607, Tensor_15609], outputs = [Tensor_15611], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_15611: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15612: operator = LayernormBackward, inputs = [Tensor_15611, Tensor_574, Tensor_576, Tensor_28, Tensor_29, Tensor_577, Tensor_578], outputs = [Tensor_15613, Tensor_15616, Tensor_15617], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_15613: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15614: operator = ElemwiseBinaryForward, inputs = [Tensor_15521, Tensor_15613], outputs = [Tensor_15615], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_15615: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_15616: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_15617: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15618: operator = ElemwiseBinaryBackward, inputs = [Tensor_15615, Tensor_564, Tensor_572, nullptr], outputs = [Tensor_15619, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_MUL]
Tensor_15619: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15620: operator = ElemwiseBinaryBackward, inputs = [Tensor_15619, nullptr, nullptr, nullptr], outputs = [Tensor_15621, Tensor_15622], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_15621: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_15622: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15623: operator = SumForward, inputs = [Tensor_15622], outputs = [Tensor_15624], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dims: 0]
Tensor_15624: shape = [197, 768], stride = [768, 1], offset = 0, size = 151296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15625: operator = ReshapeForward, inputs = [Tensor_15624], outputs = [Tensor_15626], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 0 stride: 768 stride: 1]
Tensor_15626: shape = [128, 197, 768], stride = [0, 768, 1], offset = 0, size = 151296, datatype = float32, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15627: operator = ElemwiseUnaryBackward, inputs = [Tensor_15621, nullptr, nullptr], outputs = [Tensor_15628], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 1]
Tensor_15628: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15629: operator = ReshapeBackward, inputs = [Tensor_15626], outputs = [Tensor_15630], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 0 stride: 768 stride: 1]
Tensor_15630: shape = [1, 197, 768], stride = [151296, 768, 1], offset = 0, size = 151296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15631: operator = CatBackward, inputs = [Tensor_15628, Tensor_543, Tensor_545], outputs = [Tensor_15632, Tensor_15637], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dim: 1]
Tensor_15632: shape = [128, 1, 768], stride = [768, 768, 1], offset = 0, size = 98304, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15633: operator = SumForward, inputs = [Tensor_15632], outputs = [Tensor_15634], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dims: 0]
Tensor_15634: shape = [1, 768], stride = [768, 1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15635: operator = ReshapeForward, inputs = [Tensor_15634], outputs = [Tensor_15636], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 1 shape: 768 stride: 0 stride: 768 stride: 1]
Tensor_15636: shape = [128, 1, 768], stride = [0, 768, 1], offset = 0, size = 768, datatype = float32, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_15637: shape = [128, 196, 768], stride = [150528, 768, 1], offset = 0, size = 19267584, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15638: operator = EmbeddingBackward, inputs = [Tensor_15630, Tensor_558], outputs = [Tensor_15639], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [norm_type: 2 padding_idx: -1]
Tensor_15639: shape = [199, 768], stride = [768, 1], offset = 0, size = 152832, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15640: operator = ReshapeBackward, inputs = [Tensor_15636], outputs = [Tensor_15641], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 1 shape: 768 stride: 0 stride: 768 stride: 1]
Tensor_15641: shape = [1, 1, 768], stride = [768, 768, 1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15642: operator = TypecastBackward, inputs = [Tensor_15637], outputs = [Tensor_15643], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_15643: shape = [128, 196, 768], stride = [150528, 1, 196], offset = 0, size = 19267584, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15644: operator = ReshapeBackward, inputs = [Tensor_15643], outputs = [Tensor_15645], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 196 shape: 768 stride: 150528 stride: 1 stride: 196]
Tensor_15645: shape = [128, 768, 196], stride = [150528, 196, 1], offset = 0, size = 19267584, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15646: operator = ReshapeBackward, inputs = [Tensor_15645], outputs = [Tensor_15647], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 768 shape: 196 stride: 150528 stride: 196 stride: 1]
Tensor_15647: shape = [128, 768, 14, 14], stride = [150528, 196, 14, 1], offset = 0, size = 19267584, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15648: operator = Convolution2dBackward, inputs = [Tensor_15647, Tensor_531, Tensor_533], outputs = [nullptr, Tensor_15649, Tensor_15650], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [stride_h: 16 stride_w: 16 dilation_h: 1 dilation_w: 1 groups: 1]
Tensor_15649: shape = [768, 3, 16, 16], stride = [768, 256, 16, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Tensor_15650: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15651: operator = TypecastBackward, inputs = [Tensor_13814], outputs = [Tensor_15652], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15652: shape = [1000, 768], stride = [768, 1], offset = 0, size = 768000, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15653: operator = TypecastBackward, inputs = [Tensor_13815], outputs = [Tensor_15654], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15654: shape = [1000], stride = [1], offset = 0, size = 1000, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15655: operator = TypecastBackward, inputs = [Tensor_13841], outputs = [Tensor_15656], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15656: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15657: operator = TypecastBackward, inputs = [Tensor_13842], outputs = [Tensor_15658], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15658: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15659: operator = TypecastBackward, inputs = [Tensor_13859], outputs = [Tensor_15660], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15660: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15661: operator = TypecastBackward, inputs = [Tensor_13860], outputs = [Tensor_15662], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15662: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15663: operator = TypecastBackward, inputs = [Tensor_13884], outputs = [Tensor_15664], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15664: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15665: operator = TypecastBackward, inputs = [Tensor_13885], outputs = [Tensor_15666], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15666: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15667: operator = TypecastBackward, inputs = [Tensor_13925], outputs = [Tensor_15668], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15668: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15669: operator = TypecastBackward, inputs = [Tensor_13926], outputs = [Tensor_15670], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15670: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15671: operator = TypecastBackward, inputs = [Tensor_13956], outputs = [Tensor_15672], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15672: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15673: operator = TypecastBackward, inputs = [Tensor_13957], outputs = [Tensor_15674], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15674: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15675: operator = TypecastBackward, inputs = [Tensor_13960], outputs = [Tensor_15676], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15676: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15677: operator = TypecastBackward, inputs = [Tensor_13961], outputs = [Tensor_15678], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15678: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15679: operator = TypecastBackward, inputs = [Tensor_13991], outputs = [Tensor_15680], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15680: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15681: operator = TypecastBackward, inputs = [Tensor_13992], outputs = [Tensor_15682], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15682: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15683: operator = TypecastBackward, inputs = [Tensor_14009], outputs = [Tensor_15684], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15684: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15685: operator = TypecastBackward, inputs = [Tensor_14010], outputs = [Tensor_15686], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15686: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15687: operator = TypecastBackward, inputs = [Tensor_14034], outputs = [Tensor_15688], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15688: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15689: operator = TypecastBackward, inputs = [Tensor_14035], outputs = [Tensor_15690], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15690: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15691: operator = TypecastBackward, inputs = [Tensor_14075], outputs = [Tensor_15692], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15692: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15693: operator = TypecastBackward, inputs = [Tensor_14076], outputs = [Tensor_15694], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15694: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15695: operator = TypecastBackward, inputs = [Tensor_14106], outputs = [Tensor_15696], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15696: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15697: operator = TypecastBackward, inputs = [Tensor_14107], outputs = [Tensor_15698], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15698: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15699: operator = TypecastBackward, inputs = [Tensor_14110], outputs = [Tensor_15700], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15700: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15701: operator = TypecastBackward, inputs = [Tensor_14111], outputs = [Tensor_15702], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15702: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15703: operator = TypecastBackward, inputs = [Tensor_14141], outputs = [Tensor_15704], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15704: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15705: operator = TypecastBackward, inputs = [Tensor_14142], outputs = [Tensor_15706], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15706: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15707: operator = TypecastBackward, inputs = [Tensor_14159], outputs = [Tensor_15708], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15708: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15709: operator = TypecastBackward, inputs = [Tensor_14160], outputs = [Tensor_15710], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15710: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15711: operator = TypecastBackward, inputs = [Tensor_14184], outputs = [Tensor_15712], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15712: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15713: operator = TypecastBackward, inputs = [Tensor_14185], outputs = [Tensor_15714], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15714: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15715: operator = TypecastBackward, inputs = [Tensor_14225], outputs = [Tensor_15716], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15716: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15717: operator = TypecastBackward, inputs = [Tensor_14226], outputs = [Tensor_15718], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15718: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15719: operator = TypecastBackward, inputs = [Tensor_14256], outputs = [Tensor_15720], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15720: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15721: operator = TypecastBackward, inputs = [Tensor_14257], outputs = [Tensor_15722], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15722: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15723: operator = TypecastBackward, inputs = [Tensor_14260], outputs = [Tensor_15724], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15724: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15725: operator = TypecastBackward, inputs = [Tensor_14261], outputs = [Tensor_15726], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15726: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15727: operator = TypecastBackward, inputs = [Tensor_14291], outputs = [Tensor_15728], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15728: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15729: operator = TypecastBackward, inputs = [Tensor_14292], outputs = [Tensor_15730], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15730: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15731: operator = TypecastBackward, inputs = [Tensor_14309], outputs = [Tensor_15732], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15732: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15733: operator = TypecastBackward, inputs = [Tensor_14310], outputs = [Tensor_15734], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15734: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15735: operator = TypecastBackward, inputs = [Tensor_14334], outputs = [Tensor_15736], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15736: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15737: operator = TypecastBackward, inputs = [Tensor_14335], outputs = [Tensor_15738], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15738: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15739: operator = TypecastBackward, inputs = [Tensor_14375], outputs = [Tensor_15740], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15740: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15741: operator = TypecastBackward, inputs = [Tensor_14376], outputs = [Tensor_15742], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15742: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15743: operator = TypecastBackward, inputs = [Tensor_14406], outputs = [Tensor_15744], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15744: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15745: operator = TypecastBackward, inputs = [Tensor_14407], outputs = [Tensor_15746], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15746: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15747: operator = TypecastBackward, inputs = [Tensor_14410], outputs = [Tensor_15748], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15748: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15749: operator = TypecastBackward, inputs = [Tensor_14411], outputs = [Tensor_15750], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15750: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15751: operator = TypecastBackward, inputs = [Tensor_14441], outputs = [Tensor_15752], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15752: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15753: operator = TypecastBackward, inputs = [Tensor_14442], outputs = [Tensor_15754], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15754: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15755: operator = TypecastBackward, inputs = [Tensor_14459], outputs = [Tensor_15756], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15756: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15757: operator = TypecastBackward, inputs = [Tensor_14460], outputs = [Tensor_15758], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15758: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15759: operator = TypecastBackward, inputs = [Tensor_14484], outputs = [Tensor_15760], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15760: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15761: operator = TypecastBackward, inputs = [Tensor_14485], outputs = [Tensor_15762], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15762: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15763: operator = TypecastBackward, inputs = [Tensor_14525], outputs = [Tensor_15764], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15764: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15765: operator = TypecastBackward, inputs = [Tensor_14526], outputs = [Tensor_15766], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15766: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15767: operator = TypecastBackward, inputs = [Tensor_14556], outputs = [Tensor_15768], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15768: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15769: operator = TypecastBackward, inputs = [Tensor_14557], outputs = [Tensor_15770], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15770: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15771: operator = TypecastBackward, inputs = [Tensor_14560], outputs = [Tensor_15772], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15772: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15773: operator = TypecastBackward, inputs = [Tensor_14561], outputs = [Tensor_15774], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15774: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15775: operator = TypecastBackward, inputs = [Tensor_14591], outputs = [Tensor_15776], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15776: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15777: operator = TypecastBackward, inputs = [Tensor_14592], outputs = [Tensor_15778], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15778: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15779: operator = TypecastBackward, inputs = [Tensor_14609], outputs = [Tensor_15780], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15780: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15781: operator = TypecastBackward, inputs = [Tensor_14610], outputs = [Tensor_15782], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15782: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15783: operator = TypecastBackward, inputs = [Tensor_14634], outputs = [Tensor_15784], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15784: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15785: operator = TypecastBackward, inputs = [Tensor_14635], outputs = [Tensor_15786], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15786: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15787: operator = TypecastBackward, inputs = [Tensor_14675], outputs = [Tensor_15788], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15788: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15789: operator = TypecastBackward, inputs = [Tensor_14676], outputs = [Tensor_15790], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15790: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15791: operator = TypecastBackward, inputs = [Tensor_14706], outputs = [Tensor_15792], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15792: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15793: operator = TypecastBackward, inputs = [Tensor_14707], outputs = [Tensor_15794], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15794: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15795: operator = TypecastBackward, inputs = [Tensor_14710], outputs = [Tensor_15796], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15796: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15797: operator = TypecastBackward, inputs = [Tensor_14711], outputs = [Tensor_15798], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15798: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15799: operator = TypecastBackward, inputs = [Tensor_14741], outputs = [Tensor_15800], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15800: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15801: operator = TypecastBackward, inputs = [Tensor_14742], outputs = [Tensor_15802], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15802: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15803: operator = TypecastBackward, inputs = [Tensor_14759], outputs = [Tensor_15804], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15804: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15805: operator = TypecastBackward, inputs = [Tensor_14760], outputs = [Tensor_15806], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15806: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15807: operator = TypecastBackward, inputs = [Tensor_14784], outputs = [Tensor_15808], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15808: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15809: operator = TypecastBackward, inputs = [Tensor_14785], outputs = [Tensor_15810], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15810: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15811: operator = TypecastBackward, inputs = [Tensor_14825], outputs = [Tensor_15812], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15812: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15813: operator = TypecastBackward, inputs = [Tensor_14826], outputs = [Tensor_15814], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15814: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15815: operator = TypecastBackward, inputs = [Tensor_14856], outputs = [Tensor_15816], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15816: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15817: operator = TypecastBackward, inputs = [Tensor_14857], outputs = [Tensor_15818], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15818: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15819: operator = TypecastBackward, inputs = [Tensor_14860], outputs = [Tensor_15820], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15820: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15821: operator = TypecastBackward, inputs = [Tensor_14861], outputs = [Tensor_15822], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15822: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15823: operator = TypecastBackward, inputs = [Tensor_14891], outputs = [Tensor_15824], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15824: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15825: operator = TypecastBackward, inputs = [Tensor_14892], outputs = [Tensor_15826], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15826: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15827: operator = TypecastBackward, inputs = [Tensor_14909], outputs = [Tensor_15828], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15828: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15829: operator = TypecastBackward, inputs = [Tensor_14910], outputs = [Tensor_15830], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15830: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15831: operator = TypecastBackward, inputs = [Tensor_14934], outputs = [Tensor_15832], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15832: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15833: operator = TypecastBackward, inputs = [Tensor_14935], outputs = [Tensor_15834], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15834: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15835: operator = TypecastBackward, inputs = [Tensor_14975], outputs = [Tensor_15836], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15836: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15837: operator = TypecastBackward, inputs = [Tensor_14976], outputs = [Tensor_15838], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15838: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15839: operator = TypecastBackward, inputs = [Tensor_15006], outputs = [Tensor_15840], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15840: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15841: operator = TypecastBackward, inputs = [Tensor_15007], outputs = [Tensor_15842], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15842: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15843: operator = TypecastBackward, inputs = [Tensor_15010], outputs = [Tensor_15844], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15844: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15845: operator = TypecastBackward, inputs = [Tensor_15011], outputs = [Tensor_15846], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15846: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15847: operator = TypecastBackward, inputs = [Tensor_15041], outputs = [Tensor_15848], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15848: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15849: operator = TypecastBackward, inputs = [Tensor_15042], outputs = [Tensor_15850], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15850: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15851: operator = TypecastBackward, inputs = [Tensor_15059], outputs = [Tensor_15852], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15852: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15853: operator = TypecastBackward, inputs = [Tensor_15060], outputs = [Tensor_15854], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15854: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15855: operator = TypecastBackward, inputs = [Tensor_15084], outputs = [Tensor_15856], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15856: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15857: operator = TypecastBackward, inputs = [Tensor_15085], outputs = [Tensor_15858], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15858: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15859: operator = TypecastBackward, inputs = [Tensor_15125], outputs = [Tensor_15860], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15860: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15861: operator = TypecastBackward, inputs = [Tensor_15126], outputs = [Tensor_15862], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15862: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15863: operator = TypecastBackward, inputs = [Tensor_15156], outputs = [Tensor_15864], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15864: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15865: operator = TypecastBackward, inputs = [Tensor_15157], outputs = [Tensor_15866], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15866: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15867: operator = TypecastBackward, inputs = [Tensor_15160], outputs = [Tensor_15868], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15868: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15869: operator = TypecastBackward, inputs = [Tensor_15161], outputs = [Tensor_15870], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15870: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15871: operator = TypecastBackward, inputs = [Tensor_15191], outputs = [Tensor_15872], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15872: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15873: operator = TypecastBackward, inputs = [Tensor_15192], outputs = [Tensor_15874], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15874: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15875: operator = TypecastBackward, inputs = [Tensor_15209], outputs = [Tensor_15876], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15876: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15877: operator = TypecastBackward, inputs = [Tensor_15210], outputs = [Tensor_15878], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15878: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15879: operator = TypecastBackward, inputs = [Tensor_15234], outputs = [Tensor_15880], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15880: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15881: operator = TypecastBackward, inputs = [Tensor_15235], outputs = [Tensor_15882], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15882: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15883: operator = TypecastBackward, inputs = [Tensor_15275], outputs = [Tensor_15884], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15884: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15885: operator = TypecastBackward, inputs = [Tensor_15276], outputs = [Tensor_15886], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15886: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15887: operator = TypecastBackward, inputs = [Tensor_15306], outputs = [Tensor_15888], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15888: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15889: operator = TypecastBackward, inputs = [Tensor_15307], outputs = [Tensor_15890], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15890: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15891: operator = TypecastBackward, inputs = [Tensor_15310], outputs = [Tensor_15892], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15892: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15893: operator = TypecastBackward, inputs = [Tensor_15311], outputs = [Tensor_15894], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15894: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15895: operator = TypecastBackward, inputs = [Tensor_15341], outputs = [Tensor_15896], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15896: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15897: operator = TypecastBackward, inputs = [Tensor_15342], outputs = [Tensor_15898], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15898: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15899: operator = TypecastBackward, inputs = [Tensor_15359], outputs = [Tensor_15900], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15900: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15901: operator = TypecastBackward, inputs = [Tensor_15360], outputs = [Tensor_15902], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15902: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15903: operator = TypecastBackward, inputs = [Tensor_15384], outputs = [Tensor_15904], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15904: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15905: operator = TypecastBackward, inputs = [Tensor_15385], outputs = [Tensor_15906], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15906: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15907: operator = TypecastBackward, inputs = [Tensor_15425], outputs = [Tensor_15908], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15908: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15909: operator = TypecastBackward, inputs = [Tensor_15426], outputs = [Tensor_15910], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15910: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15911: operator = TypecastBackward, inputs = [Tensor_15456], outputs = [Tensor_15912], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15912: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15913: operator = TypecastBackward, inputs = [Tensor_15457], outputs = [Tensor_15914], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15914: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15915: operator = TypecastBackward, inputs = [Tensor_15460], outputs = [Tensor_15916], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15916: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15917: operator = TypecastBackward, inputs = [Tensor_15461], outputs = [Tensor_15918], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15918: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15919: operator = TypecastBackward, inputs = [Tensor_15487], outputs = [Tensor_15920], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15920: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15921: operator = TypecastBackward, inputs = [Tensor_15488], outputs = [Tensor_15922], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15922: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15923: operator = TypecastBackward, inputs = [Tensor_15505], outputs = [Tensor_15924], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15924: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15925: operator = TypecastBackward, inputs = [Tensor_15506], outputs = [Tensor_15926], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15926: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15927: operator = TypecastBackward, inputs = [Tensor_15526], outputs = [Tensor_15928], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15928: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15929: operator = TypecastBackward, inputs = [Tensor_15527], outputs = [Tensor_15930], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15930: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15931: operator = TypecastBackward, inputs = [Tensor_15567], outputs = [Tensor_15932], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15932: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15933: operator = TypecastBackward, inputs = [Tensor_15568], outputs = [Tensor_15934], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15934: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15935: operator = TypecastBackward, inputs = [Tensor_15598], outputs = [Tensor_15936], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15936: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15937: operator = TypecastBackward, inputs = [Tensor_15599], outputs = [Tensor_15938], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15938: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15939: operator = TypecastBackward, inputs = [Tensor_15602], outputs = [Tensor_15940], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15940: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15941: operator = TypecastBackward, inputs = [Tensor_15603], outputs = [Tensor_15942], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15942: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15943: operator = TypecastBackward, inputs = [Tensor_15649], outputs = [Tensor_15944], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15944: shape = [768, 3, 16, 16], stride = [768, 256, 16, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_15945: operator = TypecastBackward, inputs = [Tensor_15650], outputs = [Tensor_15946], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_15946: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)

Target (result) graph:
Tensor_27633: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11542: 3072 Bnn)
Tensor_27634: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11543: 3072 Bnn)
Tensor_27635: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11544: 3072 Bnn)
Tensor_27636: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11545: 3072 Bnn)
Tensor_27637: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11550: 12288 Bnn)
Tensor_27638: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11551: 12288 Bnn)
Tensor_27639: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11552: 3072 Bnn)
Tensor_27640: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11553: 3072 Bnn)
Tensor_27641: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11562: 3072 Bnn)
Tensor_27642: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11563: 3072 Bnn)
Tensor_27643: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11564: 3072 Bnn)
Tensor_27644: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11565: 3072 Bnn)
Tensor_27645: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11570: 12288 Bnn)
Tensor_27646: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11571: 12288 Bnn)
Tensor_27647: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11572: 3072 Bnn)
Tensor_27648: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11573: 3072 Bnn)
Tensor_27649: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11582: 3072 Bnn)
Tensor_27650: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11583: 3072 Bnn)
Tensor_27651: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11584: 3072 Bnn)
Tensor_27652: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11585: 3072 Bnn)
Tensor_27653: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11590: 12288 Bnn)
Tensor_27654: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11591: 12288 Bnn)
Tensor_27655: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11592: 3072 Bnn)
Tensor_27656: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11593: 3072 Bnn)
Tensor_27657: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11602: 3072 Bnn)
Tensor_27658: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11603: 3072 Bnn)
Tensor_27659: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11604: 3072 Bnn)
Tensor_27660: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11605: 3072 Bnn)
Tensor_27661: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11610: 12288 Bnn)
Tensor_27662: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11611: 12288 Bnn)
Tensor_27663: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11612: 3072 Bnn)
Tensor_27664: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11613: 3072 Bnn)
Tensor_27665: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11622: 3072 Bnn)
Tensor_27666: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11623: 3072 Bnn)
Tensor_27667: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11624: 3072 Bnn)
Tensor_27668: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11625: 3072 Bnn)
Tensor_27669: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11630: 12288 Bnn)
Tensor_27670: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11631: 12288 Bnn)
Tensor_27671: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11632: 3072 Bnn)
Tensor_27672: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11633: 3072 Bnn)
Tensor_27673: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11642: 3072 Bnn)
Tensor_27674: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11643: 3072 Bnn)
Tensor_27675: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11644: 3072 Bnn)
Tensor_27676: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11645: 3072 Bnn)
Tensor_27677: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11650: 12288 Bnn)
Tensor_27678: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11651: 12288 Bnn)
Tensor_27679: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11652: 3072 Bnn)
Tensor_27680: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11653: 3072 Bnn)
Tensor_27681: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11662: 3072 Bnn)
Tensor_27682: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11663: 3072 Bnn)
Tensor_27683: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11664: 3072 Bnn)
Tensor_27684: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11665: 3072 Bnn)
Tensor_27685: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11670: 12288 Bnn)
Tensor_27686: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11671: 12288 Bnn)
Tensor_27687: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11672: 3072 Bnn)
Tensor_27688: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11673: 3072 Bnn)
Tensor_27689: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11682: 3072 Bnn)
Tensor_27690: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11683: 3072 Bnn)
Tensor_27691: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11684: 3072 Bnn)
Tensor_27692: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11685: 3072 Bnn)
Tensor_27693: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11690: 12288 Bnn)
Tensor_27694: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11691: 12288 Bnn)
Tensor_27695: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11692: 3072 Bnn)
Tensor_27696: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11693: 3072 Bnn)
Tensor_27697: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11702: 3072 Bnn)
Tensor_27698: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11703: 3072 Bnn)
Tensor_27699: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11704: 3072 Bnn)
Tensor_27700: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11705: 3072 Bnn)
Tensor_27701: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11710: 12288 Bnn)
Tensor_27702: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11711: 12288 Bnn)
Tensor_27703: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11712: 3072 Bnn)
Tensor_27704: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11713: 3072 Bnn)
Tensor_27705: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11722: 3072 Bnn)
Tensor_27706: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11723: 3072 Bnn)
Tensor_27707: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11724: 3072 Bnn)
Tensor_27708: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11725: 3072 Bnn)
Tensor_27709: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11730: 12288 Bnn)
Tensor_27710: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11731: 12288 Bnn)
Tensor_27711: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11732: 3072 Bnn)
Tensor_27712: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11733: 3072 Bnn)
Tensor_27713: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11742: 3072 Bnn)
Tensor_27714: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11743: 3072 Bnn)
Tensor_27715: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11744: 3072 Bnn)
Tensor_27716: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11745: 3072 Bnn)
Tensor_27717: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11750: 12288 Bnn)
Tensor_27718: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11751: 12288 Bnn)
Tensor_27719: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11752: 3072 Bnn)
Tensor_27720: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11753: 3072 Bnn)
Tensor_27721: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11762: 3072 Bnn)
Tensor_27722: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11763: 3072 Bnn)
Tensor_27723: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11764: 3072 Bnn)
Tensor_27724: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11765: 3072 Bnn)
Tensor_27725: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11770: 12288 Bnn)
Tensor_27726: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11771: 12288 Bnn)
Tensor_27727: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11772: 3072 Bnn)
Tensor_27728: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11773: 3072 Bnn)
Tensor_27729: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11774: 3072 Bnn)
Tensor_27730: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11775: 3072 Bnn)
Tensor_27731: shape = [128, 3, 224, 224], stride = [150528, 50176, 224, 1], offset = 0, size = 19267584, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11817: 38535168 Bnn)
Tensor_27732: shape = [768, 3, 16, 16], stride = [768, 256, 16, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11819: 1179648 Bnr)
Tensor_27733: shape = [128, 1, 768], stride = [0, 768, 1], offset = 0, size = 768, datatype = float32, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11827: 3072 Bnn)
Tensor_27734: shape = [128, 196, 768], stride = [150528, 768, 1], offset = 0, size = 19267584, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11831: 38535168 Bnr)
Tensor_27735: shape = [1, 197], stride = [197, 1], offset = 0, size = 197, datatype = int64, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11844: 1576 Bnr)
Tensor_27736: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11840: 38731776 Bnr)
Tensor_27737: shape = [128, 197, 768], stride = [197, 1, 0], offset = 0, size = 25216, datatype = float32, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11836: 50432 Bnr)
Tensor_27738: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11860: 38731776 Bnr)
Tensor_27739: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11862: 38731776 Bnr)
Tensor_27740: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11863: 50432 Bnr)
Tensor_27741: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11864: 50432 Bnr)
Tensor_27742: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11866: 19365888 Bnr)
Tensor_27743: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11868: 1179648 Bnn)
Tensor_27744: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11874: 19365888 Bnr)
Tensor_27745: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11876: 1179648 Bnr)
Tensor_27746: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11882: 19365888 Bnn)
Tensor_27747: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11884: 1179648 Bnn)
Tensor_27748: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11896: 19365888 Bnr)
Tensor_27749: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11898: 19365888 Bnr)
Tensor_27750: shape = [1536, 64, 197], stride = [12608, 1, 64], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11890: 19365888 Bnn)
Tensor_27751: shape = [128, 12, 197, 197], stride = [197, 0, 0, 1], offset = 0, size = 25216, datatype = boolean, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11852: 12608 Bnr)
Tensor_27752: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11922: 119221248 Bnr)
Tensor_27753: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11914: 59610624 Bnr)
Tensor_27754: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11942: 77463552 Bnn)
Tensor_27755: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11940: 38731776 Bnr)
Tensor_27756: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11947: 50432 Bnr)
Tensor_27757: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11948: 50432 Bnr)
Tensor_27758: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11902: 19365888 Bnn)
Tensor_27759: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11951: 1179648 Bnr)
Tensor_27760: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11959: 38731776 Bnn)
Tensor_27761: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11936: 38731776 Bnn)
Tensor_27762: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11962: 50432 Bnr)
Tensor_27763: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11963: 50432 Bnr)
Tensor_27764: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11954: 19365888 Bnr)
Tensor_27765: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11968: 4718592 Bnr)
Tensor_27766: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11973: 154927104 Bnr)
Tensor_27767: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11975: 154927104 Bnr)
Tensor_27768: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11979: 154927104 Bnr)
Tensor_27769: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11981: 154927104 Bnr)
Tensor_27770: shape = [25216], stride = [1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11982: 50432 Bnr)
Tensor_27771: shape = [25216], stride = [1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11983: 50432 Bnr)
Tensor_27772: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11971: 77463552 Bnr)
Tensor_27773: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11987: 4718592 Bnn)
Tensor_27774: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11999: 38731776 Bnr)
Tensor_27775: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11997: 38731776 Bnr)
Tensor_27776: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12002: 50432 Bnr)
Tensor_27777: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12003: 50432 Bnr)
Tensor_27778: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11995: 19365888 Bnr)
Tensor_27779: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12007: 1179648 Bnn)
Tensor_27780: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12013: 19365888 Bnn)
Tensor_27781: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12015: 1179648 Bnr)
Tensor_27782: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12021: 19365888 Bnn)
Tensor_27783: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12023: 1179648 Bnr)
Tensor_27784: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12035: 19365888 Bnn)
Tensor_27785: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12037: 19365888 Bnr)
Tensor_27786: shape = [1536, 64, 197], stride = [12608, 1, 64], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12029: 19365888 Bnn)
Tensor_27787: shape = [128, 12, 197, 197], stride = [197, 0, 0, 1], offset = 0, size = 25216, datatype = boolean, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11852: 12608 Bnn)
Tensor_27788: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12059: 119221248 Bnr)
Tensor_27789: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12051: 59610624 Bnr)
Tensor_27790: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12079: 77463552 Bnn)
Tensor_27791: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12077: 38731776 Bnr)
Tensor_27792: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12084: 50432 Bnr)
Tensor_27793: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12085: 50432 Bnr)
Tensor_27794: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12041: 19365888 Bnr)
Tensor_27795: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12088: 1179648 Bnn)
Tensor_27796: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12098: 19365888 Bnn)
Tensor_27797: shape = [128, 197, 768], stride = [1, 0, 0], offset = 0, size = 128, datatype = float16, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12094: 128 Bnr)
Tensor_27798: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12107: 38731776 Bnr)
Tensor_27799: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12073: 38731776 Bnr)
Tensor_27800: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12110: 50432 Bnr)
Tensor_27801: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12111: 50432 Bnr)
Tensor_27802: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12102: 19365888 Bnr)
Tensor_27803: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12116: 4718592 Bnr)
Tensor_27804: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12121: 154927104 Bnr)
Tensor_27805: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12123: 154927104 Bnd)
Tensor_27806: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12127: 154927104 Bnr)
Tensor_27807: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12129: 154927104 Bnr)
Tensor_27808: shape = [25216], stride = [1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12130: 50432 Bnr)
Tensor_27809: shape = [25216], stride = [1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12131: 50432 Bnr)
Tensor_27810: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12119: 77463552 Bnr)
Tensor_27811: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12135: 4718592 Bnr)
Tensor_27812: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12146: 19365888 Bnr)
Tensor_27813: shape = [128, 197, 768], stride = [1, 0, 0], offset = 0, size = 128, datatype = float16, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12142: 128 Bnr)
Tensor_27814: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12158: 38731776 Bnr)
Tensor_27815: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12156: 38731776 Bnn)
Tensor_27816: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12161: 50432 Bnr)
Tensor_27817: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12162: 50432 Bnr)
Tensor_27818: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12152: 19365888 Bnr)
Tensor_27819: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12166: 1179648 Bnn)
Tensor_27820: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12172: 19365888 Bnr)
Tensor_27821: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12174: 1179648 Bnr)
Tensor_27822: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12180: 19365888 Bnn)
Tensor_27823: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12182: 1179648 Bnn)
Tensor_27824: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12194: 19365888 Bnr)
Tensor_27825: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12196: 19365888 Bnr)
Tensor_27826: shape = [1536, 64, 197], stride = [12608, 1, 64], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12188: 19365888 Bnn)
Tensor_27827: shape = [128, 12, 197, 197], stride = [197, 0, 0, 1], offset = 0, size = 25216, datatype = boolean, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11852: 12608 Bnn)
Tensor_27828: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12218: 119221248 Bnr)
Tensor_27829: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12210: 59610624 Bnn)
Tensor_27830: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12238: 77463552 Bnn)
Tensor_27831: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12236: 38731776 Bnr)
Tensor_27832: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12243: 50432 Bnr)
Tensor_27833: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12244: 50432 Bnr)
Tensor_27834: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12200: 19365888 Bnr)
Tensor_27835: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12247: 1179648 Bnr)
Tensor_27836: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12257: 19365888 Bnn)
Tensor_27837: shape = [128, 197, 768], stride = [1, 0, 0], offset = 0, size = 128, datatype = float16, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12253: 128 Bnr)
Tensor_27838: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12266: 38731776 Bnr)
Tensor_27839: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12232: 38731776 Bnr)
Tensor_27840: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12269: 50432 Bnr)
Tensor_27841: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12270: 50432 Bnr)
Tensor_27842: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12261: 19365888 Bnr)
Tensor_27843: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12275: 4718592 Bnr)
Tensor_27844: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12280: 154927104 Bnr)
Tensor_27845: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12282: 154927104 Bnr)
Tensor_27846: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12286: 154927104 Bnr)
Tensor_27847: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12288: 154927104 Bnr)
Tensor_27848: shape = [25216], stride = [1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12289: 50432 Bnr)
Tensor_27849: shape = [25216], stride = [1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12290: 50432 Bnr)
Tensor_27850: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12278: 77463552 Bnr)
Tensor_27851: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12294: 4718592 Bnr)
Tensor_27852: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12305: 19365888 Bnr)
Tensor_27853: shape = [128, 197, 768], stride = [1, 0, 0], offset = 0, size = 128, datatype = float16, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12301: 128 Bnr)
Tensor_27854: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12317: 38731776 Bnn)
Tensor_27855: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12315: 38731776 Bnr)
Tensor_27856: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12320: 50432 Bnr)
Tensor_27857: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12321: 50432 Bnr)
Tensor_27858: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12311: 19365888 Bnr)
Tensor_27859: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12325: 1179648 Bnn)
Tensor_27860: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12331: 19365888 Bnr)
Tensor_27861: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12333: 1179648 Bnr)
Tensor_27862: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12339: 19365888 Bnr)
Tensor_27863: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12341: 1179648 Bnn)
Tensor_27864: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12353: 19365888 Bnn)
Tensor_27865: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12355: 19365888 Bnr)
Tensor_27866: shape = [1536, 64, 197], stride = [12608, 1, 64], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12347: 19365888 Bnn)
Tensor_27867: shape = [128, 12, 197, 197], stride = [197, 0, 0, 1], offset = 0, size = 25216, datatype = boolean, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11852: 12608 Bnn)
Tensor_27868: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12377: 119221248 Bnn)
Tensor_27869: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12369: 59610624 Bnr)
Tensor_27870: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12397: 77463552 Bnn)
Tensor_27871: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12395: 38731776 Bnr)
Tensor_27872: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12402: 50432 Bnr)
Tensor_27873: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12403: 50432 Bnr)
Tensor_27874: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12359: 19365888 Bnr)
Tensor_27875: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12406: 1179648 Bnr)
Tensor_27876: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12416: 19365888 Bnr)
Tensor_27877: shape = [128, 197, 768], stride = [1, 0, 0], offset = 0, size = 128, datatype = float16, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12412: 128 Bnr)
Tensor_27878: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12425: 38731776 Bnr)
Tensor_27879: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12391: 38731776 Bnr)
Tensor_27880: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12428: 50432 Bnr)
Tensor_27881: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12429: 50432 Bnr)
Tensor_27882: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12420: 19365888 Bnr)
Tensor_27883: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12434: 4718592 Bnr)
Tensor_27884: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12439: 154927104 Bnr)
Tensor_27885: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12441: 154927104 Bnr)
Tensor_27886: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12445: 154927104 Bnd)
Tensor_27887: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12447: 154927104 Bnr)
Tensor_27888: shape = [25216], stride = [1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12448: 50432 Bnr)
Tensor_27889: shape = [25216], stride = [1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12449: 50432 Bnr)
Tensor_27890: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12437: 77463552 Bnn)
Tensor_27891: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12453: 4718592 Bnn)
Tensor_27892: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12464: 19365888 Bnr)
Tensor_27893: shape = [128, 197, 768], stride = [1, 0, 0], offset = 0, size = 128, datatype = float16, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12460: 128 Bnr)
Tensor_27894: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12476: 38731776 Bnn)
Tensor_27895: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12474: 38731776 Bnn)
Tensor_27896: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12479: 50432 Bnr)
Tensor_27897: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12480: 50432 Bnr)
Tensor_27898: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12470: 19365888 Bnn)
Tensor_27899: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12484: 1179648 Bnr)
Tensor_27900: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12490: 19365888 Bnr)
Tensor_27901: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12492: 1179648 Bnr)
Tensor_27902: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12498: 19365888 Bnn)
Tensor_27903: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12500: 1179648 Bnn)
Tensor_27904: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12512: 19365888 Bnn)
Tensor_27905: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12514: 19365888 Bnr)
Tensor_27906: shape = [1536, 64, 197], stride = [12608, 1, 64], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12506: 19365888 Bnn)
Tensor_27907: shape = [128, 12, 197, 197], stride = [197, 0, 0, 1], offset = 0, size = 25216, datatype = boolean, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11852: 12608 Bnn)
Tensor_27908: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12536: 119221248 Bnn)
Tensor_27909: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12528: 59610624 Bnr)
Tensor_27910: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12556: 77463552 Bnr)
Tensor_27911: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12554: 38731776 Bnn)
Tensor_27912: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12561: 50432 Bnr)
Tensor_27913: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12562: 50432 Bnr)
Tensor_27914: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12518: 19365888 Bnr)
Tensor_27915: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12565: 1179648 Bnr)
Tensor_27916: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12575: 19365888 Bnr)
Tensor_27917: shape = [128, 197, 768], stride = [1, 0, 0], offset = 0, size = 128, datatype = float16, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12571: 128 Bnr)
Tensor_27918: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12584: 38731776 Bnn)
Tensor_27919: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12550: 38731776 Bnn)
Tensor_27920: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12587: 50432 Bnr)
Tensor_27921: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12588: 50432 Bnr)
Tensor_27922: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12579: 19365888 Bnr)
Tensor_27923: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12593: 4718592 Bnn)
Tensor_27924: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12598: 154927104 Bnn)
Tensor_27925: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12600: 154927104 Bnn)
Tensor_27926: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12604: 154927104 Bnn)
Tensor_27927: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12606: 154927104 Bnn)
Tensor_27928: shape = [25216], stride = [1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12607: 50432 Bnr)
Tensor_27929: shape = [25216], stride = [1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12608: 50432 Bnr)
Tensor_27930: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12596: 77463552 Bnn)
Tensor_27931: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12612: 4718592 Bnn)
Tensor_27932: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12623: 19365888 Bnn)
Tensor_27933: shape = [128, 197, 768], stride = [1, 0, 0], offset = 0, size = 128, datatype = float16, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12619: 128 Bnr)
Tensor_27934: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12635: 38731776 Bnn)
Tensor_27935: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12633: 38731776 Bnn)
Tensor_27936: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12638: 50432 Bnr)
Tensor_27937: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12639: 50432 Bnr)
Tensor_27938: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12629: 19365888 Bnr)
Tensor_27939: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12643: 1179648 Bnn)
Tensor_27940: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12649: 19365888 Bnn)
Tensor_27941: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12651: 1179648 Bnn)
Tensor_27942: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12657: 19365888 Bnn)
Tensor_27943: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12659: 1179648 Bnn)
Tensor_27944: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12671: 19365888 Bnn)
Tensor_27945: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12673: 19365888 Bnr)
Tensor_27946: shape = [1536, 64, 197], stride = [12608, 1, 64], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12665: 19365888 Bnr)
Tensor_27947: shape = [128, 12, 197, 197], stride = [197, 0, 0, 1], offset = 0, size = 25216, datatype = boolean, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11852: 12608 Bnn)
Tensor_27948: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12695: 119221248 Bnn)
Tensor_27949: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12687: 59610624 Bnn)
Tensor_27950: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12715: 77463552 Bnn)
Tensor_27951: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12713: 38731776 Bnn)
Tensor_27952: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12720: 50432 Bnr)
Tensor_27953: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12721: 50432 Bnr)
Tensor_27954: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12677: 19365888 Bnn)
Tensor_27955: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12724: 1179648 Bnn)
Tensor_27956: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12734: 19365888 Bnn)
Tensor_27957: shape = [128, 197, 768], stride = [1, 0, 0], offset = 0, size = 128, datatype = float16, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12730: 128 Bnr)
Tensor_27958: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12743: 38731776 Bnn)
Tensor_27959: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12709: 38731776 Bnn)
Tensor_27960: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12746: 50432 Bnr)
Tensor_27961: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12747: 50432 Bnr)
Tensor_27962: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12738: 19365888 Bnr)
Tensor_27963: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12752: 4718592 Bnn)
Tensor_27964: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12757: 154927104 Bnn)
Tensor_27965: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12759: 154927104 Bnn)
Tensor_27966: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12763: 154927104 Bnr)
Tensor_27967: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12765: 154927104 Bnr)
Tensor_27968: shape = [25216], stride = [1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12766: 50432 Bnr)
Tensor_27969: shape = [25216], stride = [1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12767: 50432 Bnr)
Tensor_27970: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12755: 77463552 Bnr)
Tensor_27971: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12771: 4718592 Bnr)
Tensor_27972: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12782: 19365888 Bnn)
Tensor_27973: shape = [128, 197, 768], stride = [1, 0, 0], offset = 0, size = 128, datatype = float16, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12778: 128 Bnr)
Tensor_27974: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12794: 38731776 Bnr)
Tensor_27975: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12792: 38731776 Bnr)
Tensor_27976: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12797: 50432 Bnr)
Tensor_27977: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12798: 50432 Bnr)
Tensor_27978: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12788: 19365888 Bnn)
Tensor_27979: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12802: 1179648 Bnn)
Tensor_27980: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12808: 19365888 Bnr)
Tensor_27981: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12810: 1179648 Bnn)
Tensor_27982: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12816: 19365888 Bnn)
Tensor_27983: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12818: 1179648 Bnn)
Tensor_27984: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12830: 19365888 Bnr)
Tensor_27985: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12832: 19365888 Bnn)
Tensor_27986: shape = [1536, 64, 197], stride = [12608, 1, 64], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12824: 19365888 Bnr)
Tensor_27987: shape = [128, 12, 197, 197], stride = [197, 0, 0, 1], offset = 0, size = 25216, datatype = boolean, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11852: 12608 Bnn)
Tensor_27988: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12854: 119221248 Bnr)
Tensor_27989: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12846: 59610624 Bnn)
Tensor_27990: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12874: 77463552 Bnn)
Tensor_27991: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12872: 38731776 Bnn)
Tensor_27992: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12879: 50432 Bnr)
Tensor_27993: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12880: 50432 Bnr)
Tensor_27994: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12836: 19365888 Bnn)
Tensor_27995: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12883: 1179648 Bnn)
Tensor_27996: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12893: 19365888 Bnn)
Tensor_27997: shape = [128, 197, 768], stride = [1, 0, 0], offset = 0, size = 128, datatype = float16, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12889: 128 Bnr)
Tensor_27998: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12902: 38731776 Bnr)
Tensor_27999: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12868: 38731776 Bnn)
Tensor_28000: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12905: 50432 Bnr)
Tensor_28001: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12906: 50432 Bnr)
Tensor_28002: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12897: 19365888 Bnn)
Tensor_28003: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12911: 4718592 Bnn)
Tensor_28004: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12916: 154927104 Bnn)
Tensor_28005: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12918: 154927104 Bnr)
Tensor_28006: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12922: 154927104 Bnr)
Tensor_28007: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12924: 154927104 Bnr)
Tensor_28008: shape = [25216], stride = [1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12925: 50432 Bnr)
Tensor_28009: shape = [25216], stride = [1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12926: 50432 Bnr)
Tensor_28010: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12914: 77463552 Bnn)
Tensor_28011: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12930: 4718592 Bnr)
Tensor_28012: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12941: 19365888 Bnr)
Tensor_28013: shape = [128, 197, 768], stride = [1, 0, 0], offset = 0, size = 128, datatype = float16, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12937: 128 Bnr)
Tensor_28014: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12953: 38731776 Bnr)
Tensor_28015: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12951: 38731776 Bnr)
Tensor_28016: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12956: 50432 Bnr)
Tensor_28017: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12957: 50432 Bnr)
Tensor_28018: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12947: 19365888 Bnr)
Tensor_28019: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12961: 1179648 Bnn)
Tensor_28020: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12967: 19365888 Bnr)
Tensor_28021: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12969: 1179648 Bnn)
Tensor_28022: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12975: 19365888 Bnn)
Tensor_28023: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12977: 1179648 Bnn)
Tensor_28024: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12989: 19365888 Bnn)
Tensor_28025: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12991: 19365888 Bnn)
Tensor_28026: shape = [1536, 64, 197], stride = [12608, 1, 64], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12983: 19365888 Bnn)
Tensor_28027: shape = [128, 12, 197, 197], stride = [197, 0, 0, 1], offset = 0, size = 25216, datatype = boolean, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11852: 12608 Bnn)
Tensor_28028: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13013: 119221248 Bnn)
Tensor_28029: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13005: 59610624 Bnn)
Tensor_28030: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13033: 77463552 Bnn)
Tensor_28031: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13031: 38731776 Bnn)
Tensor_28032: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13038: 50432 Bnr)
Tensor_28033: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13039: 50432 Bnr)
Tensor_28034: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12995: 19365888 Bnr)
Tensor_28035: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13042: 1179648 Bnn)
Tensor_28036: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13052: 19365888 Bnr)
Tensor_28037: shape = [128, 197, 768], stride = [1, 0, 0], offset = 0, size = 128, datatype = float16, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13048: 128 Bnr)
Tensor_28038: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13061: 38731776 Bnr)
Tensor_28039: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13027: 38731776 Bnn)
Tensor_28040: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13064: 50432 Bnr)
Tensor_28041: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13065: 50432 Bnr)
Tensor_28042: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13056: 19365888 Bnn)
Tensor_28043: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13070: 4718592 Bnn)
Tensor_28044: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13075: 154927104 Bnr)
Tensor_28045: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13077: 154927104 Bnr)
Tensor_28046: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13081: 154927104 Bnr)
Tensor_28047: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13083: 154927104 Bnr)
Tensor_28048: shape = [25216], stride = [1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13084: 50432 Bnr)
Tensor_28049: shape = [25216], stride = [1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13085: 50432 Bnr)
Tensor_28050: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13073: 77463552 Bnn)
Tensor_28051: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13089: 4718592 Bnn)
Tensor_28052: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13100: 19365888 Bnn)
Tensor_28053: shape = [128, 197, 768], stride = [1, 0, 0], offset = 0, size = 128, datatype = float16, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13096: 128 Bnr)
Tensor_28054: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13112: 38731776 Bnn)
Tensor_28055: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13110: 38731776 Bnn)
Tensor_28056: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13115: 50432 Bnr)
Tensor_28057: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13116: 50432 Bnr)
Tensor_28058: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13106: 19365888 Bnn)
Tensor_28059: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13120: 1179648 Bnn)
Tensor_28060: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13126: 19365888 Bnr)
Tensor_28061: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13128: 1179648 Bnn)
Tensor_28062: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13134: 19365888 Bnn)
Tensor_28063: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13136: 1179648 Bnn)
Tensor_28064: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13148: 19365888 Bnr)
Tensor_28065: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13150: 19365888 Bnn)
Tensor_28066: shape = [1536, 64, 197], stride = [12608, 1, 64], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13142: 19365888 Bnn)
Tensor_28067: shape = [128, 12, 197, 197], stride = [197, 0, 0, 1], offset = 0, size = 25216, datatype = boolean, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11852: 12608 Bnn)
Tensor_28068: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13172: 119221248 Bnn)
Tensor_28069: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13164: 59610624 Bnn)
Tensor_28070: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13192: 77463552 Bnn)
Tensor_28071: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13190: 38731776 Bnn)
Tensor_28072: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13197: 50432 Bnr)
Tensor_28073: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13198: 50432 Bnr)
Tensor_28074: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13154: 19365888 Bnr)
Tensor_28075: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13201: 1179648 Bnn)
Tensor_28076: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13211: 19365888 Bnn)
Tensor_28077: shape = [128, 197, 768], stride = [1, 0, 0], offset = 0, size = 128, datatype = float16, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13207: 128 Bnr)
Tensor_28078: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13220: 38731776 Bnn)
Tensor_28079: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13186: 38731776 Bnn)
Tensor_28080: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13223: 50432 Bnr)
Tensor_28081: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13224: 50432 Bnr)
Tensor_28082: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13215: 19365888 Bnn)
Tensor_28083: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13229: 4718592 Bnn)
Tensor_28084: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13234: 154927104 Bnn)
Tensor_28085: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13236: 154927104 Bnn)
Tensor_28086: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13240: 154927104 Bnn)
Tensor_28087: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13242: 154927104 Bnd)
Tensor_28088: shape = [25216], stride = [1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13243: 50432 Bnr)
Tensor_28089: shape = [25216], stride = [1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13244: 50432 Bnr)
Tensor_28090: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13232: 77463552 Bnn)
Tensor_28091: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13248: 4718592 Bnn)
Tensor_28092: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13259: 19365888 Bnn)
Tensor_28093: shape = [128, 197, 768], stride = [1, 0, 0], offset = 0, size = 128, datatype = float16, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13255: 128 Bnr)
Tensor_28094: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13271: 38731776 Bnn)
Tensor_28095: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13269: 38731776 Bnn)
Tensor_28096: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13274: 50432 Bnr)
Tensor_28097: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13275: 50432 Bnr)
Tensor_28098: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13265: 19365888 Bnn)
Tensor_28099: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13279: 1179648 Bnn)
Tensor_28100: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13285: 19365888 Bnn)
Tensor_28101: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13287: 1179648 Bnn)
Tensor_28102: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13293: 19365888 Bnr)
Tensor_28103: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13295: 1179648 Bnn)
Tensor_28104: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13307: 19365888 Bnn)
Tensor_28105: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13309: 19365888 Bnn)
Tensor_28106: shape = [1536, 64, 197], stride = [12608, 1, 64], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13301: 19365888 Bnr)
Tensor_28107: shape = [128, 12, 197, 197], stride = [197, 0, 0, 1], offset = 0, size = 25216, datatype = boolean, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11852: 12608 Bnn)
Tensor_28108: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13331: 119221248 Bnn)
Tensor_28109: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13323: 59610624 Bnn)
Tensor_28110: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13351: 77463552 Bnn)
Tensor_28111: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13349: 38731776 Bnn)
Tensor_28112: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13356: 50432 Bnr)
Tensor_28113: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13357: 50432 Bnr)
Tensor_28114: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13313: 19365888 Bnr)
Tensor_28115: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13360: 1179648 Bnn)
Tensor_28116: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13370: 19365888 Bnn)
Tensor_28117: shape = [128, 197, 768], stride = [1, 0, 0], offset = 0, size = 128, datatype = float16, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13366: 128 Bnr)
Tensor_28118: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13379: 38731776 Bnr)
Tensor_28119: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13345: 38731776 Bnn)
Tensor_28120: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13382: 50432 Bnr)
Tensor_28121: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13383: 50432 Bnr)
Tensor_28122: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13374: 19365888 Bnr)
Tensor_28123: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13388: 4718592 Bnn)
Tensor_28124: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13393: 154927104 Bnr)
Tensor_28125: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13395: 154927104 Bnr)
Tensor_28126: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13399: 154927104 Bnr)
Tensor_28127: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13401: 154927104 Bnr)
Tensor_28128: shape = [25216], stride = [1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13402: 50432 Bnr)
Tensor_28129: shape = [25216], stride = [1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13403: 50432 Bnr)
Tensor_28130: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13391: 77463552 Bnn)
Tensor_28131: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13407: 4718592 Bnn)
Tensor_28132: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13418: 19365888 Bnn)
Tensor_28133: shape = [128, 197, 768], stride = [1, 0, 0], offset = 0, size = 128, datatype = float16, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13414: 128 Bnr)
Tensor_28134: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13430: 38731776 Bnn)
Tensor_28135: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13428: 38731776 Bnn)
Tensor_28136: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13433: 50432 Bnr)
Tensor_28137: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13434: 50432 Bnr)
Tensor_28138: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13424: 19365888 Bnn)
Tensor_28139: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13438: 1179648 Bnn)
Tensor_28140: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13444: 19365888 Bnn)
Tensor_28141: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13446: 1179648 Bnn)
Tensor_28142: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13452: 19365888 Bnn)
Tensor_28143: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13454: 1179648 Bnn)
Tensor_28144: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13466: 19365888 Bnn)
Tensor_28145: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13468: 19365888 Bnn)
Tensor_28146: shape = [1536, 64, 197], stride = [12608, 1, 64], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13460: 19365888 Bnn)
Tensor_28147: shape = [128, 12, 197, 197], stride = [197, 0, 0, 1], offset = 0, size = 25216, datatype = boolean, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11852: 12608 Bnn)
Tensor_28148: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13490: 119221248 Bnn)
Tensor_28149: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13482: 59610624 Bnn)
Tensor_28150: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13510: 77463552 Bnn)
Tensor_28151: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13508: 38731776 Bnn)
Tensor_28152: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13515: 50432 Bnr)
Tensor_28153: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13516: 50432 Bnr)
Tensor_28154: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13472: 19365888 Bnr)
Tensor_28155: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13519: 1179648 Bnn)
Tensor_28156: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13529: 19365888 Bnn)
Tensor_28157: shape = [128, 197, 768], stride = [1, 0, 0], offset = 0, size = 128, datatype = float16, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13525: 128 Bnr)
Tensor_28158: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13538: 38731776 Bnn)
Tensor_28159: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13504: 38731776 Bnn)
Tensor_28160: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13541: 50432 Bnr)
Tensor_28161: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13542: 50432 Bnr)
Tensor_28162: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13533: 19365888 Bnn)
Tensor_28163: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13547: 4718592 Bnn)
Tensor_28164: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13552: 154927104 Bnn)
Tensor_28165: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13554: 154927104 Bnn)
Tensor_28166: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13558: 154927104 Bnn)
Tensor_28167: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13560: 154927104 Bnn)
Tensor_28168: shape = [25216], stride = [1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13561: 50432 Bnr)
Tensor_28169: shape = [25216], stride = [1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13562: 50432 Bnr)
Tensor_28170: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13550: 77463552 Bnn)
Tensor_28171: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13566: 4718592 Bnn)
Tensor_28172: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13577: 19365888 Bnn)
Tensor_28173: shape = [128, 197, 768], stride = [1, 0, 0], offset = 0, size = 128, datatype = float16, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13573: 128 Bnr)
Tensor_28174: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13589: 38731776 Bnn)
Tensor_28175: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13587: 38731776 Bnn)
Tensor_28176: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13592: 50432 Bnr)
Tensor_28177: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13593: 50432 Bnr)
Tensor_28178: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13583: 19365888 Bnn)
Tensor_28179: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13597: 1179648 Bnn)
Tensor_28180: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13603: 19365888 Bnn)
Tensor_28181: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13605: 1179648 Bnn)
Tensor_28182: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13611: 19365888 Bnn)
Tensor_28183: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13613: 1179648 Bnn)
Tensor_28184: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13625: 19365888 Bnn)
Tensor_28185: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13627: 19365888 Bnn)
Tensor_28186: shape = [1536, 64, 197], stride = [12608, 1, 64], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13619: 19365888 Bnr)
Tensor_28187: shape = [128, 12, 197, 197], stride = [197, 0, 0, 1], offset = 0, size = 25216, datatype = boolean, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11852: 12608 Bnn)
Tensor_28188: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13649: 119221248 Bnn)
Tensor_28189: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13641: 59610624 Bnn)
Tensor_28190: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13669: 77463552 Bnn)
Tensor_28191: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13667: 38731776 Bnn)
Tensor_28192: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13674: 50432 Bnr)
Tensor_28193: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13675: 50432 Bnr)
Tensor_28194: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13631: 19365888 Bnn)
Tensor_28195: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13678: 1179648 Bnn)
Tensor_28196: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13688: 19365888 Bnn)
Tensor_28197: shape = [128, 197, 768], stride = [1, 0, 0], offset = 0, size = 128, datatype = float16, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13684: 128 Bnr)
Tensor_28198: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13697: 38731776 Bnn)
Tensor_28199: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13663: 38731776 Bnn)
Tensor_28200: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13700: 50432 Bnr)
Tensor_28201: shape = [128, 197], stride = [197, 1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13701: 50432 Bnr)
Tensor_28202: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13692: 19365888 Bnn)
Tensor_28203: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13706: 4718592 Bnn)
Tensor_28204: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13711: 154927104 Bnn)
Tensor_28205: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13713: 154927104 Bnn)
Tensor_28206: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13717: 154927104 Bnd)
Tensor_28207: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13719: 154927104 Bnn)
Tensor_28208: shape = [25216], stride = [1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13720: 50432 Bnr)
Tensor_28209: shape = [25216], stride = [1], offset = 0, size = 25216, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13721: 50432 Bnr)
Tensor_28210: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13709: 77463552 Bnn)
Tensor_28211: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13725: 4718592 Bnn)
Tensor_28212: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13736: 19365888 Bnn)
Tensor_28213: shape = [128, 197, 768], stride = [1, 0, 0], offset = 0, size = 128, datatype = float16, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13732: 128 Bnr)
Tensor_28214: shape = [128, 768], stride = [768, 1], offset = 0, size = 98304, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13752: 196608 Bnr)
Tensor_28215: shape = [128, 768], stride = [768, 1], offset = 0, size = 98304, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13754: 196608 Bnr)
Tensor_28216: shape = [128], stride = [1], offset = 0, size = 128, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13755: 256 Bnr)
Tensor_28217: shape = [128], stride = [1], offset = 0, size = 128, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13756: 256 Bnr)
Tensor_28218: shape = [128, 768], stride = [768, 1], offset = 0, size = 98304, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13758: 98304 Bnr)
Tensor_28219: shape = [1000, 768], stride = [768, 1], offset = 0, size = 768000, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13760: 1536000 Bnr)
Tensor_28220: shape = [128, 1000], stride = [1000, 1], offset = 0, size = 128000, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11802: 512000 Bnr)
Tensor_28221: shape = [128, 1000], stride = [1000, 1], offset = 0, size = 128000, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13770: 256000 Bnr)
Tensor_28222: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11795: 4 Bnn)
Operation_13782: operator = ElemwiseUnaryForward, inputs = [Tensor_28222], outputs = [Tensor_28224], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_DIV scal1_float: 1]
Tensor_28224: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11795: 4 Bnn)
Tensor_28225: shape = [1], stride = [1], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:1), attr<Storage> = (28227: 4 Ban)
Operation_13785: operator = ElemwiseUnaryForward, inputs = [Tensor_28225], outputs = [Tensor_28227], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_FILL scal1_float: 65536]
Tensor_28227: shape = [1], stride = [1], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:1), attr<Storage> = (28227: 4 Bnn)
Operation_13787: operator = ReshapeForward, inputs = [Tensor_28224], outputs = [Tensor_28229], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1 stride: 0]
Tensor_28229: shape = [1], stride = [0], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11795: 4 Bnn)
Operation_21719: operator = DistributeWithMemAllocForward, inputs = [Tensor_28229], outputs = [Tensor_28231], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_28231: shape = [1], stride = [0], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:1), attr<Storage> = (28231: 4 Ban)
Operation_13789: operator = ElemwiseBinaryForward, inputs = [Tensor_28231, Tensor_28227], outputs = [Tensor_28233], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_MUL]
Tensor_28233: shape = [1], stride = [1], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:1), attr<Storage> = (28233: 4 Ban)
Tensor_28234: shape = [1], stride = [1], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:1), attr<Storage> = (28234: 4 Ban)
Operation_21724: operator = DistributeWithMemAllocForward, inputs = [Tensor_28229], outputs = [Tensor_28236], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_28236: shape = [1], stride = [0], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:1), attr<Storage> = (28231: 4 Bnn)
Operation_13792: operator = ElemwiseBinaryBackward, inputs = [Tensor_28234, Tensor_28236, Tensor_28227, nullptr], outputs = [Tensor_28238, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_MUL]
Tensor_28238: shape = [1], stride = [1], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:1), attr<Storage> = (28238: 4 Bar)
Operation_13794: operator = SumForward, inputs = [Tensor_28238], outputs = [Tensor_28240], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = this->21731
  - Operand: [dims: 0]
Tensor_28240: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (28231: 4 Bnn)
Operation_21731: operator = AllReduceDuplicated, inputs = [Tensor_28240, nullptr], outputs = [Tensor_28243, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 13794->this->13800
  - Operand: [subop: SUM tag: 21730]
Operation_28242: operator = Consume, inputs = [Tensor_28240], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_28243: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28231: 4 Bnn)
Operation_21741: operator = DistributeWithMemAllocForward, inputs = [Tensor_28220], outputs = [Tensor_28257], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Operation_21861: operator = DistributeWithMemAllocForward, inputs = [Tensor_28190], outputs = [Tensor_28376], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Operation_22062: operator = DistributeWithMemAllocForward, inputs = [Tensor_28150], outputs = [Tensor_28581], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Operation_22263: operator = DistributeWithMemAllocForward, inputs = [Tensor_28110], outputs = [Tensor_28786], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Operation_22464: operator = DistributeWithMemAllocForward, inputs = [Tensor_28070], outputs = [Tensor_28991], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Operation_13800: operator = ElemwiseUnaryBackward, inputs = [Tensor_28243, nullptr, nullptr], outputs = [Tensor_28250], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 21731->this
  - Operand: [subop: ELEMWISE_UNARY_DIV scal1_float: 1]
Tensor_28250: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28231: 4 Bnr)
Operation_13802: operator = MeanBackward, inputs = [Tensor_28250], outputs = [Tensor_28252], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dims: 0 divisor: 128]
Tensor_28252: shape = [128], stride = [1], offset = 0, size = 128, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28252: 512 Bar)
Operation_21737: operator = DistributeWithMemAllocForward, inputs = [Tensor_28252], outputs = [Tensor_28254], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_28254: shape = [128], stride = [1], offset = 0, size = 128, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13776: 256 Brr)
Operation_13804: operator = SumBackward, inputs = [Tensor_28254, nullptr], outputs = [Tensor_28256], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dims: 1]
Tensor_28256: shape = [128, 1000], stride = [1000, 1], offset = 0, size = 128000, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (28256: 256000 Bar)
Tensor_28257: shape = [128, 1000], stride = [1000, 1], offset = 0, size = 128000, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13768: 256000 Brn)
Operation_13806: operator = ElemwiseBinaryBackward, inputs = [Tensor_28256, Tensor_28257, Tensor_28221, nullptr], outputs = [nullptr, Tensor_28259], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_MUL]
Tensor_28259: shape = [128, 1000], stride = [1000, 1], offset = 0, size = 128000, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (28259: 256000 Bar)
Operation_13808: operator = SoftmaxBackward, inputs = [Tensor_28259, Tensor_28221], outputs = [Tensor_28261], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [method: SOFTMAX_LOG dim: 1]
Tensor_28261: shape = [128, 1000], stride = [1000, 1], offset = 0, size = 128000, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13768: 256000 Bnr)
Operation_13810: operator = TypecastBackward, inputs = [Tensor_28261], outputs = [Tensor_28263], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_28263: shape = [128, 1000], stride = [1000, 1], offset = 0, size = 128000, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13764: 128000 Brr)
Operation_13812: operator = LinearBackward, inputs = [Tensor_28263, Tensor_28218, Tensor_28219], outputs = [Tensor_28265, Tensor_28266, Tensor_28270], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->21753
  - Operand: []
Tensor_28265: shape = [128, 768], stride = [768, 1], offset = 0, size = 98304, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (28265: 98304 Bar)
Tensor_28266: shape = [1000, 768], stride = [768, 1], offset = 0, size = 768000, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (28269: 1536000 Ban)
Operation_21753: operator = AllReduceDuplicated, inputs = [Tensor_28266, nullptr], outputs = [Tensor_28269, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 13812->this
  - Operand: [subop: SUM tag: 21752]
Operation_28268: operator = Consume, inputs = [Tensor_28266], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_28269: shape = [1000, 768], stride = [768, 1], offset = 0, size = 768000, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28269: 1536000 Bnr)
Tensor_28270: shape = [1000], stride = [1], offset = 0, size = 1000, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (13762: 2000 Brn)
Operation_21757: operator = AllReduceDuplicated, inputs = [Tensor_28270, nullptr], outputs = [Tensor_28273, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: SUM tag: 21756]
Operation_28272: operator = Consume, inputs = [Tensor_28270], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_28273: shape = [1000], stride = [1], offset = 0, size = 1000, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13762: 2000 Bnr)
Operation_13816: operator = TypecastBackward, inputs = [Tensor_28265], outputs = [Tensor_28275], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_28275: shape = [128, 768], stride = [768, 1], offset = 0, size = 98304, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (28275: 196608 Bar)
Operation_13818: operator = LayernormBackward, inputs = [Tensor_28275, Tensor_28214, Tensor_28215, Tensor_27729, Tensor_27730, Tensor_28216, Tensor_28217], outputs = [Tensor_28277, Tensor_28278, Tensor_28282], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->21765
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_28277: shape = [128, 768], stride = [768, 1], offset = 0, size = 98304, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (28277: 196608 Ban)
Tensor_28278: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (28281: 3072 Ban)
Operation_21765: operator = AllReduceDuplicated, inputs = [Tensor_28278, nullptr], outputs = [Tensor_28281, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 13818->this
  - Operand: [subop: SUM tag: 21764]
Operation_28280: operator = Consume, inputs = [Tensor_28278], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_28281: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28281: 3072 Bnn)
Tensor_28282: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (28285: 3072 Ban)
Operation_21769: operator = AllReduceDuplicated, inputs = [Tensor_28282, nullptr], outputs = [Tensor_28285, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: SUM tag: 21768]
Operation_28284: operator = Consume, inputs = [Tensor_28282], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_28285: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28285: 3072 Bnn)
Operation_13822: operator = MeanBackward, inputs = [Tensor_28277], outputs = [Tensor_28287], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dims: 1 divisor: 196]
Tensor_28287: shape = [128, 196, 768], stride = [151296, 768, 1], offset = 768, size = 19365888, datatype = float32, contiguous? = false, complete? = false, dense? = false, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (28289: 38731776 Ban)
Operation_13824: operator = ReshapeBackward, inputs = [Tensor_28287], outputs = [Tensor_28289], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 196 shape: 768 stride: 151296 stride: 768 stride: 1 offset: 768]
Tensor_28289: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (28289: 38731776 Bnn)
Operation_13829: operator = ElemwiseUnaryBackward, inputs = [Tensor_28289, nullptr, nullptr], outputs = [Tensor_28291], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 1]
Tensor_28291: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (28355: 38731776 Ban)
Operation_13831: operator = TypecastBackward, inputs = [Tensor_28289], outputs = [Tensor_28293], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_28293: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13742: 19365888 Brn)
Operation_13833: operator = ElemwiseBinaryBackward, inputs = [Tensor_28293, Tensor_28212, Tensor_28213, nullptr], outputs = [Tensor_28295, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_MUL]
Tensor_28295: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13742: 19365888 Bnn)
Operation_13835: operator = ElemwiseUnaryBackward, inputs = [Tensor_28295, nullptr, nullptr], outputs = [Tensor_28297], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_DIV scal1_float: 0.85]
Tensor_28297: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13742: 19365888 Bnn)
Operation_13837: operator = ReshapeBackward, inputs = [Tensor_28297], outputs = [Tensor_28299], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 151296 stride: 768 stride: 1]
Tensor_28299: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13742: 19365888 Bnn)
Operation_13839: operator = LinearBackward, inputs = [Tensor_28299, Tensor_28210, Tensor_28211], outputs = [Tensor_28301, Tensor_28302, Tensor_28306], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->21789
  - Operand: []
Tensor_28301: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13669: 77463552 Bnn)
Tensor_28302: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (28305: 4718592 Ban)
Operation_21789: operator = AllReduceDuplicated, inputs = [Tensor_28302, nullptr], outputs = [Tensor_28305, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 13839->this
  - Operand: [subop: SUM tag: 21788]
Operation_28304: operator = Consume, inputs = [Tensor_28302], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_28305: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28305: 4718592 Bnn)
Tensor_28306: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (11821: 1536 Brn)
Operation_21793: operator = AllReduceDuplicated, inputs = [Tensor_28306, nullptr], outputs = [Tensor_28309, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: SUM tag: 21792]
Operation_28308: operator = Consume, inputs = [Tensor_28306], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_28309: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11821: 1536 Bnn)
Operation_13843: operator = TypecastBackward, inputs = [Tensor_28301], outputs = [Tensor_28311], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_28311: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (28311: 154927104 Ban)
Operation_13845: operator = LayernormBackward, inputs = [Tensor_28311, Tensor_28206, Tensor_28207, Tensor_27725, Tensor_27726, Tensor_28208, Tensor_28209], outputs = [Tensor_28313, Tensor_28314, Tensor_28318], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->21801
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_28313: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (28313: 154927104 Ban)
Tensor_28314: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (28317: 12288 Ban)
Operation_21801: operator = AllReduceDuplicated, inputs = [Tensor_28314, nullptr], outputs = [Tensor_28317, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 13845->this
  - Operand: [subop: SUM tag: 21800]
Operation_28316: operator = Consume, inputs = [Tensor_28314], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_28317: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28317: 12288 Bnn)
Tensor_28318: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (28321: 12288 Ban)
Operation_21805: operator = AllReduceDuplicated, inputs = [Tensor_28318, nullptr], outputs = [Tensor_28321, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: SUM tag: 21804]
Operation_28320: operator = Consume, inputs = [Tensor_28318], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_28321: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28321: 12288 Bnn)
Operation_13849: operator = TypecastBackward, inputs = [Tensor_28313], outputs = [Tensor_28323], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_28323: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13510: 77463552 Bnn)
Operation_13851: operator = TypecastBackward, inputs = [Tensor_28323], outputs = [Tensor_28325], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_28325: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (28311: 154927104 Bnn)
Operation_13853: operator = ElemwiseUnaryBackward, inputs = [Tensor_28325, Tensor_28204, Tensor_28205], outputs = [Tensor_28327], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_GELU]
Tensor_28327: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (28311: 154927104 Bnn)
Operation_13855: operator = TypecastBackward, inputs = [Tensor_28327], outputs = [Tensor_28329], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_28329: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13709: 77463552 Bnn)
Operation_13857: operator = LinearBackward, inputs = [Tensor_28329, Tensor_28202, Tensor_28203], outputs = [Tensor_28331, Tensor_28332, Tensor_28336], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->21819
  - Operand: []
Tensor_28331: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13736: 19365888 Bnn)
Tensor_28332: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (13725: 4718592 Bnn)
Operation_21819: operator = AllReduceDuplicated, inputs = [Tensor_28332, nullptr], outputs = [Tensor_28335, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 13857->this
  - Operand: [subop: SUM tag: 21818]
Operation_28334: operator = Consume, inputs = [Tensor_28332], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_28335: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13725: 4718592 Bnn)
Tensor_28336: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (11969: 6144 Brn)
Operation_21823: operator = AllReduceDuplicated, inputs = [Tensor_28336, nullptr], outputs = [Tensor_28339, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: SUM tag: 21822]
Operation_28338: operator = Consume, inputs = [Tensor_28336], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_28339: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11969: 6144 Bnn)
Operation_13861: operator = TypecastBackward, inputs = [Tensor_28331], outputs = [Tensor_28341], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_28341: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (28289: 38731776 Bnn)
Operation_13863: operator = ReshapeBackward, inputs = [Tensor_28341], outputs = [Tensor_28343], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 25216 shape: 768 stride: 768 stride: 1]
Tensor_28343: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (28289: 38731776 Bnn)
Operation_13865: operator = LayernormBackward, inputs = [Tensor_28343, Tensor_28198, Tensor_28199, Tensor_27727, Tensor_27728, Tensor_28200, Tensor_28201], outputs = [Tensor_28345, Tensor_28346, Tensor_28350], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->21833
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_28345: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (28345: 38731776 Ban)
Tensor_28346: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (28349: 3072 Ban)
Operation_21833: operator = AllReduceDuplicated, inputs = [Tensor_28346, nullptr], outputs = [Tensor_28349, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 13865->this
  - Operand: [subop: SUM tag: 21832]
Operation_28348: operator = Consume, inputs = [Tensor_28346], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_28349: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28349: 3072 Bnn)
Tensor_28350: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (28353: 3072 Ban)
Operation_21837: operator = AllReduceDuplicated, inputs = [Tensor_28350, nullptr], outputs = [Tensor_28353, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: SUM tag: 21836]
Operation_28352: operator = Consume, inputs = [Tensor_28350], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_28353: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28353: 3072 Bnn)
Operation_13867: operator = ElemwiseBinaryForward, inputs = [Tensor_28291, Tensor_28345], outputs = [Tensor_28355], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_28355: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (28355: 38731776 Bnn)
Operation_13874: operator = ElemwiseUnaryBackward, inputs = [Tensor_28355, nullptr, nullptr], outputs = [Tensor_28357], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 1]
Tensor_28357: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (28289: 38731776 Bnn)
Operation_13876: operator = TypecastBackward, inputs = [Tensor_28355], outputs = [Tensor_28359], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_28359: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13742: 19365888 Bnn)
Operation_13878: operator = ElemwiseBinaryBackward, inputs = [Tensor_28359, Tensor_28196, Tensor_28197, nullptr], outputs = [Tensor_28361, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_MUL]
Tensor_28361: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13742: 19365888 Bnn)
Operation_13880: operator = ElemwiseUnaryBackward, inputs = [Tensor_28361, nullptr, nullptr], outputs = [Tensor_28363], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_DIV scal1_float: 0.85]
Tensor_28363: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13742: 19365888 Bnn)
Operation_13882: operator = LinearBackward, inputs = [Tensor_28363, Tensor_28194, Tensor_28195], outputs = [Tensor_28365, Tensor_28366, Tensor_28370], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->21853
  - Operand: []
Tensor_28365: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13688: 19365888 Bnn)
Tensor_28366: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (28369: 1179648 Ban)
Operation_21853: operator = AllReduceDuplicated, inputs = [Tensor_28366, nullptr], outputs = [Tensor_28369, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 13882->this
  - Operand: [subop: SUM tag: 21852]
Operation_28368: operator = Consume, inputs = [Tensor_28366], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_28369: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28369: 1179648 Bnn)
Tensor_28370: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (28373: 1536 Ban)
Operation_21857: operator = AllReduceDuplicated, inputs = [Tensor_28370, nullptr], outputs = [Tensor_28373, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: SUM tag: 21856]
Operation_28372: operator = Consume, inputs = [Tensor_28370], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_28373: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28373: 1536 Bnn)
Operation_13886: operator = TypecastBackward, inputs = [Tensor_28365], outputs = [Tensor_28375], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_28375: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13697: 38731776 Bnn)
Tensor_28376: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13746: 38731776 Brn)
Operation_13888: operator = LayernormBackward, inputs = [Tensor_28375, Tensor_28376, Tensor_28191, Tensor_27721, Tensor_27722, Tensor_28192, Tensor_28193], outputs = [Tensor_28378, Tensor_28379, Tensor_28383], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->21867
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_28378: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (28345: 38731776 Bnn)
Tensor_28379: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (28382: 3072 Ban)
Operation_21867: operator = AllReduceDuplicated, inputs = [Tensor_28379, nullptr], outputs = [Tensor_28382, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 13888->this
  - Operand: [subop: SUM tag: 21866]
Operation_28381: operator = Consume, inputs = [Tensor_28379], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_28382: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28382: 3072 Bnn)
Tensor_28383: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (28386: 3072 Ban)
Operation_21871: operator = AllReduceDuplicated, inputs = [Tensor_28383, nullptr], outputs = [Tensor_28386, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: SUM tag: 21870]
Operation_28385: operator = Consume, inputs = [Tensor_28383], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_28386: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28386: 3072 Bnn)
Operation_21874: operator = AllGatherDuplicated, inputs = [Tensor_28378], outputs = [Tensor_28389], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->23469
  - Operand: [tag: 21873]
Operation_28388: operator = Consume, inputs = [Tensor_28378], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_28389: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13351: 77463552 Bnn)
Operation_22665: operator = DistributeWithMemAllocForward, inputs = [Tensor_28030], outputs = [Tensor_29196], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Operation_22866: operator = DistributeWithMemAllocForward, inputs = [Tensor_27990], outputs = [Tensor_29401], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Operation_23067: operator = DistributeWithMemAllocForward, inputs = [Tensor_27950], outputs = [Tensor_29606], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Operation_23268: operator = DistributeWithMemAllocForward, inputs = [Tensor_27910], outputs = [Tensor_29811], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Operation_23469: operator = DistributeWithMemAllocForward, inputs = [Tensor_27870], outputs = [Tensor_30016], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 21874->this
  - Operand: []
Operation_13892: operator = TypecastBackward, inputs = [Tensor_28389], outputs = [Tensor_28396], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_28396: shape = [128, 197, 768], stride = [768, 98304, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13663: 38731776 Bnn)
Operation_13896: operator = ReshapeBackward, inputs = [Tensor_28396], outputs = [Tensor_28398], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 197 shape: 128 shape: 768 stride: 98304 stride: 768 stride: 1]
Tensor_28398: shape = [197, 1536, 64], stride = [98304, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13663: 38731776 Bnn)
Operation_13898: operator = ContiguousBackward, inputs = [Tensor_28398], outputs = [Tensor_28400], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_28400: shape = [197, 1536, 64], stride = [64, 12608, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28402: 38731776 Ban)
Operation_13900: operator = ReshapeBackward, inputs = [Tensor_28400], outputs = [Tensor_28402], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 197 shape: 1536 shape: 64 stride: 64 stride: 12608 stride: 1]
Tensor_28402: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28402: 38731776 Bnn)
Operation_21884: operator = DistributeWithMemAllocForward, inputs = [Tensor_28402], outputs = [Tensor_28404], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_28404: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13742: 19365888 Bnn)
Operation_13902: operator = BatchedMatmulBackward, inputs = [Tensor_28404, Tensor_28189, Tensor_28185], outputs = [Tensor_28406, Tensor_28407], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_28406: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (28406: 59610624 Ban)
Tensor_28407: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13631: 19365888 Bnn)
Operation_13905: operator = TypecastBackward, inputs = [Tensor_28406], outputs = [Tensor_28409], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_28409: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11920: 119221248 Brn)
Operation_13909: operator = SoftmaxBackward, inputs = [Tensor_28409, Tensor_28188], outputs = [Tensor_28411], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dim: 2]
Tensor_28411: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (28411: 119221248 Ban)
Operation_13907: operator = ReshapeBackward, inputs = [Tensor_28407], outputs = [Tensor_28413], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_28413: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13631: 19365888 Bnn)
Operation_13911: operator = ContiguousBackward, inputs = [Tensor_28413], outputs = [Tensor_28415], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_28415: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13627: 19365888 Bnn)
Operation_13913: operator = TypecastBackward, inputs = [Tensor_28411], outputs = [Tensor_28417], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_28417: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13641: 59610624 Bnn)
Operation_13917: operator = ReshapeBackward, inputs = [Tensor_28417], outputs = [Tensor_28419], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 197 stride: 38809 stride: 197 stride: 1]
Tensor_28419: shape = [128, 12, 197, 197], stride = [465708, 38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13641: 59610624 Bnn)
Operation_13921: operator = MaskedFillBackward, inputs = [Tensor_28419, Tensor_28187], outputs = [Tensor_28421], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [value_float: -inf]
Tensor_28421: shape = [128, 12, 197, 197], stride = [465708, 38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13641: 59610624 Bnn)
Operation_13919: operator = ReshapeBackward, inputs = [Tensor_28415], outputs = [Tensor_28423], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 12 shape: 64 stride: 151296 stride: 768 stride: 64 stride: 1]
Tensor_28423: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13627: 19365888 Bnn)
Operation_13923: operator = LinearBackward, inputs = [Tensor_28423, Tensor_28182, Tensor_28183], outputs = [Tensor_28425, Tensor_28426, Tensor_28430], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->21909
  - Operand: []
Tensor_28425: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13692: 19365888 Bnn)
Tensor_28426: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (13678: 1179648 Bnn)
Operation_21909: operator = AllReduceDuplicated, inputs = [Tensor_28426, nullptr], outputs = [Tensor_28429, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 13923->this
  - Operand: [subop: SUM tag: 21908]
Operation_28428: operator = Consume, inputs = [Tensor_28426], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_28429: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13678: 1179648 Bnn)
Tensor_28430: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (28433: 1536 Ban)
Operation_21913: operator = AllReduceDuplicated, inputs = [Tensor_28430, nullptr], outputs = [Tensor_28433, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: SUM tag: 21912]
Operation_28432: operator = Consume, inputs = [Tensor_28430], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_28433: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28433: 1536 Bnn)
Operation_13929: operator = TypecastBackward, inputs = [Tensor_28425], outputs = [Tensor_28435], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_28435: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (28402: 38731776 Bnn)
Operation_13927: operator = ReshapeBackward, inputs = [Tensor_28421], outputs = [Tensor_28437], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 197 stride: 465708 stride: 38809 stride: 197 stride: 1]
Tensor_28437: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13641: 59610624 Bnr)
Operation_13931: operator = BatchedMatmulBackward, inputs = [Tensor_28437, Tensor_28184, Tensor_28186], outputs = [Tensor_28439, Tensor_28440], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_28439: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13688: 19365888 Bnn)
Tensor_28440: shape = [1536, 64, 197], stride = [12608, 1, 64], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13692: 19365888 Bnn)
Operation_13934: operator = ReshapeBackward, inputs = [Tensor_28439], outputs = [Tensor_28442], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_28442: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13688: 19365888 Bnr)
Operation_13938: operator = ContiguousBackward, inputs = [Tensor_28442], outputs = [Tensor_28444], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_28444: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13736: 19365888 Bnn)
Operation_13940: operator = ReshapeBackward, inputs = [Tensor_28440], outputs = [Tensor_28446], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_28446: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13692: 19365888 Bnn)
Operation_13944: operator = ContiguousBackward, inputs = [Tensor_28446], outputs = [Tensor_28448], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_28448: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13611: 19365888 Bnn)
Operation_13946: operator = ReshapeBackward, inputs = [Tensor_28444], outputs = [Tensor_28450], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 12 shape: 64 stride: 151296 stride: 768 stride: 64 stride: 1]
Tensor_28450: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13736: 19365888 Bnn)
Operation_13950: operator = ElemwiseUnaryBackward, inputs = [Tensor_28450, nullptr, nullptr], outputs = [Tensor_28452], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 0.125]
Tensor_28452: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13736: 19365888 Bnr)
Operation_13954: operator = LinearBackward, inputs = [Tensor_28452, Tensor_28178, Tensor_28179], outputs = [Tensor_28454, Tensor_28455, Tensor_28459], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->21938
  - Operand: []
Tensor_28454: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13692: 19365888 Bnn)
Tensor_28455: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (13613: 1179648 Bnn)
Operation_21938: operator = AllReduceDuplicated, inputs = [Tensor_28455, nullptr], outputs = [Tensor_28458, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 13954->this
  - Operand: [subop: SUM tag: 21937]
Operation_28457: operator = Consume, inputs = [Tensor_28455], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_28458: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13613: 1179648 Bnn)
Tensor_28459: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (28462: 1536 Ban)
Operation_21942: operator = AllReduceDuplicated, inputs = [Tensor_28459, nullptr], outputs = [Tensor_28462, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: SUM tag: 21941]
Operation_28461: operator = Consume, inputs = [Tensor_28459], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_28462: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28462: 1536 Bnn)
Operation_13952: operator = ReshapeBackward, inputs = [Tensor_28448], outputs = [Tensor_28464], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 12 shape: 64 stride: 151296 stride: 768 stride: 64 stride: 1]
Tensor_28464: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13611: 19365888 Bnr)
Operation_13958: operator = LinearBackward, inputs = [Tensor_28464, Tensor_28180, Tensor_28181], outputs = [Tensor_28466, Tensor_28467, Tensor_28471], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->21950
  - Operand: []
Tensor_28466: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13625: 19365888 Bnn)
Tensor_28467: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (13597: 1179648 Bnn)
Operation_21950: operator = AllReduceDuplicated, inputs = [Tensor_28467, nullptr], outputs = [Tensor_28470, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 13958->this
  - Operand: [subop: SUM tag: 21949]
Operation_28469: operator = Consume, inputs = [Tensor_28467], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_28470: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13597: 1179648 Bnn)
Tensor_28471: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (28474: 1536 Ban)
Operation_21954: operator = AllReduceDuplicated, inputs = [Tensor_28471, nullptr], outputs = [Tensor_28474, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: SUM tag: 21953]
Operation_28473: operator = Consume, inputs = [Tensor_28471], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_28474: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28474: 1536 Bnn)
Operation_13962: operator = TypecastBackward, inputs = [Tensor_28454], outputs = [Tensor_28476], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_28476: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13663: 38731776 Bnn)
Operation_13964: operator = ElemwiseBinaryForward, inputs = [Tensor_28435, Tensor_28476], outputs = [Tensor_28478], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_28478: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (28402: 38731776 Bnn)
Operation_13966: operator = TypecastBackward, inputs = [Tensor_28466], outputs = [Tensor_28480], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_28480: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13663: 38731776 Bnn)
Operation_13968: operator = ElemwiseBinaryForward, inputs = [Tensor_28478, Tensor_28480], outputs = [Tensor_28482], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_28482: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (28402: 38731776 Bnn)
Operation_13970: operator = LayernormBackward, inputs = [Tensor_28482, Tensor_28174, Tensor_28175, Tensor_27723, Tensor_27724, Tensor_28176, Tensor_28177], outputs = [Tensor_28484, Tensor_28485, Tensor_28489], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->21968
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_28484: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13663: 38731776 Bnn)
Tensor_28485: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (28488: 3072 Ban)
Operation_21968: operator = AllReduceDuplicated, inputs = [Tensor_28485, nullptr], outputs = [Tensor_28488, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 13970->this
  - Operand: [subop: SUM tag: 21967]
Operation_28487: operator = Consume, inputs = [Tensor_28485], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_28488: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28488: 3072 Bnn)
Tensor_28489: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (28492: 3072 Ban)
Operation_21972: operator = AllReduceDuplicated, inputs = [Tensor_28489, nullptr], outputs = [Tensor_28492, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: SUM tag: 21971]
Operation_28491: operator = Consume, inputs = [Tensor_28489], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_28492: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28492: 3072 Bnn)
Operation_13972: operator = ElemwiseBinaryForward, inputs = [Tensor_28357, Tensor_28484], outputs = [Tensor_28494], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_28494: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (28289: 38731776 Bnn)
Operation_13979: operator = ElemwiseUnaryBackward, inputs = [Tensor_28494, nullptr, nullptr], outputs = [Tensor_28496], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 1]
Tensor_28496: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (28402: 38731776 Bnn)
Operation_13981: operator = TypecastBackward, inputs = [Tensor_28494], outputs = [Tensor_28498], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_28498: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13583: 19365888 Bnn)
Operation_13983: operator = ElemwiseBinaryBackward, inputs = [Tensor_28498, Tensor_28172, Tensor_28173, nullptr], outputs = [Tensor_28500, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_MUL]
Tensor_28500: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13583: 19365888 Bnn)
Operation_13985: operator = ElemwiseUnaryBackward, inputs = [Tensor_28500, nullptr, nullptr], outputs = [Tensor_28502], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_DIV scal1_float: 0.863636374]
Tensor_28502: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13583: 19365888 Bnn)
Operation_13987: operator = ReshapeBackward, inputs = [Tensor_28502], outputs = [Tensor_28504], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 151296 stride: 768 stride: 1]
Tensor_28504: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13583: 19365888 Bnn)
Operation_13989: operator = LinearBackward, inputs = [Tensor_28504, Tensor_28170, Tensor_28171], outputs = [Tensor_28506, Tensor_28507, Tensor_28511], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->21990
  - Operand: []
Tensor_28506: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13709: 77463552 Bnn)
Tensor_28507: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (13706: 4718592 Bnn)
Operation_21990: operator = AllReduceDuplicated, inputs = [Tensor_28507, nullptr], outputs = [Tensor_28510, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 13989->this
  - Operand: [subop: SUM tag: 21989]
Operation_28509: operator = Consume, inputs = [Tensor_28507], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_28510: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13706: 4718592 Bnn)
Tensor_28511: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (28514: 1536 Ban)
Operation_21994: operator = AllReduceDuplicated, inputs = [Tensor_28511, nullptr], outputs = [Tensor_28514, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: SUM tag: 21993]
Operation_28513: operator = Consume, inputs = [Tensor_28511], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_28514: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28514: 1536 Bnn)
Operation_13993: operator = TypecastBackward, inputs = [Tensor_28506], outputs = [Tensor_28516], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_28516: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13719: 154927104 Bnn)
Operation_13995: operator = LayernormBackward, inputs = [Tensor_28516, Tensor_28166, Tensor_28167, Tensor_27717, Tensor_27718, Tensor_28168, Tensor_28169], outputs = [Tensor_28518, Tensor_28519, Tensor_28523], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->22002
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_28518: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (28313: 154927104 Bnn)
Tensor_28519: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (28522: 12288 Ban)
Operation_22002: operator = AllReduceDuplicated, inputs = [Tensor_28519, nullptr], outputs = [Tensor_28522, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 13995->this
  - Operand: [subop: SUM tag: 22001]
Operation_28521: operator = Consume, inputs = [Tensor_28519], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_28522: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28522: 12288 Bnn)
Tensor_28523: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (28526: 12288 Ban)
Operation_22006: operator = AllReduceDuplicated, inputs = [Tensor_28523, nullptr], outputs = [Tensor_28526, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: SUM tag: 22005]
Operation_28525: operator = Consume, inputs = [Tensor_28523], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_28526: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28526: 12288 Bnn)
Operation_13999: operator = TypecastBackward, inputs = [Tensor_28518], outputs = [Tensor_28528], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_28528: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12874: 77463552 Bnn)
Operation_14001: operator = TypecastBackward, inputs = [Tensor_28528], outputs = [Tensor_28530], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_28530: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13713: 154927104 Bnn)
Operation_14003: operator = ElemwiseUnaryBackward, inputs = [Tensor_28530, Tensor_28164, Tensor_28165], outputs = [Tensor_28532], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_GELU]
Tensor_28532: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13713: 154927104 Bnn)
Operation_14005: operator = TypecastBackward, inputs = [Tensor_28532], outputs = [Tensor_28534], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_28534: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13510: 77463552 Bnn)
Operation_14007: operator = LinearBackward, inputs = [Tensor_28534, Tensor_28162, Tensor_28163], outputs = [Tensor_28536, Tensor_28537, Tensor_28541], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->22020
  - Operand: []
Tensor_28536: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13603: 19365888 Bnn)
Tensor_28537: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (13566: 4718592 Bnn)
Operation_22020: operator = AllReduceDuplicated, inputs = [Tensor_28537, nullptr], outputs = [Tensor_28540, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 14007->this
  - Operand: [subop: SUM tag: 22019]
Operation_28539: operator = Consume, inputs = [Tensor_28537], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_28540: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13566: 4718592 Bnn)
Tensor_28541: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (28544: 6144 Ban)
Operation_22024: operator = AllReduceDuplicated, inputs = [Tensor_28541, nullptr], outputs = [Tensor_28544, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: SUM tag: 22023]
Operation_28543: operator = Consume, inputs = [Tensor_28541], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_28544: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28544: 6144 Bnn)
Operation_14011: operator = TypecastBackward, inputs = [Tensor_28536], outputs = [Tensor_28546], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_28546: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13589: 38731776 Bnn)
Operation_14013: operator = ReshapeBackward, inputs = [Tensor_28546], outputs = [Tensor_28548], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 25216 shape: 768 stride: 768 stride: 1]
Tensor_28548: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13589: 38731776 Bnn)
Operation_14015: operator = LayernormBackward, inputs = [Tensor_28548, Tensor_28158, Tensor_28159, Tensor_27719, Tensor_27720, Tensor_28160, Tensor_28161], outputs = [Tensor_28550, Tensor_28551, Tensor_28555], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->22034
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_28550: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13587: 38731776 Bnn)
Tensor_28551: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (28554: 3072 Ban)
Operation_22034: operator = AllReduceDuplicated, inputs = [Tensor_28551, nullptr], outputs = [Tensor_28554, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 14015->this
  - Operand: [subop: SUM tag: 22033]
Operation_28553: operator = Consume, inputs = [Tensor_28551], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_28554: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28554: 3072 Bnn)
Tensor_28555: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (28558: 3072 Ban)
Operation_22038: operator = AllReduceDuplicated, inputs = [Tensor_28555, nullptr], outputs = [Tensor_28558, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: SUM tag: 22037]
Operation_28557: operator = Consume, inputs = [Tensor_28555], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_28558: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28558: 3072 Bnn)
Operation_14017: operator = ElemwiseBinaryForward, inputs = [Tensor_28496, Tensor_28550], outputs = [Tensor_28560], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_28560: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (28402: 38731776 Bnn)
Operation_14024: operator = ElemwiseUnaryBackward, inputs = [Tensor_28560, nullptr, nullptr], outputs = [Tensor_28562], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 1]
Tensor_28562: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (28289: 38731776 Bnn)
Operation_14026: operator = TypecastBackward, inputs = [Tensor_28560], outputs = [Tensor_28564], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_28564: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13533: 19365888 Bnn)
Operation_14028: operator = ElemwiseBinaryBackward, inputs = [Tensor_28564, Tensor_28156, Tensor_28157, nullptr], outputs = [Tensor_28566, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_MUL]
Tensor_28566: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13533: 19365888 Bnn)
Operation_14030: operator = ElemwiseUnaryBackward, inputs = [Tensor_28566, nullptr, nullptr], outputs = [Tensor_28568], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_DIV scal1_float: 0.863636374]
Tensor_28568: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13533: 19365888 Bnn)
Operation_14032: operator = LinearBackward, inputs = [Tensor_28568, Tensor_28154, Tensor_28155], outputs = [Tensor_28570, Tensor_28571, Tensor_28575], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->22054
  - Operand: []
Tensor_28570: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13692: 19365888 Bnn)
Tensor_28571: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (13605: 1179648 Bnn)
Operation_22054: operator = AllReduceDuplicated, inputs = [Tensor_28571, nullptr], outputs = [Tensor_28574, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 14032->this
  - Operand: [subop: SUM tag: 22053]
Operation_28573: operator = Consume, inputs = [Tensor_28571], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_28574: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13605: 1179648 Bnn)
Tensor_28575: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (28578: 1536 Ban)
Operation_22058: operator = AllReduceDuplicated, inputs = [Tensor_28575, nullptr], outputs = [Tensor_28578, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: SUM tag: 22057]
Operation_28577: operator = Consume, inputs = [Tensor_28575], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_28578: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28578: 1536 Bnn)
Operation_14036: operator = TypecastBackward, inputs = [Tensor_28570], outputs = [Tensor_28580], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_28580: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13587: 38731776 Bnn)
Tensor_28581: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13748: 38731776 Brn)
Operation_14038: operator = LayernormBackward, inputs = [Tensor_28580, Tensor_28581, Tensor_28151, Tensor_27713, Tensor_27714, Tensor_28152, Tensor_28153], outputs = [Tensor_28583, Tensor_28584, Tensor_28588], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->22068
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_28583: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (28402: 38731776 Bnn)
Tensor_28584: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (28587: 3072 Ban)
Operation_22068: operator = AllReduceDuplicated, inputs = [Tensor_28584, nullptr], outputs = [Tensor_28587, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 14038->this
  - Operand: [subop: SUM tag: 22067]
Operation_28586: operator = Consume, inputs = [Tensor_28584], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_28587: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28587: 3072 Bnn)
Tensor_28588: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (28591: 3072 Ban)
Operation_22072: operator = AllReduceDuplicated, inputs = [Tensor_28588, nullptr], outputs = [Tensor_28591, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: SUM tag: 22071]
Operation_28590: operator = Consume, inputs = [Tensor_28588], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_28591: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28591: 3072 Bnn)
Operation_22075: operator = AllGatherDuplicated, inputs = [Tensor_28583], outputs = [Tensor_28594], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->24064
  - Operand: [tag: 22074]
Operation_28593: operator = Consume, inputs = [Tensor_28583], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_28594: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13669: 77463552 Bnn)
Operation_23670: operator = DistributeWithMemAllocForward, inputs = [Tensor_27830], outputs = [Tensor_30221], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Operation_23871: operator = DistributeWithMemAllocForward, inputs = [Tensor_27790], outputs = [Tensor_30426], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Operation_24064: operator = DistributeWithMemAllocForward, inputs = [Tensor_27754], outputs = [Tensor_30623], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 22075->this
  - Operand: []
Operation_24200: operator = DistributeWithMemAllocForward, inputs = [Tensor_27731], outputs = [Tensor_30763], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Operation_15651: operator = TypecastBackward, inputs = [Tensor_28269], outputs = [Tensor_30773], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Operation_14042: operator = TypecastBackward, inputs = [Tensor_28594], outputs = [Tensor_28601], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_28601: shape = [128, 197, 768], stride = [768, 98304, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13748: 38731776 Bnn)
Operation_14046: operator = ReshapeBackward, inputs = [Tensor_28601], outputs = [Tensor_28603], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 197 shape: 128 shape: 768 stride: 98304 stride: 768 stride: 1]
Tensor_28603: shape = [197, 1536, 64], stride = [98304, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13748: 38731776 Bnn)
Operation_14048: operator = ContiguousBackward, inputs = [Tensor_28603], outputs = [Tensor_28605], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_28605: shape = [197, 1536, 64], stride = [64, 12608, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13663: 38731776 Bnn)
Operation_14050: operator = ReshapeBackward, inputs = [Tensor_28605], outputs = [Tensor_28607], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 197 shape: 1536 shape: 64 stride: 64 stride: 12608 stride: 1]
Tensor_28607: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13663: 38731776 Bnn)
Operation_22085: operator = DistributeWithMemAllocForward, inputs = [Tensor_28607], outputs = [Tensor_28609], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_28609: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13603: 19365888 Bnn)
Operation_14052: operator = BatchedMatmulBackward, inputs = [Tensor_28609, Tensor_28149, Tensor_28145], outputs = [Tensor_28611, Tensor_28612], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_28611: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (28406: 59610624 Bnn)
Tensor_28612: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13625: 19365888 Bnn)
Operation_14055: operator = TypecastBackward, inputs = [Tensor_28611], outputs = [Tensor_28614], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_28614: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13649: 119221248 Bnn)
Operation_14059: operator = SoftmaxBackward, inputs = [Tensor_28614, Tensor_28148], outputs = [Tensor_28616], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dim: 2]
Tensor_28616: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11920: 119221248 Bnn)
Operation_14057: operator = ReshapeBackward, inputs = [Tensor_28612], outputs = [Tensor_28618], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_28618: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13625: 19365888 Bnn)
Operation_14061: operator = ContiguousBackward, inputs = [Tensor_28618], outputs = [Tensor_28620], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_28620: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13529: 19365888 Bnn)
Operation_14063: operator = TypecastBackward, inputs = [Tensor_28616], outputs = [Tensor_28622], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_28622: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (28406: 59610624 Bnn)
Operation_14067: operator = ReshapeBackward, inputs = [Tensor_28622], outputs = [Tensor_28624], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 197 stride: 38809 stride: 197 stride: 1]
Tensor_28624: shape = [128, 12, 197, 197], stride = [465708, 38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (28406: 59610624 Bnn)
Operation_14071: operator = MaskedFillBackward, inputs = [Tensor_28624, Tensor_28147], outputs = [Tensor_28626], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [value_float: -inf]
Tensor_28626: shape = [128, 12, 197, 197], stride = [465708, 38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (28406: 59610624 Bnn)
Operation_14069: operator = ReshapeBackward, inputs = [Tensor_28620], outputs = [Tensor_28628], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 12 shape: 64 stride: 151296 stride: 768 stride: 64 stride: 1]
Tensor_28628: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13529: 19365888 Bnn)
Operation_14073: operator = LinearBackward, inputs = [Tensor_28628, Tensor_28142, Tensor_28143], outputs = [Tensor_28630, Tensor_28631, Tensor_28635], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->22110
  - Operand: []
Tensor_28630: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13692: 19365888 Bnn)
Tensor_28631: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (13519: 1179648 Bnn)
Operation_22110: operator = AllReduceDuplicated, inputs = [Tensor_28631, nullptr], outputs = [Tensor_28634, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 14073->this
  - Operand: [subop: SUM tag: 22109]
Operation_28633: operator = Consume, inputs = [Tensor_28631], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_28634: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13519: 1179648 Bnn)
Tensor_28635: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (28638: 1536 Ban)
Operation_22114: operator = AllReduceDuplicated, inputs = [Tensor_28635, nullptr], outputs = [Tensor_28638, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: SUM tag: 22113]
Operation_28637: operator = Consume, inputs = [Tensor_28635], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_28638: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28638: 1536 Bnn)
Operation_14079: operator = TypecastBackward, inputs = [Tensor_28630], outputs = [Tensor_28640], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_28640: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13748: 38731776 Bnn)
Operation_14077: operator = ReshapeBackward, inputs = [Tensor_28626], outputs = [Tensor_28642], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 197 stride: 465708 stride: 38809 stride: 197 stride: 1]
Tensor_28642: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (28406: 59610624 Bnn)
Operation_14081: operator = BatchedMatmulBackward, inputs = [Tensor_28642, Tensor_28144, Tensor_28146], outputs = [Tensor_28644, Tensor_28645], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_28644: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13625: 19365888 Bnn)
Tensor_28645: shape = [1536, 64, 197], stride = [12608, 1, 64], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13692: 19365888 Bnn)
Operation_14084: operator = ReshapeBackward, inputs = [Tensor_28644], outputs = [Tensor_28647], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_28647: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13625: 19365888 Bnn)
Operation_14088: operator = ContiguousBackward, inputs = [Tensor_28647], outputs = [Tensor_28649], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_28649: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13529: 19365888 Bnn)
Operation_14090: operator = ReshapeBackward, inputs = [Tensor_28645], outputs = [Tensor_28651], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_28651: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13692: 19365888 Bnn)
Operation_14094: operator = ContiguousBackward, inputs = [Tensor_28651], outputs = [Tensor_28653], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_28653: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13742: 19365888 Bnn)
Operation_14096: operator = ReshapeBackward, inputs = [Tensor_28649], outputs = [Tensor_28655], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 12 shape: 64 stride: 151296 stride: 768 stride: 64 stride: 1]
Tensor_28655: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13529: 19365888 Bnn)
Operation_14100: operator = ElemwiseUnaryBackward, inputs = [Tensor_28655, nullptr, nullptr], outputs = [Tensor_28657], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 0.125]
Tensor_28657: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13529: 19365888 Bnn)
Operation_14104: operator = LinearBackward, inputs = [Tensor_28657, Tensor_28138, Tensor_28139], outputs = [Tensor_28659, Tensor_28660, Tensor_28664], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->22139
  - Operand: []
Tensor_28659: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13625: 19365888 Bnn)
Tensor_28660: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (13454: 1179648 Bnn)
Operation_22139: operator = AllReduceDuplicated, inputs = [Tensor_28660, nullptr], outputs = [Tensor_28663, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 14104->this
  - Operand: [subop: SUM tag: 22138]
Operation_28662: operator = Consume, inputs = [Tensor_28660], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_28663: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13454: 1179648 Bnn)
Tensor_28664: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (28667: 1536 Ban)
Operation_22143: operator = AllReduceDuplicated, inputs = [Tensor_28664, nullptr], outputs = [Tensor_28667, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: SUM tag: 22142]
Operation_28666: operator = Consume, inputs = [Tensor_28664], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_28667: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28667: 1536 Bnn)
Operation_14102: operator = ReshapeBackward, inputs = [Tensor_28653], outputs = [Tensor_28669], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 12 shape: 64 stride: 151296 stride: 768 stride: 64 stride: 1]
Tensor_28669: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13742: 19365888 Bnn)
Operation_14108: operator = LinearBackward, inputs = [Tensor_28669, Tensor_28140, Tensor_28141], outputs = [Tensor_28671, Tensor_28672, Tensor_28676], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->22151
  - Operand: []
Tensor_28671: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13692: 19365888 Bnn)
Tensor_28672: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (13438: 1179648 Bnn)
Operation_22151: operator = AllReduceDuplicated, inputs = [Tensor_28672, nullptr], outputs = [Tensor_28675, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 14108->this
  - Operand: [subop: SUM tag: 22150]
Operation_28674: operator = Consume, inputs = [Tensor_28672], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_28675: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13438: 1179648 Bnn)
Tensor_28676: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (28679: 1536 Ban)
Operation_22155: operator = AllReduceDuplicated, inputs = [Tensor_28676, nullptr], outputs = [Tensor_28679, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: SUM tag: 22154]
Operation_28678: operator = Consume, inputs = [Tensor_28676], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_28679: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28679: 1536 Bnn)
Operation_14112: operator = TypecastBackward, inputs = [Tensor_28659], outputs = [Tensor_28681], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_28681: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13663: 38731776 Bnn)
Operation_14114: operator = ElemwiseBinaryForward, inputs = [Tensor_28640, Tensor_28681], outputs = [Tensor_28683], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_28683: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13748: 38731776 Bnn)
Operation_14116: operator = TypecastBackward, inputs = [Tensor_28671], outputs = [Tensor_28685], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_28685: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13663: 38731776 Bnn)
Operation_14118: operator = ElemwiseBinaryForward, inputs = [Tensor_28683, Tensor_28685], outputs = [Tensor_28687], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_28687: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13748: 38731776 Bnn)
Operation_14120: operator = LayernormBackward, inputs = [Tensor_28687, Tensor_28134, Tensor_28135, Tensor_27715, Tensor_27716, Tensor_28136, Tensor_28137], outputs = [Tensor_28689, Tensor_28690, Tensor_28694], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->22169
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_28689: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13663: 38731776 Bnn)
Tensor_28690: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (28693: 3072 Ban)
Operation_22169: operator = AllReduceDuplicated, inputs = [Tensor_28690, nullptr], outputs = [Tensor_28693, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 14120->this
  - Operand: [subop: SUM tag: 22168]
Operation_28692: operator = Consume, inputs = [Tensor_28690], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_28693: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28693: 3072 Bnn)
Tensor_28694: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (28697: 3072 Ban)
Operation_22173: operator = AllReduceDuplicated, inputs = [Tensor_28694, nullptr], outputs = [Tensor_28697, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: SUM tag: 22172]
Operation_28696: operator = Consume, inputs = [Tensor_28694], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_28697: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28697: 3072 Bnn)
Operation_14122: operator = ElemwiseBinaryForward, inputs = [Tensor_28562, Tensor_28689], outputs = [Tensor_28699], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_28699: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (28289: 38731776 Bnn)
Operation_14129: operator = ElemwiseUnaryBackward, inputs = [Tensor_28699, nullptr, nullptr], outputs = [Tensor_28701], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 1]
Tensor_28701: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13430: 38731776 Bnn)
Operation_14131: operator = TypecastBackward, inputs = [Tensor_28699], outputs = [Tensor_28703], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_28703: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13529: 19365888 Bnn)
Operation_14133: operator = ElemwiseBinaryBackward, inputs = [Tensor_28703, Tensor_28132, Tensor_28133, nullptr], outputs = [Tensor_28705, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_MUL]
Tensor_28705: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13529: 19365888 Bnn)
Operation_14135: operator = ElemwiseUnaryBackward, inputs = [Tensor_28705, nullptr, nullptr], outputs = [Tensor_28707], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_DIV scal1_float: 0.877272725]
Tensor_28707: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13529: 19365888 Bnn)
Operation_14137: operator = ReshapeBackward, inputs = [Tensor_28707], outputs = [Tensor_28709], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 151296 stride: 768 stride: 1]
Tensor_28709: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13529: 19365888 Bnn)
Operation_14139: operator = LinearBackward, inputs = [Tensor_28709, Tensor_28130, Tensor_28131], outputs = [Tensor_28711, Tensor_28712, Tensor_28716], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->22191
  - Operand: []
Tensor_28711: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11942: 77463552 Bnr)
Tensor_28712: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (13547: 4718592 Bnn)
Operation_22191: operator = AllReduceDuplicated, inputs = [Tensor_28712, nullptr], outputs = [Tensor_28715, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 14139->this
  - Operand: [subop: SUM tag: 22190]
Operation_28714: operator = Consume, inputs = [Tensor_28712], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_28715: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13547: 4718592 Bnn)
Tensor_28716: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (28719: 1536 Ban)
Operation_22195: operator = AllReduceDuplicated, inputs = [Tensor_28716, nullptr], outputs = [Tensor_28719, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: SUM tag: 22194]
Operation_28718: operator = Consume, inputs = [Tensor_28716], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_28719: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28719: 1536 Bnn)
Operation_14143: operator = TypecastBackward, inputs = [Tensor_28711], outputs = [Tensor_28721], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_28721: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13711: 154927104 Bnr)
Operation_14145: operator = LayernormBackward, inputs = [Tensor_28721, Tensor_28126, Tensor_28127, Tensor_27709, Tensor_27710, Tensor_28128, Tensor_28129], outputs = [Tensor_28723, Tensor_28724, Tensor_28728], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->22203
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_28723: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13560: 154927104 Bnr)
Tensor_28724: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (28727: 12288 Ban)
Operation_22203: operator = AllReduceDuplicated, inputs = [Tensor_28724, nullptr], outputs = [Tensor_28727, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 14145->this
  - Operand: [subop: SUM tag: 22202]
Operation_28726: operator = Consume, inputs = [Tensor_28724], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_28727: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28727: 12288 Bnn)
Tensor_28728: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (28731: 12288 Ban)
Operation_22207: operator = AllReduceDuplicated, inputs = [Tensor_28728, nullptr], outputs = [Tensor_28731, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: SUM tag: 22206]
Operation_28730: operator = Consume, inputs = [Tensor_28728], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_28731: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28731: 12288 Bnn)
Operation_14149: operator = TypecastBackward, inputs = [Tensor_28723], outputs = [Tensor_28733], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_28733: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12715: 77463552 Bnn)
Operation_14151: operator = TypecastBackward, inputs = [Tensor_28733], outputs = [Tensor_28735], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_28735: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13558: 154927104 Bnn)
Operation_14153: operator = ElemwiseUnaryBackward, inputs = [Tensor_28735, Tensor_28124, Tensor_28125], outputs = [Tensor_28737], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_GELU]
Tensor_28737: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13558: 154927104 Bnr)
Operation_14155: operator = TypecastBackward, inputs = [Tensor_28737], outputs = [Tensor_28739], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_28739: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13192: 77463552 Bnn)
Operation_14157: operator = LinearBackward, inputs = [Tensor_28739, Tensor_28122, Tensor_28123], outputs = [Tensor_28741, Tensor_28742, Tensor_28746], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->22221
  - Operand: []
Tensor_28741: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13460: 19365888 Bnn)
Tensor_28742: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (13407: 4718592 Bnn)
Operation_22221: operator = AllReduceDuplicated, inputs = [Tensor_28742, nullptr], outputs = [Tensor_28745, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 14157->this
  - Operand: [subop: SUM tag: 22220]
Operation_28744: operator = Consume, inputs = [Tensor_28742], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_28745: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13407: 4718592 Bnn)
Tensor_28746: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (28749: 6144 Ban)
Operation_22225: operator = AllReduceDuplicated, inputs = [Tensor_28746, nullptr], outputs = [Tensor_28749, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: SUM tag: 22224]
Operation_28748: operator = Consume, inputs = [Tensor_28746], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_28749: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28749: 6144 Bnn)
Operation_14161: operator = TypecastBackward, inputs = [Tensor_28741], outputs = [Tensor_28751], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_28751: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13748: 38731776 Bnn)
Operation_14163: operator = ReshapeBackward, inputs = [Tensor_28751], outputs = [Tensor_28753], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 25216 shape: 768 stride: 768 stride: 1]
Tensor_28753: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13748: 38731776 Bnn)
Operation_14165: operator = LayernormBackward, inputs = [Tensor_28753, Tensor_28118, Tensor_28119, Tensor_27711, Tensor_27712, Tensor_28120, Tensor_28121], outputs = [Tensor_28755, Tensor_28756, Tensor_28760], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->22235
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_28755: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13428: 38731776 Bnn)
Tensor_28756: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (28759: 3072 Ban)
Operation_22235: operator = AllReduceDuplicated, inputs = [Tensor_28756, nullptr], outputs = [Tensor_28759, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 14165->this
  - Operand: [subop: SUM tag: 22234]
Operation_28758: operator = Consume, inputs = [Tensor_28756], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_28759: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28759: 3072 Bnn)
Tensor_28760: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (28763: 3072 Ban)
Operation_22239: operator = AllReduceDuplicated, inputs = [Tensor_28760, nullptr], outputs = [Tensor_28763, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: SUM tag: 22238]
Operation_28762: operator = Consume, inputs = [Tensor_28760], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_28763: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28763: 3072 Bnn)
Operation_14167: operator = ElemwiseBinaryForward, inputs = [Tensor_28701, Tensor_28755], outputs = [Tensor_28765], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_28765: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13430: 38731776 Bnn)
Operation_14174: operator = ElemwiseUnaryBackward, inputs = [Tensor_28765, nullptr, nullptr], outputs = [Tensor_28767], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 1]
Tensor_28767: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (28289: 38731776 Bnn)
Operation_14176: operator = TypecastBackward, inputs = [Tensor_28765], outputs = [Tensor_28769], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_28769: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13583: 19365888 Bnn)
Operation_14178: operator = ElemwiseBinaryBackward, inputs = [Tensor_28769, Tensor_28116, Tensor_28117, nullptr], outputs = [Tensor_28771, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_MUL]
Tensor_28771: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13583: 19365888 Bnn)
Operation_14180: operator = ElemwiseUnaryBackward, inputs = [Tensor_28771, nullptr, nullptr], outputs = [Tensor_28773], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_DIV scal1_float: 0.877272725]
Tensor_28773: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13583: 19365888 Bnn)
Operation_14182: operator = LinearBackward, inputs = [Tensor_28773, Tensor_28114, Tensor_28115], outputs = [Tensor_28775, Tensor_28776, Tensor_28780], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->22255
  - Operand: []
Tensor_28775: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13631: 19365888 Bnn)
Tensor_28776: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (13446: 1179648 Bnn)
Operation_22255: operator = AllReduceDuplicated, inputs = [Tensor_28776, nullptr], outputs = [Tensor_28779, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 14182->this
  - Operand: [subop: SUM tag: 22254]
Operation_28778: operator = Consume, inputs = [Tensor_28776], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_28779: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13446: 1179648 Bnn)
Tensor_28780: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (28783: 1536 Ban)
Operation_22259: operator = AllReduceDuplicated, inputs = [Tensor_28780, nullptr], outputs = [Tensor_28783, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: SUM tag: 22258]
Operation_28782: operator = Consume, inputs = [Tensor_28780], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_28783: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28783: 1536 Bnn)
Operation_14186: operator = TypecastBackward, inputs = [Tensor_28775], outputs = [Tensor_28785], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_28785: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13663: 38731776 Bnn)
Tensor_28786: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (28786: 38731776 Ban)
Operation_14188: operator = LayernormBackward, inputs = [Tensor_28785, Tensor_28786, Tensor_28111, Tensor_27705, Tensor_27706, Tensor_28112, Tensor_28113], outputs = [Tensor_28788, Tensor_28789, Tensor_28793], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->22269
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_28788: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13345: 38731776 Bnn)
Tensor_28789: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (28792: 3072 Ban)
Operation_22269: operator = AllReduceDuplicated, inputs = [Tensor_28789, nullptr], outputs = [Tensor_28792, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 14188->this
  - Operand: [subop: SUM tag: 22268]
Operation_28791: operator = Consume, inputs = [Tensor_28789], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_28792: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28792: 3072 Bnn)
Tensor_28793: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (28796: 3072 Ban)
Operation_22273: operator = AllReduceDuplicated, inputs = [Tensor_28793, nullptr], outputs = [Tensor_28796, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: SUM tag: 22272]
Operation_28795: operator = Consume, inputs = [Tensor_28793], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_28796: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28796: 3072 Bnn)
Operation_22276: operator = AllGatherDuplicated, inputs = [Tensor_28788], outputs = [Tensor_28799], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->14192
  - Operand: [tag: 22275]
Operation_28798: operator = Consume, inputs = [Tensor_28788], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_28799: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13669: 77463552 Bnn)
Operation_15653: operator = TypecastBackward, inputs = [Tensor_28273], outputs = [Tensor_30774], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Operation_15655: operator = TypecastBackward, inputs = [Tensor_28305], outputs = [Tensor_30775], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Operation_15657: operator = TypecastBackward, inputs = [Tensor_28309], outputs = [Tensor_30776], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Operation_15659: operator = TypecastBackward, inputs = [Tensor_28335], outputs = [Tensor_30777], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Operation_15661: operator = TypecastBackward, inputs = [Tensor_28339], outputs = [Tensor_30778], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Operation_14192: operator = TypecastBackward, inputs = [Tensor_28799], outputs = [Tensor_28806], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 22276->this
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_28806: shape = [128, 197, 768], stride = [768, 98304, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13748: 38731776 Bnn)
Operation_14196: operator = ReshapeBackward, inputs = [Tensor_28806], outputs = [Tensor_28808], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 197 shape: 128 shape: 768 stride: 98304 stride: 768 stride: 1]
Tensor_28808: shape = [197, 1536, 64], stride = [98304, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13748: 38731776 Bnn)
Operation_14198: operator = ContiguousBackward, inputs = [Tensor_28808], outputs = [Tensor_28810], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_28810: shape = [197, 1536, 64], stride = [64, 12608, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13430: 38731776 Bnn)
Operation_14200: operator = ReshapeBackward, inputs = [Tensor_28810], outputs = [Tensor_28812], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 197 shape: 1536 shape: 64 stride: 64 stride: 12608 stride: 1]
Tensor_28812: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13430: 38731776 Bnn)
Operation_22286: operator = DistributeWithMemAllocForward, inputs = [Tensor_28812], outputs = [Tensor_28814], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_28814: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13460: 19365888 Bnn)
Operation_14202: operator = BatchedMatmulBackward, inputs = [Tensor_28814, Tensor_28109, Tensor_28105], outputs = [Tensor_28816, Tensor_28817], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_28816: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (28406: 59610624 Bnn)
Tensor_28817: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13742: 19365888 Bnn)
Operation_14205: operator = TypecastBackward, inputs = [Tensor_28816], outputs = [Tensor_28819], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_28819: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13649: 119221248 Bnn)
Operation_14209: operator = SoftmaxBackward, inputs = [Tensor_28819, Tensor_28108], outputs = [Tensor_28821], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dim: 2]
Tensor_28821: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13490: 119221248 Bnn)
Operation_14207: operator = ReshapeBackward, inputs = [Tensor_28817], outputs = [Tensor_28823], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_28823: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13742: 19365888 Bnn)
Operation_14211: operator = ContiguousBackward, inputs = [Tensor_28823], outputs = [Tensor_28825], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_28825: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13424: 19365888 Bnn)
Operation_14213: operator = TypecastBackward, inputs = [Tensor_28821], outputs = [Tensor_28827], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_28827: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (28406: 59610624 Bnn)
Operation_14217: operator = ReshapeBackward, inputs = [Tensor_28827], outputs = [Tensor_28829], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 197 stride: 38809 stride: 197 stride: 1]
Tensor_28829: shape = [128, 12, 197, 197], stride = [465708, 38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (28406: 59610624 Bnn)
Operation_14221: operator = MaskedFillBackward, inputs = [Tensor_28829, Tensor_28107], outputs = [Tensor_28831], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [value_float: -inf]
Tensor_28831: shape = [128, 12, 197, 197], stride = [465708, 38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (28406: 59610624 Bnn)
Operation_14219: operator = ReshapeBackward, inputs = [Tensor_28825], outputs = [Tensor_28833], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 12 shape: 64 stride: 151296 stride: 768 stride: 64 stride: 1]
Tensor_28833: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13424: 19365888 Bnn)
Operation_14223: operator = LinearBackward, inputs = [Tensor_28833, Tensor_28102, Tensor_28103], outputs = [Tensor_28835, Tensor_28836, Tensor_28840], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->22311
  - Operand: []
Tensor_28835: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13742: 19365888 Bnn)
Tensor_28836: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (13360: 1179648 Bnn)
Operation_22311: operator = AllReduceDuplicated, inputs = [Tensor_28836, nullptr], outputs = [Tensor_28839, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 14223->this
  - Operand: [subop: SUM tag: 22310]
Operation_28838: operator = Consume, inputs = [Tensor_28836], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_28839: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13360: 1179648 Bnn)
Tensor_28840: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (11821: 1536 Bnn)
Operation_22315: operator = AllReduceDuplicated, inputs = [Tensor_28840, nullptr], outputs = [Tensor_28843, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: SUM tag: 22314]
Operation_28842: operator = Consume, inputs = [Tensor_28840], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_28843: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11821: 1536 Bnn)
Operation_14229: operator = TypecastBackward, inputs = [Tensor_28835], outputs = [Tensor_28845], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_28845: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13428: 38731776 Bnn)
Operation_14227: operator = ReshapeBackward, inputs = [Tensor_28831], outputs = [Tensor_28847], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 197 stride: 465708 stride: 38809 stride: 197 stride: 1]
Tensor_28847: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (28406: 59610624 Bnr)
Operation_14231: operator = BatchedMatmulBackward, inputs = [Tensor_28847, Tensor_28104, Tensor_28106], outputs = [Tensor_28849, Tensor_28850], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_28849: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13418: 19365888 Bnn)
Tensor_28850: shape = [1536, 64, 197], stride = [12608, 1, 64], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13742: 19365888 Bnn)
Operation_14234: operator = ReshapeBackward, inputs = [Tensor_28849], outputs = [Tensor_28852], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_28852: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13418: 19365888 Bnn)
Operation_14238: operator = ContiguousBackward, inputs = [Tensor_28852], outputs = [Tensor_28854], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_28854: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13460: 19365888 Bnn)
Operation_14240: operator = ReshapeBackward, inputs = [Tensor_28850], outputs = [Tensor_28856], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_28856: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13742: 19365888 Bnr)
Operation_14244: operator = ContiguousBackward, inputs = [Tensor_28856], outputs = [Tensor_28858], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_28858: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13529: 19365888 Bnn)
Operation_14246: operator = ReshapeBackward, inputs = [Tensor_28854], outputs = [Tensor_28860], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 12 shape: 64 stride: 151296 stride: 768 stride: 64 stride: 1]
Tensor_28860: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13460: 19365888 Bnn)
Operation_14250: operator = ElemwiseUnaryBackward, inputs = [Tensor_28860, nullptr, nullptr], outputs = [Tensor_28862], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 0.125]
Tensor_28862: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13460: 19365888 Bnn)
Operation_14254: operator = LinearBackward, inputs = [Tensor_28862, Tensor_28098, Tensor_28099], outputs = [Tensor_28864, Tensor_28865, Tensor_28869], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->22340
  - Operand: []
Tensor_28864: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13418: 19365888 Bnn)
Tensor_28865: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (13295: 1179648 Bnn)
Operation_22340: operator = AllReduceDuplicated, inputs = [Tensor_28865, nullptr], outputs = [Tensor_28868, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 14254->this
  - Operand: [subop: SUM tag: 22339]
Operation_28867: operator = Consume, inputs = [Tensor_28865], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_28868: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13295: 1179648 Bnr)
Tensor_28869: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (28872: 1536 Ban)
Operation_22344: operator = AllReduceDuplicated, inputs = [Tensor_28869, nullptr], outputs = [Tensor_28872, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: SUM tag: 22343]
Operation_28871: operator = Consume, inputs = [Tensor_28869], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_28872: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28872: 1536 Bnn)
Operation_14252: operator = ReshapeBackward, inputs = [Tensor_28858], outputs = [Tensor_28874], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 12 shape: 64 stride: 151296 stride: 768 stride: 64 stride: 1]
Tensor_28874: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13529: 19365888 Bnr)
Operation_14258: operator = LinearBackward, inputs = [Tensor_28874, Tensor_28100, Tensor_28101], outputs = [Tensor_28876, Tensor_28877, Tensor_28881], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->22352
  - Operand: []
Tensor_28876: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13370: 19365888 Bnn)
Tensor_28877: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (13279: 1179648 Bnn)
Operation_22352: operator = AllReduceDuplicated, inputs = [Tensor_28877, nullptr], outputs = [Tensor_28880, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 14258->this
  - Operand: [subop: SUM tag: 22351]
Operation_28879: operator = Consume, inputs = [Tensor_28877], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_28880: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13279: 1179648 Bnr)
Tensor_28881: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (28884: 1536 Ban)
Operation_22356: operator = AllReduceDuplicated, inputs = [Tensor_28881, nullptr], outputs = [Tensor_28884, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: SUM tag: 22355]
Operation_28883: operator = Consume, inputs = [Tensor_28881], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_28884: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28884: 1536 Bnn)
Operation_14262: operator = TypecastBackward, inputs = [Tensor_28864], outputs = [Tensor_28886], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_28886: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13349: 38731776 Bnn)
Operation_14264: operator = ElemwiseBinaryForward, inputs = [Tensor_28845, Tensor_28886], outputs = [Tensor_28888], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_28888: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13428: 38731776 Bnn)
Operation_14266: operator = TypecastBackward, inputs = [Tensor_28876], outputs = [Tensor_28890], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_28890: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13349: 38731776 Bnn)
Operation_14268: operator = ElemwiseBinaryForward, inputs = [Tensor_28888, Tensor_28890], outputs = [Tensor_28892], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_28892: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13428: 38731776 Bnn)
Operation_14270: operator = LayernormBackward, inputs = [Tensor_28892, Tensor_28094, Tensor_28095, Tensor_27707, Tensor_27708, Tensor_28096, Tensor_28097], outputs = [Tensor_28894, Tensor_28895, Tensor_28899], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->22370
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_28894: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13349: 38731776 Bnn)
Tensor_28895: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (28898: 3072 Ban)
Operation_22370: operator = AllReduceDuplicated, inputs = [Tensor_28895, nullptr], outputs = [Tensor_28898, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 14270->this
  - Operand: [subop: SUM tag: 22369]
Operation_28897: operator = Consume, inputs = [Tensor_28895], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_28898: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28898: 3072 Bnn)
Tensor_28899: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (28902: 3072 Ban)
Operation_22374: operator = AllReduceDuplicated, inputs = [Tensor_28899, nullptr], outputs = [Tensor_28902, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: SUM tag: 22373]
Operation_28901: operator = Consume, inputs = [Tensor_28899], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_28902: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28902: 3072 Bnn)
Operation_14272: operator = ElemwiseBinaryForward, inputs = [Tensor_28767, Tensor_28894], outputs = [Tensor_28904], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_28904: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (28289: 38731776 Bnn)
Operation_14279: operator = ElemwiseUnaryBackward, inputs = [Tensor_28904, nullptr, nullptr], outputs = [Tensor_28906], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 1]
Tensor_28906: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13428: 38731776 Bnn)
Operation_14281: operator = TypecastBackward, inputs = [Tensor_28904], outputs = [Tensor_28908], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_28908: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13627: 19365888 Bnn)
Operation_14283: operator = ElemwiseBinaryBackward, inputs = [Tensor_28908, Tensor_28092, Tensor_28093, nullptr], outputs = [Tensor_28910, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_MUL]
Tensor_28910: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13627: 19365888 Bnn)
Operation_14285: operator = ElemwiseUnaryBackward, inputs = [Tensor_28910, nullptr, nullptr], outputs = [Tensor_28912], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_DIV scal1_float: 0.890909076]
Tensor_28912: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13627: 19365888 Bnn)
Operation_14287: operator = ReshapeBackward, inputs = [Tensor_28912], outputs = [Tensor_28914], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 151296 stride: 768 stride: 1]
Tensor_28914: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13627: 19365888 Bnr)
Operation_14289: operator = LinearBackward, inputs = [Tensor_28914, Tensor_28090, Tensor_28091], outputs = [Tensor_28916, Tensor_28917, Tensor_28921], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->22392
  - Operand: []
Tensor_28916: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13550: 77463552 Bnn)
Tensor_28917: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (28305: 4718592 Bnn)
Operation_22392: operator = AllReduceDuplicated, inputs = [Tensor_28917, nullptr], outputs = [Tensor_28920, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 14289->this
  - Operand: [subop: SUM tag: 22391]
Operation_28919: operator = Consume, inputs = [Tensor_28917], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_28920: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28305: 4718592 Bnn)
Tensor_28921: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (28924: 1536 Ban)
Operation_22396: operator = AllReduceDuplicated, inputs = [Tensor_28921, nullptr], outputs = [Tensor_28924, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: SUM tag: 22395]
Operation_28923: operator = Consume, inputs = [Tensor_28921], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_28924: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28924: 1536 Bnn)
Operation_14293: operator = TypecastBackward, inputs = [Tensor_28916], outputs = [Tensor_28926], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_28926: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13719: 154927104 Bnn)
Operation_14295: operator = LayernormBackward, inputs = [Tensor_28926, Tensor_28086, Tensor_28087, Tensor_27701, Tensor_27702, Tensor_28088, Tensor_28089], outputs = [Tensor_28928, Tensor_28929, Tensor_28933], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->22404
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_28928: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (28313: 154927104 Bnn)
Tensor_28929: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (28932: 12288 Ban)
Operation_22404: operator = AllReduceDuplicated, inputs = [Tensor_28929, nullptr], outputs = [Tensor_28932, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 14295->this
  - Operand: [subop: SUM tag: 22403]
Operation_28931: operator = Consume, inputs = [Tensor_28929], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_28932: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28932: 12288 Bnn)
Tensor_28933: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (28936: 12288 Ban)
Operation_22408: operator = AllReduceDuplicated, inputs = [Tensor_28933, nullptr], outputs = [Tensor_28936, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: SUM tag: 22407]
Operation_28935: operator = Consume, inputs = [Tensor_28933], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_28936: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28936: 12288 Bnn)
Operation_14299: operator = TypecastBackward, inputs = [Tensor_28928], outputs = [Tensor_28938], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_28938: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13033: 77463552 Bnn)
Operation_14301: operator = TypecastBackward, inputs = [Tensor_28938], outputs = [Tensor_28940], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_28940: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13552: 154927104 Bnn)
Operation_14303: operator = ElemwiseUnaryBackward, inputs = [Tensor_28940, Tensor_28084, Tensor_28085], outputs = [Tensor_28942], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_GELU]
Tensor_28942: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13552: 154927104 Bnr)
Operation_14305: operator = TypecastBackward, inputs = [Tensor_28942], outputs = [Tensor_28944], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_28944: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12874: 77463552 Bnn)
Operation_14307: operator = LinearBackward, inputs = [Tensor_28944, Tensor_28082, Tensor_28083], outputs = [Tensor_28946, Tensor_28947, Tensor_28951], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->22422
  - Operand: []
Tensor_28946: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13583: 19365888 Bnn)
Tensor_28947: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (13725: 4718592 Bnn)
Operation_22422: operator = AllReduceDuplicated, inputs = [Tensor_28947, nullptr], outputs = [Tensor_28950, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 14307->this
  - Operand: [subop: SUM tag: 22421]
Operation_28949: operator = Consume, inputs = [Tensor_28947], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_28950: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13725: 4718592 Bnr)
Tensor_28951: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (11969: 6144 Bnn)
Operation_22426: operator = AllReduceDuplicated, inputs = [Tensor_28951, nullptr], outputs = [Tensor_28954, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: SUM tag: 22425]
Operation_28953: operator = Consume, inputs = [Tensor_28951], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_28954: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11969: 6144 Bnn)
Operation_14311: operator = TypecastBackward, inputs = [Tensor_28946], outputs = [Tensor_28956], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_28956: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13349: 38731776 Bnn)
Operation_14313: operator = ReshapeBackward, inputs = [Tensor_28956], outputs = [Tensor_28958], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 25216 shape: 768 stride: 768 stride: 1]
Tensor_28958: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13349: 38731776 Bnn)
Operation_14315: operator = LayernormBackward, inputs = [Tensor_28958, Tensor_28078, Tensor_28079, Tensor_27703, Tensor_27704, Tensor_28080, Tensor_28081], outputs = [Tensor_28960, Tensor_28961, Tensor_28965], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->22436
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_28960: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (28786: 38731776 Bnn)
Tensor_28961: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (28964: 3072 Ban)
Operation_22436: operator = AllReduceDuplicated, inputs = [Tensor_28961, nullptr], outputs = [Tensor_28964, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 14315->this
  - Operand: [subop: SUM tag: 22435]
Operation_28963: operator = Consume, inputs = [Tensor_28961], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_28964: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28964: 3072 Bnn)
Tensor_28965: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (28968: 3072 Ban)
Operation_22440: operator = AllReduceDuplicated, inputs = [Tensor_28965, nullptr], outputs = [Tensor_28968, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: SUM tag: 22439]
Operation_28967: operator = Consume, inputs = [Tensor_28965], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_28968: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28968: 3072 Bnn)
Operation_14317: operator = ElemwiseBinaryForward, inputs = [Tensor_28906, Tensor_28960], outputs = [Tensor_28970], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_28970: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13428: 38731776 Bnn)
Operation_14324: operator = ElemwiseUnaryBackward, inputs = [Tensor_28970, nullptr, nullptr], outputs = [Tensor_28972], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 1]
Tensor_28972: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13663: 38731776 Bnn)
Operation_14326: operator = TypecastBackward, inputs = [Tensor_28970], outputs = [Tensor_28974], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_28974: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13285: 19365888 Bnn)
Operation_14328: operator = ElemwiseBinaryBackward, inputs = [Tensor_28974, Tensor_28076, Tensor_28077, nullptr], outputs = [Tensor_28976, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_MUL]
Tensor_28976: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13285: 19365888 Bnn)
Operation_14330: operator = ElemwiseUnaryBackward, inputs = [Tensor_28976, nullptr, nullptr], outputs = [Tensor_28978], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_DIV scal1_float: 0.890909076]
Tensor_28978: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13285: 19365888 Bnr)
Operation_14332: operator = LinearBackward, inputs = [Tensor_28978, Tensor_28074, Tensor_28075], outputs = [Tensor_28980, Tensor_28981, Tensor_28985], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->22456
  - Operand: []
Tensor_28980: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13370: 19365888 Bnn)
Tensor_28981: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (13287: 1179648 Bnn)
Operation_22456: operator = AllReduceDuplicated, inputs = [Tensor_28981, nullptr], outputs = [Tensor_28984, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 14332->this
  - Operand: [subop: SUM tag: 22455]
Operation_28983: operator = Consume, inputs = [Tensor_28981], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_28984: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13287: 1179648 Bnr)
Tensor_28985: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (28988: 1536 Ban)
Operation_22460: operator = AllReduceDuplicated, inputs = [Tensor_28985, nullptr], outputs = [Tensor_28988, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: SUM tag: 22459]
Operation_28987: operator = Consume, inputs = [Tensor_28985], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_28988: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28988: 1536 Bnn)
Operation_14336: operator = TypecastBackward, inputs = [Tensor_28980], outputs = [Tensor_28990], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_28990: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13345: 38731776 Bnn)
Tensor_28991: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (28991: 38731776 Ban)
Operation_14338: operator = LayernormBackward, inputs = [Tensor_28990, Tensor_28991, Tensor_28071, Tensor_27697, Tensor_27698, Tensor_28072, Tensor_28073], outputs = [Tensor_28993, Tensor_28994, Tensor_28998], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->22470
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_28993: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13748: 38731776 Bnn)
Tensor_28994: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (28997: 3072 Ban)
Operation_22470: operator = AllReduceDuplicated, inputs = [Tensor_28994, nullptr], outputs = [Tensor_28997, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 14338->this
  - Operand: [subop: SUM tag: 22469]
Operation_28996: operator = Consume, inputs = [Tensor_28994], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_28997: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28997: 3072 Bnn)
Tensor_28998: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (29001: 3072 Ban)
Operation_22474: operator = AllReduceDuplicated, inputs = [Tensor_28998, nullptr], outputs = [Tensor_29001, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: SUM tag: 22473]
Operation_29000: operator = Consume, inputs = [Tensor_28998], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29001: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29001: 3072 Bnn)
Operation_22477: operator = AllGatherDuplicated, inputs = [Tensor_28993], outputs = [Tensor_29004], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->14342
  - Operand: [tag: 22476]
Operation_29003: operator = Consume, inputs = [Tensor_28993], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29004: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12874: 77463552 Bnn)
Operation_15663: operator = TypecastBackward, inputs = [Tensor_28369], outputs = [Tensor_30779], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Operation_15665: operator = TypecastBackward, inputs = [Tensor_28373], outputs = [Tensor_30780], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Operation_15667: operator = TypecastBackward, inputs = [Tensor_28429], outputs = [Tensor_30781], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Operation_15669: operator = TypecastBackward, inputs = [Tensor_28433], outputs = [Tensor_30782], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Operation_15671: operator = TypecastBackward, inputs = [Tensor_28458], outputs = [Tensor_30783], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Operation_14342: operator = TypecastBackward, inputs = [Tensor_29004], outputs = [Tensor_29011], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 22477->this
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_29011: shape = [128, 197, 768], stride = [768, 98304, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13345: 38731776 Bnn)
Operation_14346: operator = ReshapeBackward, inputs = [Tensor_29011], outputs = [Tensor_29013], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 197 shape: 128 shape: 768 stride: 98304 stride: 768 stride: 1]
Tensor_29013: shape = [197, 1536, 64], stride = [98304, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13345: 38731776 Bnn)
Operation_14348: operator = ContiguousBackward, inputs = [Tensor_29013], outputs = [Tensor_29015], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29015: shape = [197, 1536, 64], stride = [64, 12608, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13271: 38731776 Bnn)
Operation_14350: operator = ReshapeBackward, inputs = [Tensor_29015], outputs = [Tensor_29017], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 197 shape: 1536 shape: 64 stride: 64 stride: 12608 stride: 1]
Tensor_29017: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13271: 38731776 Bnn)
Operation_22487: operator = DistributeWithMemAllocForward, inputs = [Tensor_29017], outputs = [Tensor_29019], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29019: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13583: 19365888 Bnn)
Operation_14352: operator = BatchedMatmulBackward, inputs = [Tensor_29019, Tensor_28069, Tensor_28065], outputs = [Tensor_29021, Tensor_29022], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29021: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13323: 59610624 Bnn)
Tensor_29022: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13418: 19365888 Bnn)
Operation_14355: operator = TypecastBackward, inputs = [Tensor_29021], outputs = [Tensor_29024], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_29024: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13649: 119221248 Bnn)
Operation_14359: operator = SoftmaxBackward, inputs = [Tensor_29024, Tensor_28068], outputs = [Tensor_29026], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dim: 2]
Tensor_29026: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13490: 119221248 Bnn)
Operation_14357: operator = ReshapeBackward, inputs = [Tensor_29022], outputs = [Tensor_29028], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_29028: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13418: 19365888 Bnn)
Operation_14361: operator = ContiguousBackward, inputs = [Tensor_29028], outputs = [Tensor_29030], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29030: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13309: 19365888 Bnn)
Operation_14363: operator = TypecastBackward, inputs = [Tensor_29026], outputs = [Tensor_29032], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_29032: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13164: 59610624 Bnn)
Operation_14367: operator = ReshapeBackward, inputs = [Tensor_29032], outputs = [Tensor_29034], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 197 stride: 38809 stride: 197 stride: 1]
Tensor_29034: shape = [128, 12, 197, 197], stride = [465708, 38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13164: 59610624 Bnn)
Operation_14371: operator = MaskedFillBackward, inputs = [Tensor_29034, Tensor_28067], outputs = [Tensor_29036], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [value_float: -inf]
Tensor_29036: shape = [128, 12, 197, 197], stride = [465708, 38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13164: 59610624 Bnn)
Operation_14369: operator = ReshapeBackward, inputs = [Tensor_29030], outputs = [Tensor_29038], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 12 shape: 64 stride: 151296 stride: 768 stride: 64 stride: 1]
Tensor_29038: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13309: 19365888 Bnr)
Operation_14373: operator = LinearBackward, inputs = [Tensor_29038, Tensor_28062, Tensor_28063], outputs = [Tensor_29040, Tensor_29041, Tensor_29045], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->22512
  - Operand: []
Tensor_29040: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13418: 19365888 Bnn)
Tensor_29041: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (13678: 1179648 Bnn)
Operation_22512: operator = AllReduceDuplicated, inputs = [Tensor_29041, nullptr], outputs = [Tensor_29044, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 14373->this
  - Operand: [subop: SUM tag: 22511]
Operation_29043: operator = Consume, inputs = [Tensor_29041], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29044: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13678: 1179648 Bnr)
Tensor_29045: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (28433: 1536 Bnn)
Operation_22516: operator = AllReduceDuplicated, inputs = [Tensor_29045, nullptr], outputs = [Tensor_29048, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: SUM tag: 22515]
Operation_29047: operator = Consume, inputs = [Tensor_29045], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29048: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28433: 1536 Bnn)
Operation_14379: operator = TypecastBackward, inputs = [Tensor_29040], outputs = [Tensor_29050], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_29050: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13271: 38731776 Bnn)
Operation_14377: operator = ReshapeBackward, inputs = [Tensor_29036], outputs = [Tensor_29052], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 197 stride: 465708 stride: 38809 stride: 197 stride: 1]
Tensor_29052: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13164: 59610624 Bnn)
Operation_14381: operator = BatchedMatmulBackward, inputs = [Tensor_29052, Tensor_28064, Tensor_28066], outputs = [Tensor_29054, Tensor_29055], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29054: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13444: 19365888 Bnn)
Tensor_29055: shape = [1536, 64, 197], stride = [12608, 1, 64], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13418: 19365888 Bnn)
Operation_14384: operator = ReshapeBackward, inputs = [Tensor_29054], outputs = [Tensor_29057], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_29057: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13444: 19365888 Bnn)
Operation_14388: operator = ContiguousBackward, inputs = [Tensor_29057], outputs = [Tensor_29059], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29059: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13134: 19365888 Bnn)
Operation_14390: operator = ReshapeBackward, inputs = [Tensor_29055], outputs = [Tensor_29061], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_29061: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13418: 19365888 Bnn)
Operation_14394: operator = ContiguousBackward, inputs = [Tensor_29061], outputs = [Tensor_29063], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29063: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13625: 19365888 Bnn)
Operation_14396: operator = ReshapeBackward, inputs = [Tensor_29059], outputs = [Tensor_29065], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 12 shape: 64 stride: 151296 stride: 768 stride: 64 stride: 1]
Tensor_29065: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13134: 19365888 Bnn)
Operation_14400: operator = ElemwiseUnaryBackward, inputs = [Tensor_29065, nullptr, nullptr], outputs = [Tensor_29067], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 0.125]
Tensor_29067: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13134: 19365888 Bnn)
Operation_14404: operator = LinearBackward, inputs = [Tensor_29067, Tensor_28058, Tensor_28059], outputs = [Tensor_29069, Tensor_29070, Tensor_29074], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->22541
  - Operand: []
Tensor_29069: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13444: 19365888 Bnr)
Tensor_29070: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (13136: 1179648 Bnn)
Operation_22541: operator = AllReduceDuplicated, inputs = [Tensor_29070, nullptr], outputs = [Tensor_29073, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 14404->this
  - Operand: [subop: SUM tag: 22540]
Operation_29072: operator = Consume, inputs = [Tensor_29070], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29073: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13136: 1179648 Bnn)
Tensor_29074: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (28373: 1536 Bnn)
Operation_22545: operator = AllReduceDuplicated, inputs = [Tensor_29074, nullptr], outputs = [Tensor_29077, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: SUM tag: 22544]
Operation_29076: operator = Consume, inputs = [Tensor_29074], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29077: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28373: 1536 Bnn)
Operation_14402: operator = ReshapeBackward, inputs = [Tensor_29063], outputs = [Tensor_29079], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 12 shape: 64 stride: 151296 stride: 768 stride: 64 stride: 1]
Tensor_29079: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13625: 19365888 Bnn)
Operation_14408: operator = LinearBackward, inputs = [Tensor_29079, Tensor_28060, Tensor_28061], outputs = [Tensor_29081, Tensor_29082, Tensor_29086], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->22553
  - Operand: []
Tensor_29081: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13418: 19365888 Bnn)
Tensor_29082: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (13613: 1179648 Bnn)
Operation_22553: operator = AllReduceDuplicated, inputs = [Tensor_29082, nullptr], outputs = [Tensor_29085, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 14408->this
  - Operand: [subop: SUM tag: 22552]
Operation_29084: operator = Consume, inputs = [Tensor_29082], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29085: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13613: 1179648 Bnr)
Tensor_29086: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (29089: 1536 Ban)
Operation_22557: operator = AllReduceDuplicated, inputs = [Tensor_29086, nullptr], outputs = [Tensor_29089, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: SUM tag: 22556]
Operation_29088: operator = Consume, inputs = [Tensor_29086], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29089: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29089: 1536 Bnn)
Operation_14412: operator = TypecastBackward, inputs = [Tensor_29069], outputs = [Tensor_29091], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_29091: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13345: 38731776 Bnn)
Operation_14414: operator = ElemwiseBinaryForward, inputs = [Tensor_29050, Tensor_29091], outputs = [Tensor_29093], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_29093: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13271: 38731776 Bnn)
Operation_14416: operator = TypecastBackward, inputs = [Tensor_29081], outputs = [Tensor_29095], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_29095: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13430: 38731776 Bnn)
Operation_14418: operator = ElemwiseBinaryForward, inputs = [Tensor_29093, Tensor_29095], outputs = [Tensor_29097], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_29097: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13271: 38731776 Bnn)
Operation_14420: operator = LayernormBackward, inputs = [Tensor_29097, Tensor_28054, Tensor_28055, Tensor_27699, Tensor_27700, Tensor_28056, Tensor_28057], outputs = [Tensor_29099, Tensor_29100, Tensor_29104], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->22571
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_29099: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13538: 38731776 Bnn)
Tensor_29100: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (29103: 3072 Ban)
Operation_22571: operator = AllReduceDuplicated, inputs = [Tensor_29100, nullptr], outputs = [Tensor_29103, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 14420->this
  - Operand: [subop: SUM tag: 22570]
Operation_29102: operator = Consume, inputs = [Tensor_29100], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29103: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29103: 3072 Bnn)
Tensor_29104: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (29107: 3072 Ban)
Operation_22575: operator = AllReduceDuplicated, inputs = [Tensor_29104, nullptr], outputs = [Tensor_29107, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: SUM tag: 22574]
Operation_29106: operator = Consume, inputs = [Tensor_29104], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29107: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29107: 3072 Bnn)
Operation_14422: operator = ElemwiseBinaryForward, inputs = [Tensor_28972, Tensor_29099], outputs = [Tensor_29109], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_29109: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13663: 38731776 Bnn)
Operation_14429: operator = ElemwiseUnaryBackward, inputs = [Tensor_29109, nullptr, nullptr], outputs = [Tensor_29111], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 1]
Tensor_29111: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13430: 38731776 Bnn)
Operation_14431: operator = TypecastBackward, inputs = [Tensor_29109], outputs = [Tensor_29113], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_29113: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13134: 19365888 Bnn)
Operation_14433: operator = ElemwiseBinaryBackward, inputs = [Tensor_29113, Tensor_28052, Tensor_28053, nullptr], outputs = [Tensor_29115, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_MUL]
Tensor_29115: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13134: 19365888 Bnn)
Operation_14435: operator = ElemwiseUnaryBackward, inputs = [Tensor_29115, nullptr, nullptr], outputs = [Tensor_29117], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_DIV scal1_float: 0.904545426]
Tensor_29117: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13134: 19365888 Bnn)
Operation_14437: operator = ReshapeBackward, inputs = [Tensor_29117], outputs = [Tensor_29119], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 151296 stride: 768 stride: 1]
Tensor_29119: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13134: 19365888 Bnr)
Operation_14439: operator = LinearBackward, inputs = [Tensor_29119, Tensor_28050, Tensor_28051], outputs = [Tensor_29121, Tensor_29122, Tensor_29126], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->22593
  - Operand: []
Tensor_29121: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13669: 77463552 Bnn)
Tensor_29122: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (13388: 4718592 Bnn)
Operation_22593: operator = AllReduceDuplicated, inputs = [Tensor_29122, nullptr], outputs = [Tensor_29125, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 14439->this
  - Operand: [subop: SUM tag: 22592]
Operation_29124: operator = Consume, inputs = [Tensor_29122], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29125: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13388: 4718592 Bnr)
Tensor_29126: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (29129: 1536 Ban)
Operation_22597: operator = AllReduceDuplicated, inputs = [Tensor_29126, nullptr], outputs = [Tensor_29129, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: SUM tag: 22596]
Operation_29128: operator = Consume, inputs = [Tensor_29126], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29129: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29129: 1536 Bnr)
Operation_14443: operator = TypecastBackward, inputs = [Tensor_29121], outputs = [Tensor_29131], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_29131: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (28311: 154927104 Bnn)
Operation_14445: operator = LayernormBackward, inputs = [Tensor_29131, Tensor_28046, Tensor_28047, Tensor_27693, Tensor_27694, Tensor_28048, Tensor_28049], outputs = [Tensor_29133, Tensor_29134, Tensor_29138], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->22605
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_29133: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13240: 154927104 Bnr)
Tensor_29134: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (29137: 12288 Ban)
Operation_22605: operator = AllReduceDuplicated, inputs = [Tensor_29134, nullptr], outputs = [Tensor_29137, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 14445->this
  - Operand: [subop: SUM tag: 22604]
Operation_29136: operator = Consume, inputs = [Tensor_29134], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29137: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29137: 12288 Bnn)
Tensor_29138: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (29141: 12288 Ban)
Operation_22609: operator = AllReduceDuplicated, inputs = [Tensor_29138, nullptr], outputs = [Tensor_29141, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: SUM tag: 22608]
Operation_29140: operator = Consume, inputs = [Tensor_29138], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29141: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29141: 12288 Bnn)
Operation_14449: operator = TypecastBackward, inputs = [Tensor_29133], outputs = [Tensor_29143], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_29143: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12238: 77463552 Bnr)
Operation_14451: operator = TypecastBackward, inputs = [Tensor_29143], outputs = [Tensor_29145], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_29145: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13719: 154927104 Bnn)
Operation_14453: operator = ElemwiseUnaryBackward, inputs = [Tensor_29145, Tensor_28044, Tensor_28045], outputs = [Tensor_29147], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_GELU]
Tensor_29147: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13719: 154927104 Bnn)
Operation_14455: operator = TypecastBackward, inputs = [Tensor_29147], outputs = [Tensor_29149], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_29149: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13709: 77463552 Bnr)
Operation_14457: operator = LinearBackward, inputs = [Tensor_29149, Tensor_28042, Tensor_28043], outputs = [Tensor_29151, Tensor_29152, Tensor_29156], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->22623
  - Operand: []
Tensor_29151: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13100: 19365888 Bnn)
Tensor_29152: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (13089: 4718592 Bnn)
Operation_22623: operator = AllReduceDuplicated, inputs = [Tensor_29152, nullptr], outputs = [Tensor_29155, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 14457->this
  - Operand: [subop: SUM tag: 22622]
Operation_29154: operator = Consume, inputs = [Tensor_29152], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29155: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13089: 4718592 Bnr)
Tensor_29156: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (29159: 6144 Ban)
Operation_22627: operator = AllReduceDuplicated, inputs = [Tensor_29156, nullptr], outputs = [Tensor_29159, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: SUM tag: 22626]
Operation_29158: operator = Consume, inputs = [Tensor_29156], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29159: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29159: 6144 Bnr)
Operation_14461: operator = TypecastBackward, inputs = [Tensor_29151], outputs = [Tensor_29161], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_29161: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13269: 38731776 Bnn)
Operation_14463: operator = ReshapeBackward, inputs = [Tensor_29161], outputs = [Tensor_29163], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 25216 shape: 768 stride: 768 stride: 1]
Tensor_29163: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13269: 38731776 Bnn)
Operation_14465: operator = LayernormBackward, inputs = [Tensor_29163, Tensor_28038, Tensor_28039, Tensor_27695, Tensor_27696, Tensor_28040, Tensor_28041], outputs = [Tensor_29165, Tensor_29166, Tensor_29170], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->22637
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_29165: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (28289: 38731776 Bnn)
Tensor_29166: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (29169: 3072 Ban)
Operation_22637: operator = AllReduceDuplicated, inputs = [Tensor_29166, nullptr], outputs = [Tensor_29169, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 14465->this
  - Operand: [subop: SUM tag: 22636]
Operation_29168: operator = Consume, inputs = [Tensor_29166], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29169: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29169: 3072 Bnn)
Tensor_29170: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (29173: 3072 Ban)
Operation_22641: operator = AllReduceDuplicated, inputs = [Tensor_29170, nullptr], outputs = [Tensor_29173, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: SUM tag: 22640]
Operation_29172: operator = Consume, inputs = [Tensor_29170], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29173: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29173: 3072 Bnn)
Operation_14467: operator = ElemwiseBinaryForward, inputs = [Tensor_29111, Tensor_29165], outputs = [Tensor_29175], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_29175: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13430: 38731776 Bnn)
Operation_14474: operator = ElemwiseUnaryBackward, inputs = [Tensor_29175, nullptr, nullptr], outputs = [Tensor_29177], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 1]
Tensor_29177: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (28289: 38731776 Bnn)
Operation_14476: operator = TypecastBackward, inputs = [Tensor_29175], outputs = [Tensor_29179], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_29179: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13100: 19365888 Bnn)
Operation_14478: operator = ElemwiseBinaryBackward, inputs = [Tensor_29179, Tensor_28036, Tensor_28037, nullptr], outputs = [Tensor_29181, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_MUL]
Tensor_29181: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13100: 19365888 Bnn)
Operation_14480: operator = ElemwiseUnaryBackward, inputs = [Tensor_29181, nullptr, nullptr], outputs = [Tensor_29183], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_DIV scal1_float: 0.904545426]
Tensor_29183: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13100: 19365888 Bnr)
Operation_14482: operator = LinearBackward, inputs = [Tensor_29183, Tensor_28034, Tensor_28035], outputs = [Tensor_29185, Tensor_29186, Tensor_29190], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->22657
  - Operand: []
Tensor_29185: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13466: 19365888 Bnn)
Tensor_29186: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (13201: 1179648 Bnn)
Operation_22657: operator = AllReduceDuplicated, inputs = [Tensor_29186, nullptr], outputs = [Tensor_29189, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 14482->this
  - Operand: [subop: SUM tag: 22656]
Operation_29188: operator = Consume, inputs = [Tensor_29186], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29189: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13201: 1179648 Bnr)
Tensor_29190: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (29193: 1536 Ban)
Operation_22661: operator = AllReduceDuplicated, inputs = [Tensor_29190, nullptr], outputs = [Tensor_29193, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: SUM tag: 22660]
Operation_29192: operator = Consume, inputs = [Tensor_29190], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29193: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29193: 1536 Bnr)
Operation_14486: operator = TypecastBackward, inputs = [Tensor_29185], outputs = [Tensor_29195], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_29195: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13349: 38731776 Bnn)
Tensor_29196: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (28355: 38731776 Bnn)
Operation_14488: operator = LayernormBackward, inputs = [Tensor_29195, Tensor_29196, Tensor_28031, Tensor_27689, Tensor_27690, Tensor_28032, Tensor_28033], outputs = [Tensor_29198, Tensor_29199, Tensor_29203], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->22671
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_29198: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13186: 38731776 Bnn)
Tensor_29199: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (29202: 3072 Ban)
Operation_22671: operator = AllReduceDuplicated, inputs = [Tensor_29199, nullptr], outputs = [Tensor_29202, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 14488->this
  - Operand: [subop: SUM tag: 22670]
Operation_29201: operator = Consume, inputs = [Tensor_29199], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29202: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29202: 3072 Bnn)
Tensor_29203: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (29206: 3072 Ban)
Operation_22675: operator = AllReduceDuplicated, inputs = [Tensor_29203, nullptr], outputs = [Tensor_29206, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: SUM tag: 22674]
Operation_29205: operator = Consume, inputs = [Tensor_29203], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29206: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29206: 3072 Bnn)
Operation_22678: operator = AllGatherDuplicated, inputs = [Tensor_29198], outputs = [Tensor_29209], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->14492
  - Operand: [tag: 22677]
Operation_29208: operator = Consume, inputs = [Tensor_29198], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29209: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13550: 77463552 Bnn)
Operation_15673: operator = TypecastBackward, inputs = [Tensor_28462], outputs = [Tensor_30784], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Operation_15675: operator = TypecastBackward, inputs = [Tensor_28470], outputs = [Tensor_30785], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Operation_15677: operator = TypecastBackward, inputs = [Tensor_28474], outputs = [Tensor_30786], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Operation_15679: operator = TypecastBackward, inputs = [Tensor_28510], outputs = [Tensor_30787], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Operation_15681: operator = TypecastBackward, inputs = [Tensor_28514], outputs = [Tensor_30788], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Operation_14492: operator = TypecastBackward, inputs = [Tensor_29209], outputs = [Tensor_29216], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 22678->this
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_29216: shape = [128, 197, 768], stride = [768, 98304, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28355: 38731776 Bnn)
Operation_14496: operator = ReshapeBackward, inputs = [Tensor_29216], outputs = [Tensor_29218], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 197 shape: 128 shape: 768 stride: 98304 stride: 768 stride: 1]
Tensor_29218: shape = [197, 1536, 64], stride = [98304, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28355: 38731776 Bnn)
Operation_14498: operator = ContiguousBackward, inputs = [Tensor_29218], outputs = [Tensor_29220], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29220: shape = [197, 1536, 64], stride = [64, 12608, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13349: 38731776 Bnn)
Operation_14500: operator = ReshapeBackward, inputs = [Tensor_29220], outputs = [Tensor_29222], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 197 shape: 1536 shape: 64 stride: 64 stride: 12608 stride: 1]
Tensor_29222: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13349: 38731776 Bnn)
Operation_22688: operator = DistributeWithMemAllocForward, inputs = [Tensor_29222], outputs = [Tensor_29224], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29224: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13533: 19365888 Bnr)
Operation_14502: operator = BatchedMatmulBackward, inputs = [Tensor_29224, Tensor_28029, Tensor_28025], outputs = [Tensor_29226, Tensor_29227], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29226: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13323: 59610624 Bnn)
Tensor_29227: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13692: 19365888 Bnn)
Operation_14505: operator = TypecastBackward, inputs = [Tensor_29226], outputs = [Tensor_29229], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_29229: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (28411: 119221248 Bnr)
Operation_14509: operator = SoftmaxBackward, inputs = [Tensor_29229, Tensor_28028], outputs = [Tensor_29231], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dim: 2]
Tensor_29231: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13331: 119221248 Bnr)
Operation_14507: operator = ReshapeBackward, inputs = [Tensor_29227], outputs = [Tensor_29233], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_29233: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13692: 19365888 Bnn)
Operation_14511: operator = ContiguousBackward, inputs = [Tensor_29233], outputs = [Tensor_29235], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29235: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13370: 19365888 Bnn)
Operation_14513: operator = TypecastBackward, inputs = [Tensor_29231], outputs = [Tensor_29237], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_29237: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13323: 59610624 Bnn)
Operation_14517: operator = ReshapeBackward, inputs = [Tensor_29237], outputs = [Tensor_29239], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 197 stride: 38809 stride: 197 stride: 1]
Tensor_29239: shape = [128, 12, 197, 197], stride = [465708, 38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13323: 59610624 Bnn)
Operation_14521: operator = MaskedFillBackward, inputs = [Tensor_29239, Tensor_28027], outputs = [Tensor_29241], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [value_float: -inf]
Tensor_29241: shape = [128, 12, 197, 197], stride = [465708, 38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13323: 59610624 Bnn)
Operation_14519: operator = ReshapeBackward, inputs = [Tensor_29235], outputs = [Tensor_29243], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 12 shape: 64 stride: 151296 stride: 768 stride: 64 stride: 1]
Tensor_29243: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13370: 19365888 Bnn)
Operation_14523: operator = LinearBackward, inputs = [Tensor_29243, Tensor_28022, Tensor_28023], outputs = [Tensor_29245, Tensor_29246, Tensor_29250], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->22713
  - Operand: []
Tensor_29245: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13056: 19365888 Bnn)
Tensor_29246: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (28369: 1179648 Bnn)
Operation_22713: operator = AllReduceDuplicated, inputs = [Tensor_29246, nullptr], outputs = [Tensor_29249, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 14523->this
  - Operand: [subop: SUM tag: 22712]
Operation_29248: operator = Consume, inputs = [Tensor_29246], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29249: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28369: 1179648 Bnr)
Tensor_29250: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (28474: 1536 Bnn)
Operation_22717: operator = AllReduceDuplicated, inputs = [Tensor_29250, nullptr], outputs = [Tensor_29253, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: SUM tag: 22716]
Operation_29252: operator = Consume, inputs = [Tensor_29250], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29253: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28474: 1536 Bnr)
Operation_14529: operator = TypecastBackward, inputs = [Tensor_29245], outputs = [Tensor_29255], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_29255: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13349: 38731776 Bnn)
Operation_14527: operator = ReshapeBackward, inputs = [Tensor_29241], outputs = [Tensor_29257], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 197 stride: 465708 stride: 38809 stride: 197 stride: 1]
Tensor_29257: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13323: 59610624 Bnn)
Operation_14531: operator = BatchedMatmulBackward, inputs = [Tensor_29257, Tensor_28024, Tensor_28026], outputs = [Tensor_29259, Tensor_29260], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29259: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13370: 19365888 Bnn)
Tensor_29260: shape = [1536, 64, 197], stride = [12608, 1, 64], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13056: 19365888 Bnn)
Operation_14534: operator = ReshapeBackward, inputs = [Tensor_29259], outputs = [Tensor_29262], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_29262: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13370: 19365888 Bnn)
Operation_14538: operator = ContiguousBackward, inputs = [Tensor_29262], outputs = [Tensor_29264], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29264: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12975: 19365888 Bnn)
Operation_14540: operator = ReshapeBackward, inputs = [Tensor_29260], outputs = [Tensor_29266], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_29266: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13056: 19365888 Bnn)
Operation_14544: operator = ContiguousBackward, inputs = [Tensor_29266], outputs = [Tensor_29268], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29268: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12989: 19365888 Bnn)
Operation_14546: operator = ReshapeBackward, inputs = [Tensor_29264], outputs = [Tensor_29270], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 12 shape: 64 stride: 151296 stride: 768 stride: 64 stride: 1]
Tensor_29270: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12975: 19365888 Bnn)
Operation_14550: operator = ElemwiseUnaryBackward, inputs = [Tensor_29270, nullptr, nullptr], outputs = [Tensor_29272], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 0.125]
Tensor_29272: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12975: 19365888 Bnn)
Operation_14554: operator = LinearBackward, inputs = [Tensor_29272, Tensor_28018, Tensor_28019], outputs = [Tensor_29274, Tensor_29275, Tensor_29279], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->22742
  - Operand: []
Tensor_29274: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13370: 19365888 Bnr)
Tensor_29275: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (13120: 1179648 Bnn)
Operation_22742: operator = AllReduceDuplicated, inputs = [Tensor_29275, nullptr], outputs = [Tensor_29278, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 14554->this
  - Operand: [subop: SUM tag: 22741]
Operation_29277: operator = Consume, inputs = [Tensor_29275], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29278: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13120: 1179648 Bnr)
Tensor_29279: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (28462: 1536 Bnn)
Operation_22746: operator = AllReduceDuplicated, inputs = [Tensor_29279, nullptr], outputs = [Tensor_29282, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: SUM tag: 22745]
Operation_29281: operator = Consume, inputs = [Tensor_29279], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29282: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28462: 1536 Bnr)
Operation_14552: operator = ReshapeBackward, inputs = [Tensor_29268], outputs = [Tensor_29284], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 12 shape: 64 stride: 151296 stride: 768 stride: 64 stride: 1]
Tensor_29284: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12989: 19365888 Bnr)
Operation_14558: operator = LinearBackward, inputs = [Tensor_29284, Tensor_28020, Tensor_28021], outputs = [Tensor_29286, Tensor_29287, Tensor_29291], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->22754
  - Operand: []
Tensor_29286: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12991: 19365888 Bnr)
Tensor_29287: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (13128: 1179648 Bnn)
Operation_22754: operator = AllReduceDuplicated, inputs = [Tensor_29287, nullptr], outputs = [Tensor_29290, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 14558->this
  - Operand: [subop: SUM tag: 22753]
Operation_29289: operator = Consume, inputs = [Tensor_29287], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29290: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13128: 1179648 Bnr)
Tensor_29291: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (28514: 1536 Bnn)
Operation_22758: operator = AllReduceDuplicated, inputs = [Tensor_29291, nullptr], outputs = [Tensor_29294, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: SUM tag: 22757]
Operation_29293: operator = Consume, inputs = [Tensor_29291], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29294: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28514: 1536 Bnr)
Operation_14562: operator = TypecastBackward, inputs = [Tensor_29274], outputs = [Tensor_29296], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_29296: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13428: 38731776 Bnr)
Operation_14564: operator = ElemwiseBinaryForward, inputs = [Tensor_29255, Tensor_29296], outputs = [Tensor_29298], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_29298: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13349: 38731776 Bnn)
Operation_14566: operator = TypecastBackward, inputs = [Tensor_29286], outputs = [Tensor_29300], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_29300: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (28786: 38731776 Bnn)
Operation_14568: operator = ElemwiseBinaryForward, inputs = [Tensor_29298, Tensor_29300], outputs = [Tensor_29302], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_29302: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13349: 38731776 Bnn)
Operation_14570: operator = LayernormBackward, inputs = [Tensor_29302, Tensor_28014, Tensor_28015, Tensor_27691, Tensor_27692, Tensor_28016, Tensor_28017], outputs = [Tensor_29304, Tensor_29305, Tensor_29309], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->22772
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_29304: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (28991: 38731776 Bnn)
Tensor_29305: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (29308: 3072 Ban)
Operation_22772: operator = AllReduceDuplicated, inputs = [Tensor_29305, nullptr], outputs = [Tensor_29308, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 14570->this
  - Operand: [subop: SUM tag: 22771]
Operation_29307: operator = Consume, inputs = [Tensor_29305], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29308: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29308: 3072 Bnn)
Tensor_29309: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (29312: 3072 Ban)
Operation_22776: operator = AllReduceDuplicated, inputs = [Tensor_29309, nullptr], outputs = [Tensor_29312, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: SUM tag: 22775]
Operation_29311: operator = Consume, inputs = [Tensor_29309], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29312: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29312: 3072 Bnn)
Operation_14572: operator = ElemwiseBinaryForward, inputs = [Tensor_29177, Tensor_29304], outputs = [Tensor_29314], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_29314: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (28289: 38731776 Bnr)
Operation_14579: operator = ElemwiseUnaryBackward, inputs = [Tensor_29314, nullptr, nullptr], outputs = [Tensor_29316], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 1]
Tensor_29316: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (28786: 38731776 Bnn)
Operation_14581: operator = TypecastBackward, inputs = [Tensor_29314], outputs = [Tensor_29318], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_29318: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12975: 19365888 Bnn)
Operation_14583: operator = ElemwiseBinaryBackward, inputs = [Tensor_29318, Tensor_28012, Tensor_28013, nullptr], outputs = [Tensor_29320, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_MUL]
Tensor_29320: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12975: 19365888 Bnn)
Operation_14585: operator = ElemwiseUnaryBackward, inputs = [Tensor_29320, nullptr, nullptr], outputs = [Tensor_29322], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_DIV scal1_float: 0.918181837]
Tensor_29322: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12975: 19365888 Bnn)
Operation_14587: operator = ReshapeBackward, inputs = [Tensor_29322], outputs = [Tensor_29324], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 151296 stride: 768 stride: 1]
Tensor_29324: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12975: 19365888 Bnr)
Operation_14589: operator = LinearBackward, inputs = [Tensor_29324, Tensor_28010, Tensor_28011], outputs = [Tensor_29326, Tensor_29327, Tensor_29331], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->22794
  - Operand: []
Tensor_29326: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13391: 77463552 Bnr)
Tensor_29327: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (13248: 4718592 Bnn)
Operation_22794: operator = AllReduceDuplicated, inputs = [Tensor_29327, nullptr], outputs = [Tensor_29330, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 14589->this
  - Operand: [subop: SUM tag: 22793]
Operation_29329: operator = Consume, inputs = [Tensor_29327], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29330: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13248: 4718592 Bnr)
Tensor_29331: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (29334: 1536 Ban)
Operation_22798: operator = AllReduceDuplicated, inputs = [Tensor_29331, nullptr], outputs = [Tensor_29334, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: SUM tag: 22797]
Operation_29333: operator = Consume, inputs = [Tensor_29331], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29334: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29334: 1536 Bnr)
Operation_14593: operator = TypecastBackward, inputs = [Tensor_29326], outputs = [Tensor_29336], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_29336: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (28313: 154927104 Bnr)
Operation_14595: operator = LayernormBackward, inputs = [Tensor_29336, Tensor_28006, Tensor_28007, Tensor_27685, Tensor_27686, Tensor_28008, Tensor_28009], outputs = [Tensor_29338, Tensor_29339, Tensor_29343], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->22806
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_29338: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13234: 154927104 Bnr)
Tensor_29339: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (29342: 12288 Ban)
Operation_22806: operator = AllReduceDuplicated, inputs = [Tensor_29339, nullptr], outputs = [Tensor_29342, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 14595->this
  - Operand: [subop: SUM tag: 22805]
Operation_29341: operator = Consume, inputs = [Tensor_29339], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29342: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29342: 12288 Bnn)
Tensor_29343: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (29346: 12288 Ban)
Operation_22810: operator = AllReduceDuplicated, inputs = [Tensor_29343, nullptr], outputs = [Tensor_29346, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: SUM tag: 22809]
Operation_29345: operator = Consume, inputs = [Tensor_29343], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29346: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29346: 12288 Bnn)
Operation_14599: operator = TypecastBackward, inputs = [Tensor_29338], outputs = [Tensor_29348], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_29348: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13232: 77463552 Bnr)
Operation_14601: operator = TypecastBackward, inputs = [Tensor_29348], outputs = [Tensor_29350], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_29350: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13719: 154927104 Bnn)
Operation_14603: operator = ElemwiseUnaryBackward, inputs = [Tensor_29350, Tensor_28004, Tensor_28005], outputs = [Tensor_29352], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_GELU]
Tensor_29352: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13719: 154927104 Bnr)
Operation_14605: operator = TypecastBackward, inputs = [Tensor_29352], outputs = [Tensor_29354], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_29354: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13033: 77463552 Bnr)
Operation_14607: operator = LinearBackward, inputs = [Tensor_29354, Tensor_28002, Tensor_28003], outputs = [Tensor_29356, Tensor_29357, Tensor_29361], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->22824
  - Operand: []
Tensor_29356: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13106: 19365888 Bnn)
Tensor_29357: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (13229: 4718592 Bnn)
Operation_22824: operator = AllReduceDuplicated, inputs = [Tensor_29357, nullptr], outputs = [Tensor_29360, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 14607->this
  - Operand: [subop: SUM tag: 22823]
Operation_29359: operator = Consume, inputs = [Tensor_29357], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29360: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13229: 4718592 Bnr)
Tensor_29361: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (29364: 6144 Ban)
Operation_22828: operator = AllReduceDuplicated, inputs = [Tensor_29361, nullptr], outputs = [Tensor_29364, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: SUM tag: 22827]
Operation_29363: operator = Consume, inputs = [Tensor_29361], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29364: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29364: 6144 Bnr)
Operation_14611: operator = TypecastBackward, inputs = [Tensor_29356], outputs = [Tensor_29366], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_29366: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (28991: 38731776 Bnn)
Operation_14613: operator = ReshapeBackward, inputs = [Tensor_29366], outputs = [Tensor_29368], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 25216 shape: 768 stride: 768 stride: 1]
Tensor_29368: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (28991: 38731776 Bnn)
Operation_14615: operator = LayernormBackward, inputs = [Tensor_29368, Tensor_27998, Tensor_27999, Tensor_27687, Tensor_27688, Tensor_28000, Tensor_28001], outputs = [Tensor_29370, Tensor_29371, Tensor_29375], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->22838
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_29370: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13220: 38731776 Bnr)
Tensor_29371: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (29374: 3072 Ban)
Operation_22838: operator = AllReduceDuplicated, inputs = [Tensor_29371, nullptr], outputs = [Tensor_29374, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 14615->this
  - Operand: [subop: SUM tag: 22837]
Operation_29373: operator = Consume, inputs = [Tensor_29371], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29374: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29374: 3072 Bnn)
Tensor_29375: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (29378: 3072 Ban)
Operation_22842: operator = AllReduceDuplicated, inputs = [Tensor_29375, nullptr], outputs = [Tensor_29378, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: SUM tag: 22841]
Operation_29377: operator = Consume, inputs = [Tensor_29375], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29378: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29378: 3072 Bnn)
Operation_14617: operator = ElemwiseBinaryForward, inputs = [Tensor_29316, Tensor_29370], outputs = [Tensor_29380], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_29380: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (28786: 38731776 Bnn)
Operation_14624: operator = ElemwiseUnaryBackward, inputs = [Tensor_29380, nullptr, nullptr], outputs = [Tensor_29382], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 1]
Tensor_29382: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13031: 38731776 Bnn)
Operation_14626: operator = TypecastBackward, inputs = [Tensor_29380], outputs = [Tensor_29384], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_29384: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13106: 19365888 Bnn)
Operation_14628: operator = ElemwiseBinaryBackward, inputs = [Tensor_29384, Tensor_27996, Tensor_27997, nullptr], outputs = [Tensor_29386, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_MUL]
Tensor_29386: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13106: 19365888 Bnn)
Operation_14630: operator = ElemwiseUnaryBackward, inputs = [Tensor_29386, nullptr, nullptr], outputs = [Tensor_29388], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_DIV scal1_float: 0.918181837]
Tensor_29388: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13106: 19365888 Bnr)
Operation_14632: operator = LinearBackward, inputs = [Tensor_29388, Tensor_27994, Tensor_27995], outputs = [Tensor_29390, Tensor_29391, Tensor_29395], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->22858
  - Operand: []
Tensor_29390: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13603: 19365888 Bnn)
Tensor_29391: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (12969: 1179648 Bnn)
Operation_22858: operator = AllReduceDuplicated, inputs = [Tensor_29391, nullptr], outputs = [Tensor_29394, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 14632->this
  - Operand: [subop: SUM tag: 22857]
Operation_29393: operator = Consume, inputs = [Tensor_29391], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29394: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12969: 1179648 Bnr)
Tensor_29395: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (29398: 1536 Ban)
Operation_22862: operator = AllReduceDuplicated, inputs = [Tensor_29395, nullptr], outputs = [Tensor_29398, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: SUM tag: 22861]
Operation_29397: operator = Consume, inputs = [Tensor_29395], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29398: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29398: 1536 Bnr)
Operation_14636: operator = TypecastBackward, inputs = [Tensor_29390], outputs = [Tensor_29400], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_29400: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12868: 38731776 Bnr)
Tensor_29401: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13746: 38731776 Bnr)
Operation_14638: operator = LayernormBackward, inputs = [Tensor_29400, Tensor_29401, Tensor_27991, Tensor_27681, Tensor_27682, Tensor_27992, Tensor_27993], outputs = [Tensor_29403, Tensor_29404, Tensor_29408], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->22872
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_29403: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13186: 38731776 Bnn)
Tensor_29404: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (29407: 3072 Ban)
Operation_22872: operator = AllReduceDuplicated, inputs = [Tensor_29404, nullptr], outputs = [Tensor_29407, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 14638->this
  - Operand: [subop: SUM tag: 22871]
Operation_29406: operator = Consume, inputs = [Tensor_29404], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29407: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29407: 3072 Bnn)
Tensor_29408: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (29411: 3072 Ban)
Operation_22876: operator = AllReduceDuplicated, inputs = [Tensor_29408, nullptr], outputs = [Tensor_29411, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: SUM tag: 22875]
Operation_29410: operator = Consume, inputs = [Tensor_29408], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29411: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29411: 3072 Bnn)
Operation_22879: operator = AllGatherDuplicated, inputs = [Tensor_29403], outputs = [Tensor_29414], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->14642
  - Operand: [tag: 22878]
Operation_29413: operator = Consume, inputs = [Tensor_29403], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29414: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12079: 77463552 Bnn)
Operation_15683: operator = TypecastBackward, inputs = [Tensor_28540], outputs = [Tensor_30789], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Operation_15685: operator = TypecastBackward, inputs = [Tensor_28544], outputs = [Tensor_30790], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Operation_15687: operator = TypecastBackward, inputs = [Tensor_28574], outputs = [Tensor_30791], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Operation_15689: operator = TypecastBackward, inputs = [Tensor_28578], outputs = [Tensor_30792], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Operation_15691: operator = TypecastBackward, inputs = [Tensor_28634], outputs = [Tensor_30793], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Operation_14642: operator = TypecastBackward, inputs = [Tensor_29414], outputs = [Tensor_29421], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 22879->this
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_29421: shape = [128, 197, 768], stride = [768, 98304, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12872: 38731776 Bnn)
Operation_14646: operator = ReshapeBackward, inputs = [Tensor_29421], outputs = [Tensor_29423], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 197 shape: 128 shape: 768 stride: 98304 stride: 768 stride: 1]
Tensor_29423: shape = [197, 1536, 64], stride = [98304, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12872: 38731776 Bnr)
Operation_14648: operator = ContiguousBackward, inputs = [Tensor_29423], outputs = [Tensor_29425], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29425: shape = [197, 1536, 64], stride = [64, 12608, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13269: 38731776 Bnn)
Operation_14650: operator = ReshapeBackward, inputs = [Tensor_29425], outputs = [Tensor_29427], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 197 shape: 1536 shape: 64 stride: 64 stride: 12608 stride: 1]
Tensor_29427: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13269: 38731776 Bnn)
Operation_22889: operator = DistributeWithMemAllocForward, inputs = [Tensor_29427], outputs = [Tensor_29429], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29429: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12836: 19365888 Bnr)
Operation_14652: operator = BatchedMatmulBackward, inputs = [Tensor_29429, Tensor_27989, Tensor_27985], outputs = [Tensor_29431, Tensor_29432], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29431: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13323: 59610624 Bnr)
Tensor_29432: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13631: 19365888 Bnn)
Operation_14655: operator = TypecastBackward, inputs = [Tensor_29431], outputs = [Tensor_29434], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_29434: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13649: 119221248 Bnr)
Operation_14659: operator = SoftmaxBackward, inputs = [Tensor_29434, Tensor_27988], outputs = [Tensor_29436], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dim: 2]
Tensor_29436: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13172: 119221248 Bnr)
Operation_14657: operator = ReshapeBackward, inputs = [Tensor_29432], outputs = [Tensor_29438], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_29438: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13631: 19365888 Bnn)
Operation_14661: operator = ContiguousBackward, inputs = [Tensor_29438], outputs = [Tensor_29440], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29440: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12893: 19365888 Bnn)
Operation_14663: operator = TypecastBackward, inputs = [Tensor_29436], outputs = [Tensor_29442], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_29442: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13005: 59610624 Bnn)
Operation_14667: operator = ReshapeBackward, inputs = [Tensor_29442], outputs = [Tensor_29444], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 197 stride: 38809 stride: 197 stride: 1]
Tensor_29444: shape = [128, 12, 197, 197], stride = [465708, 38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13005: 59610624 Bnn)
Operation_14671: operator = MaskedFillBackward, inputs = [Tensor_29444, Tensor_27987], outputs = [Tensor_29446], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [value_float: -inf]
Tensor_29446: shape = [128, 12, 197, 197], stride = [465708, 38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13005: 59610624 Bnn)
Operation_14669: operator = ReshapeBackward, inputs = [Tensor_29440], outputs = [Tensor_29448], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 12 shape: 64 stride: 151296 stride: 768 stride: 64 stride: 1]
Tensor_29448: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12893: 19365888 Bnn)
Operation_14673: operator = LinearBackward, inputs = [Tensor_29448, Tensor_27982, Tensor_27983], outputs = [Tensor_29450, Tensor_29451, Tensor_29455], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->22914
  - Operand: []
Tensor_29450: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13424: 19365888 Bnn)
Tensor_29451: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (13605: 1179648 Bnn)
Operation_22914: operator = AllReduceDuplicated, inputs = [Tensor_29451, nullptr], outputs = [Tensor_29454, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 14673->this
  - Operand: [subop: SUM tag: 22913]
Operation_29453: operator = Consume, inputs = [Tensor_29451], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29454: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13605: 1179648 Bnr)
Tensor_29455: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (28578: 1536 Bnn)
Operation_22918: operator = AllReduceDuplicated, inputs = [Tensor_29455, nullptr], outputs = [Tensor_29458, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: SUM tag: 22917]
Operation_29457: operator = Consume, inputs = [Tensor_29455], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29458: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28578: 1536 Bnr)
Operation_14679: operator = TypecastBackward, inputs = [Tensor_29450], outputs = [Tensor_29460], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_29460: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13269: 38731776 Bnn)
Operation_14677: operator = ReshapeBackward, inputs = [Tensor_29446], outputs = [Tensor_29462], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 197 stride: 465708 stride: 38809 stride: 197 stride: 1]
Tensor_29462: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13005: 59610624 Bnr)
Operation_14681: operator = BatchedMatmulBackward, inputs = [Tensor_29462, Tensor_27984, Tensor_27986], outputs = [Tensor_29464, Tensor_29465], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29464: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13424: 19365888 Bnn)
Tensor_29465: shape = [1536, 64, 197], stride = [12608, 1, 64], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12893: 19365888 Bnn)
Operation_14684: operator = ReshapeBackward, inputs = [Tensor_29464], outputs = [Tensor_29467], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_29467: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13424: 19365888 Bnn)
Operation_14688: operator = ContiguousBackward, inputs = [Tensor_29467], outputs = [Tensor_29469], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29469: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12816: 19365888 Bnn)
Operation_14690: operator = ReshapeBackward, inputs = [Tensor_29465], outputs = [Tensor_29471], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_29471: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12893: 19365888 Bnn)
Operation_14694: operator = ContiguousBackward, inputs = [Tensor_29471], outputs = [Tensor_29473], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29473: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12983: 19365888 Bnn)
Operation_14696: operator = ReshapeBackward, inputs = [Tensor_29469], outputs = [Tensor_29475], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 12 shape: 64 stride: 151296 stride: 768 stride: 64 stride: 1]
Tensor_29475: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12816: 19365888 Bnn)
Operation_14700: operator = ElemwiseUnaryBackward, inputs = [Tensor_29475, nullptr, nullptr], outputs = [Tensor_29477], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 0.125]
Tensor_29477: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12816: 19365888 Bnr)
Operation_14704: operator = LinearBackward, inputs = [Tensor_29477, Tensor_27978, Tensor_27979], outputs = [Tensor_29479, Tensor_29480, Tensor_29484], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->22943
  - Operand: []
Tensor_29479: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13424: 19365888 Bnr)
Tensor_29480: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (12961: 1179648 Bnn)
Operation_22943: operator = AllReduceDuplicated, inputs = [Tensor_29480, nullptr], outputs = [Tensor_29483, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 14704->this
  - Operand: [subop: SUM tag: 22942]
Operation_29482: operator = Consume, inputs = [Tensor_29480], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29483: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12961: 1179648 Bnr)
Tensor_29484: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (29487: 1536 Ban)
Operation_22947: operator = AllReduceDuplicated, inputs = [Tensor_29484, nullptr], outputs = [Tensor_29487, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: SUM tag: 22946]
Operation_29486: operator = Consume, inputs = [Tensor_29484], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29487: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29487: 1536 Bnr)
Operation_14702: operator = ReshapeBackward, inputs = [Tensor_29473], outputs = [Tensor_29489], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 12 shape: 64 stride: 151296 stride: 768 stride: 64 stride: 1]
Tensor_29489: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12983: 19365888 Bnr)
Operation_14708: operator = LinearBackward, inputs = [Tensor_29489, Tensor_27980, Tensor_27981], outputs = [Tensor_29491, Tensor_29492, Tensor_29496], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->22955
  - Operand: []
Tensor_29491: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12893: 19365888 Bnr)
Tensor_29492: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (13519: 1179648 Bnn)
Operation_22955: operator = AllReduceDuplicated, inputs = [Tensor_29492, nullptr], outputs = [Tensor_29495, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 14708->this
  - Operand: [subop: SUM tag: 22954]
Operation_29494: operator = Consume, inputs = [Tensor_29492], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29495: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13519: 1179648 Bnr)
Tensor_29496: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (29499: 1536 Ban)
Operation_22959: operator = AllReduceDuplicated, inputs = [Tensor_29496, nullptr], outputs = [Tensor_29499, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: SUM tag: 22958]
Operation_29498: operator = Consume, inputs = [Tensor_29496], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29499: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29499: 1536 Bnr)
Operation_14712: operator = TypecastBackward, inputs = [Tensor_29479], outputs = [Tensor_29501], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_29501: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13186: 38731776 Bnr)
Operation_14714: operator = ElemwiseBinaryForward, inputs = [Tensor_29460, Tensor_29501], outputs = [Tensor_29503], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_29503: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13269: 38731776 Bnn)
Operation_14716: operator = TypecastBackward, inputs = [Tensor_29491], outputs = [Tensor_29505], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_29505: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13345: 38731776 Bnr)
Operation_14718: operator = ElemwiseBinaryForward, inputs = [Tensor_29503, Tensor_29505], outputs = [Tensor_29507], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_29507: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13269: 38731776 Bnr)
Operation_14720: operator = LayernormBackward, inputs = [Tensor_29507, Tensor_27974, Tensor_27975, Tensor_27683, Tensor_27684, Tensor_27976, Tensor_27977], outputs = [Tensor_29509, Tensor_29510, Tensor_29514], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->22973
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_29509: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13110: 38731776 Bnn)
Tensor_29510: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (29513: 3072 Ban)
Operation_22973: operator = AllReduceDuplicated, inputs = [Tensor_29510, nullptr], outputs = [Tensor_29513, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 14720->this
  - Operand: [subop: SUM tag: 22972]
Operation_29512: operator = Consume, inputs = [Tensor_29510], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29513: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29513: 3072 Bnn)
Tensor_29514: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (29517: 3072 Ban)
Operation_22977: operator = AllReduceDuplicated, inputs = [Tensor_29514, nullptr], outputs = [Tensor_29517, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: SUM tag: 22976]
Operation_29516: operator = Consume, inputs = [Tensor_29514], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29517: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29517: 3072 Bnn)
Operation_14722: operator = ElemwiseBinaryForward, inputs = [Tensor_29382, Tensor_29509], outputs = [Tensor_29519], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_29519: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13031: 38731776 Bnr)
Operation_14729: operator = ElemwiseUnaryBackward, inputs = [Tensor_29519, nullptr, nullptr], outputs = [Tensor_29521], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 1]
Tensor_29521: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13110: 38731776 Bnn)
Operation_14731: operator = TypecastBackward, inputs = [Tensor_29519], outputs = [Tensor_29523], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_29523: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12788: 19365888 Bnn)
Operation_14733: operator = ElemwiseBinaryBackward, inputs = [Tensor_29523, Tensor_27972, Tensor_27973, nullptr], outputs = [Tensor_29525, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_MUL]
Tensor_29525: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12788: 19365888 Bnn)
Operation_14735: operator = ElemwiseUnaryBackward, inputs = [Tensor_29525, nullptr, nullptr], outputs = [Tensor_29527], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_DIV scal1_float: 0.931818187]
Tensor_29527: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12788: 19365888 Bnn)
Operation_14737: operator = ReshapeBackward, inputs = [Tensor_29527], outputs = [Tensor_29529], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 151296 stride: 768 stride: 1]
Tensor_29529: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12788: 19365888 Bnr)
Operation_14739: operator = LinearBackward, inputs = [Tensor_29529, Tensor_27970, Tensor_27971], outputs = [Tensor_29531, Tensor_29532, Tensor_29536], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->22995
  - Operand: []
Tensor_29531: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12079: 77463552 Bnr)
Tensor_29532: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (12911: 4718592 Bnn)
Operation_22995: operator = AllReduceDuplicated, inputs = [Tensor_29532, nullptr], outputs = [Tensor_29535, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 14739->this
  - Operand: [subop: SUM tag: 22994]
Operation_29534: operator = Consume, inputs = [Tensor_29532], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29535: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12911: 4718592 Bnr)
Tensor_29536: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (29539: 1536 Ban)
Operation_22999: operator = AllReduceDuplicated, inputs = [Tensor_29536, nullptr], outputs = [Tensor_29539, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: SUM tag: 22998]
Operation_29538: operator = Consume, inputs = [Tensor_29536], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29539: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29539: 1536 Bnr)
Operation_14743: operator = TypecastBackward, inputs = [Tensor_29531], outputs = [Tensor_29541], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_29541: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13236: 154927104 Bnr)
Operation_14745: operator = LayernormBackward, inputs = [Tensor_29541, Tensor_27966, Tensor_27967, Tensor_27677, Tensor_27678, Tensor_27968, Tensor_27969], outputs = [Tensor_29543, Tensor_29544, Tensor_29548], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->23007
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_29543: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13554: 154927104 Bnn)
Tensor_29544: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (29547: 12288 Ban)
Operation_23007: operator = AllReduceDuplicated, inputs = [Tensor_29544, nullptr], outputs = [Tensor_29547, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 14745->this
  - Operand: [subop: SUM tag: 23006]
Operation_29546: operator = Consume, inputs = [Tensor_29544], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29547: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29547: 12288 Bnn)
Tensor_29548: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (29551: 12288 Ban)
Operation_23011: operator = AllReduceDuplicated, inputs = [Tensor_29548, nullptr], outputs = [Tensor_29551, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: SUM tag: 23010]
Operation_29550: operator = Consume, inputs = [Tensor_29548], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29551: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29551: 12288 Bnn)
Operation_14749: operator = TypecastBackward, inputs = [Tensor_29543], outputs = [Tensor_29553], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_29553: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13351: 77463552 Bnr)
Operation_14751: operator = TypecastBackward, inputs = [Tensor_29553], outputs = [Tensor_29555], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_29555: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12916: 154927104 Bnn)
Operation_14753: operator = ElemwiseUnaryBackward, inputs = [Tensor_29555, Tensor_27964, Tensor_27965], outputs = [Tensor_29557], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_GELU]
Tensor_29557: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12916: 154927104 Bnn)
Operation_14755: operator = TypecastBackward, inputs = [Tensor_29557], outputs = [Tensor_29559], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_29559: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13510: 77463552 Bnn)
Operation_14757: operator = LinearBackward, inputs = [Tensor_29559, Tensor_27962, Tensor_27963], outputs = [Tensor_29561, Tensor_29562, Tensor_29566], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->23025
  - Operand: []
Tensor_29561: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12782: 19365888 Bnn)
Tensor_29562: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (13566: 4718592 Bnn)
Operation_23025: operator = AllReduceDuplicated, inputs = [Tensor_29562, nullptr], outputs = [Tensor_29565, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 14757->this
  - Operand: [subop: SUM tag: 23024]
Operation_29564: operator = Consume, inputs = [Tensor_29562], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29565: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13566: 4718592 Bnr)
Tensor_29566: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (28544: 6144 Bnn)
Operation_23029: operator = AllReduceDuplicated, inputs = [Tensor_29566, nullptr], outputs = [Tensor_29569, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: SUM tag: 23028]
Operation_29568: operator = Consume, inputs = [Tensor_29566], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29569: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28544: 6144 Bnr)
Operation_14761: operator = TypecastBackward, inputs = [Tensor_29561], outputs = [Tensor_29571], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_29571: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13190: 38731776 Bnn)
Operation_14763: operator = ReshapeBackward, inputs = [Tensor_29571], outputs = [Tensor_29573], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 25216 shape: 768 stride: 768 stride: 1]
Tensor_29573: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13190: 38731776 Bnn)
Operation_14765: operator = LayernormBackward, inputs = [Tensor_29573, Tensor_27958, Tensor_27959, Tensor_27679, Tensor_27680, Tensor_27960, Tensor_27961], outputs = [Tensor_29575, Tensor_29576, Tensor_29580], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->23039
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_29575: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13504: 38731776 Bnn)
Tensor_29576: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (29579: 3072 Ban)
Operation_23039: operator = AllReduceDuplicated, inputs = [Tensor_29576, nullptr], outputs = [Tensor_29579, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 14765->this
  - Operand: [subop: SUM tag: 23038]
Operation_29578: operator = Consume, inputs = [Tensor_29576], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29579: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29579: 3072 Bnn)
Tensor_29580: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (29583: 3072 Ban)
Operation_23043: operator = AllReduceDuplicated, inputs = [Tensor_29580, nullptr], outputs = [Tensor_29583, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: SUM tag: 23042]
Operation_29582: operator = Consume, inputs = [Tensor_29580], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29583: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29583: 3072 Bnn)
Operation_14767: operator = ElemwiseBinaryForward, inputs = [Tensor_29521, Tensor_29575], outputs = [Tensor_29585], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_29585: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13110: 38731776 Bnn)
Operation_14774: operator = ElemwiseUnaryBackward, inputs = [Tensor_29585, nullptr, nullptr], outputs = [Tensor_29587], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 1]
Tensor_29587: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13748: 38731776 Bnn)
Operation_14776: operator = TypecastBackward, inputs = [Tensor_29585], outputs = [Tensor_29589], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_29589: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13259: 19365888 Bnn)
Operation_14778: operator = ElemwiseBinaryBackward, inputs = [Tensor_29589, Tensor_27956, Tensor_27957, nullptr], outputs = [Tensor_29591, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_MUL]
Tensor_29591: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13259: 19365888 Bnn)
Operation_14780: operator = ElemwiseUnaryBackward, inputs = [Tensor_29591, nullptr, nullptr], outputs = [Tensor_29593], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_DIV scal1_float: 0.931818187]
Tensor_29593: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13259: 19365888 Bnn)
Operation_14782: operator = LinearBackward, inputs = [Tensor_29593, Tensor_27954, Tensor_27955], outputs = [Tensor_29595, Tensor_29596, Tensor_29600], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->23059
  - Operand: []
Tensor_29595: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13468: 19365888 Bnn)
Tensor_29596: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (13042: 1179648 Bnn)
Operation_23059: operator = AllReduceDuplicated, inputs = [Tensor_29596, nullptr], outputs = [Tensor_29599, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 14782->this
  - Operand: [subop: SUM tag: 23058]
Operation_29598: operator = Consume, inputs = [Tensor_29596], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29599: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13042: 1179648 Bnr)
Tensor_29600: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (29603: 1536 Ban)
Operation_23063: operator = AllReduceDuplicated, inputs = [Tensor_29600, nullptr], outputs = [Tensor_29603, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: SUM tag: 23062]
Operation_29602: operator = Consume, inputs = [Tensor_29600], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29603: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29603: 1536 Bnr)
Operation_14786: operator = TypecastBackward, inputs = [Tensor_29595], outputs = [Tensor_29605], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_29605: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13027: 38731776 Bnn)
Tensor_29606: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13667: 38731776 Bnn)
Operation_14788: operator = LayernormBackward, inputs = [Tensor_29605, Tensor_29606, Tensor_27951, Tensor_27673, Tensor_27674, Tensor_27952, Tensor_27953], outputs = [Tensor_29608, Tensor_29609, Tensor_29613], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->23073
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_29608: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13430: 38731776 Bnn)
Tensor_29609: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (29612: 3072 Ban)
Operation_23073: operator = AllReduceDuplicated, inputs = [Tensor_29609, nullptr], outputs = [Tensor_29612, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 14788->this
  - Operand: [subop: SUM tag: 23072]
Operation_29611: operator = Consume, inputs = [Tensor_29609], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29612: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29612: 3072 Bnn)
Tensor_29613: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (29616: 3072 Ban)
Operation_23077: operator = AllReduceDuplicated, inputs = [Tensor_29613, nullptr], outputs = [Tensor_29616, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: SUM tag: 23076]
Operation_29615: operator = Consume, inputs = [Tensor_29613], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29616: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29616: 3072 Bnn)
Operation_23080: operator = AllGatherDuplicated, inputs = [Tensor_29608], outputs = [Tensor_29619], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->14792
  - Operand: [tag: 23079]
Operation_29618: operator = Consume, inputs = [Tensor_29608], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29619: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12874: 77463552 Bnn)
Operation_15693: operator = TypecastBackward, inputs = [Tensor_28638], outputs = [Tensor_30794], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Operation_15695: operator = TypecastBackward, inputs = [Tensor_28663], outputs = [Tensor_30795], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Operation_15697: operator = TypecastBackward, inputs = [Tensor_28667], outputs = [Tensor_30796], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Operation_15699: operator = TypecastBackward, inputs = [Tensor_28675], outputs = [Tensor_30797], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Operation_15701: operator = TypecastBackward, inputs = [Tensor_28679], outputs = [Tensor_30798], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Operation_14792: operator = TypecastBackward, inputs = [Tensor_29619], outputs = [Tensor_29626], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 23080->this
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_29626: shape = [128, 197, 768], stride = [768, 98304, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13027: 38731776 Bnn)
Operation_14796: operator = ReshapeBackward, inputs = [Tensor_29626], outputs = [Tensor_29628], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 197 shape: 128 shape: 768 stride: 98304 stride: 768 stride: 1]
Tensor_29628: shape = [197, 1536, 64], stride = [98304, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13027: 38731776 Bnn)
Operation_14798: operator = ContiguousBackward, inputs = [Tensor_29628], outputs = [Tensor_29630], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29630: shape = [197, 1536, 64], stride = [64, 12608, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12713: 38731776 Bnn)
Operation_14800: operator = ReshapeBackward, inputs = [Tensor_29630], outputs = [Tensor_29632], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 197 shape: 1536 shape: 64 stride: 64 stride: 12608 stride: 1]
Tensor_29632: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12713: 38731776 Bnn)
Operation_23090: operator = DistributeWithMemAllocForward, inputs = [Tensor_29632], outputs = [Tensor_29634], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29634: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12734: 19365888 Bnn)
Operation_14802: operator = BatchedMatmulBackward, inputs = [Tensor_29634, Tensor_27949, Tensor_27945], outputs = [Tensor_29636, Tensor_29637], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29636: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13164: 59610624 Bnn)
Tensor_29637: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13577: 19365888 Bnn)
Operation_14805: operator = TypecastBackward, inputs = [Tensor_29636], outputs = [Tensor_29639], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_29639: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13490: 119221248 Bnn)
Operation_14809: operator = SoftmaxBackward, inputs = [Tensor_29639, Tensor_27948], outputs = [Tensor_29641], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dim: 2]
Tensor_29641: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11920: 119221248 Bnn)
Operation_14807: operator = ReshapeBackward, inputs = [Tensor_29637], outputs = [Tensor_29643], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_29643: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13577: 19365888 Bnn)
Operation_14811: operator = ContiguousBackward, inputs = [Tensor_29643], outputs = [Tensor_29645], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29645: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12832: 19365888 Bnn)
Operation_14813: operator = TypecastBackward, inputs = [Tensor_29641], outputs = [Tensor_29647], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_29647: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13164: 59610624 Bnn)
Operation_14817: operator = ReshapeBackward, inputs = [Tensor_29647], outputs = [Tensor_29649], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 197 stride: 38809 stride: 197 stride: 1]
Tensor_29649: shape = [128, 12, 197, 197], stride = [465708, 38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13164: 59610624 Bnn)
Operation_14821: operator = MaskedFillBackward, inputs = [Tensor_29649, Tensor_27947], outputs = [Tensor_29651], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [value_float: -inf]
Tensor_29651: shape = [128, 12, 197, 197], stride = [465708, 38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13164: 59610624 Bnn)
Operation_14819: operator = ReshapeBackward, inputs = [Tensor_29645], outputs = [Tensor_29653], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 12 shape: 64 stride: 151296 stride: 768 stride: 64 stride: 1]
Tensor_29653: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12832: 19365888 Bnn)
Operation_14823: operator = LinearBackward, inputs = [Tensor_29653, Tensor_27942, Tensor_27943], outputs = [Tensor_29655, Tensor_29656, Tensor_29660], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->23115
  - Operand: []
Tensor_29655: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13577: 19365888 Bnn)
Tensor_29656: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (13438: 1179648 Bnn)
Operation_23115: operator = AllReduceDuplicated, inputs = [Tensor_29656, nullptr], outputs = [Tensor_29659, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 14823->this
  - Operand: [subop: SUM tag: 23114]
Operation_29658: operator = Consume, inputs = [Tensor_29656], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29659: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13438: 1179648 Bnr)
Tensor_29660: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (28679: 1536 Bnn)
Operation_23119: operator = AllReduceDuplicated, inputs = [Tensor_29660, nullptr], outputs = [Tensor_29663, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: SUM tag: 23118]
Operation_29662: operator = Consume, inputs = [Tensor_29660], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29663: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28679: 1536 Bnr)
Operation_14829: operator = TypecastBackward, inputs = [Tensor_29655], outputs = [Tensor_29665], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_29665: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13589: 38731776 Bnn)
Operation_14827: operator = ReshapeBackward, inputs = [Tensor_29651], outputs = [Tensor_29667], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 197 stride: 465708 stride: 38809 stride: 197 stride: 1]
Tensor_29667: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13164: 59610624 Bnn)
Operation_14831: operator = BatchedMatmulBackward, inputs = [Tensor_29667, Tensor_27944, Tensor_27946], outputs = [Tensor_29669, Tensor_29670], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29669: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13577: 19365888 Bnn)
Tensor_29670: shape = [1536, 64, 197], stride = [12608, 1, 64], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13631: 19365888 Bnn)
Operation_14834: operator = ReshapeBackward, inputs = [Tensor_29669], outputs = [Tensor_29672], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_29672: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13577: 19365888 Bnr)
Operation_14838: operator = ContiguousBackward, inputs = [Tensor_29672], outputs = [Tensor_29674], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29674: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13215: 19365888 Bnn)
Operation_14840: operator = ReshapeBackward, inputs = [Tensor_29670], outputs = [Tensor_29676], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_29676: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13631: 19365888 Bnn)
Operation_14844: operator = ContiguousBackward, inputs = [Tensor_29676], outputs = [Tensor_29678], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29678: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13466: 19365888 Bnn)
Operation_14846: operator = ReshapeBackward, inputs = [Tensor_29674], outputs = [Tensor_29680], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 12 shape: 64 stride: 151296 stride: 768 stride: 64 stride: 1]
Tensor_29680: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13215: 19365888 Bnn)
Operation_14850: operator = ElemwiseUnaryBackward, inputs = [Tensor_29680, nullptr, nullptr], outputs = [Tensor_29682], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 0.125]
Tensor_29682: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13215: 19365888 Bnn)
Operation_14854: operator = LinearBackward, inputs = [Tensor_29682, Tensor_27938, Tensor_27939], outputs = [Tensor_29684, Tensor_29685, Tensor_29689], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->23144
  - Operand: []
Tensor_29684: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13211: 19365888 Bnr)
Tensor_29685: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (12659: 1179648 Bnn)
Operation_23144: operator = AllReduceDuplicated, inputs = [Tensor_29685, nullptr], outputs = [Tensor_29688, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 14854->this
  - Operand: [subop: SUM tag: 23143]
Operation_29687: operator = Consume, inputs = [Tensor_29685], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29688: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12659: 1179648 Bnr)
Tensor_29689: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (28638: 1536 Bnn)
Operation_23148: operator = AllReduceDuplicated, inputs = [Tensor_29689, nullptr], outputs = [Tensor_29692, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: SUM tag: 23147]
Operation_29691: operator = Consume, inputs = [Tensor_29689], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29692: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28638: 1536 Bnr)
Operation_14852: operator = ReshapeBackward, inputs = [Tensor_29678], outputs = [Tensor_29694], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 12 shape: 64 stride: 151296 stride: 768 stride: 64 stride: 1]
Tensor_29694: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13466: 19365888 Bnn)
Operation_14858: operator = LinearBackward, inputs = [Tensor_29694, Tensor_27940, Tensor_27941], outputs = [Tensor_29696, Tensor_29697, Tensor_29701], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->23156
  - Operand: []
Tensor_29696: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12671: 19365888 Bnr)
Tensor_29697: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (13454: 1179648 Bnn)
Operation_23156: operator = AllReduceDuplicated, inputs = [Tensor_29697, nullptr], outputs = [Tensor_29700, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 14858->this
  - Operand: [subop: SUM tag: 23155]
Operation_29699: operator = Consume, inputs = [Tensor_29697], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29700: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13454: 1179648 Bnr)
Tensor_29701: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (28667: 1536 Bnn)
Operation_23160: operator = AllReduceDuplicated, inputs = [Tensor_29701, nullptr], outputs = [Tensor_29704, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: SUM tag: 23159]
Operation_29703: operator = Consume, inputs = [Tensor_29701], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29704: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28667: 1536 Bnr)
Operation_14862: operator = TypecastBackward, inputs = [Tensor_29684], outputs = [Tensor_29706], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_29706: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13271: 38731776 Bnn)
Operation_14864: operator = ElemwiseBinaryForward, inputs = [Tensor_29665, Tensor_29706], outputs = [Tensor_29708], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_29708: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13589: 38731776 Bnn)
Operation_14866: operator = TypecastBackward, inputs = [Tensor_29696], outputs = [Tensor_29710], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_29710: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13663: 38731776 Bnn)
Operation_14868: operator = ElemwiseBinaryForward, inputs = [Tensor_29708, Tensor_29710], outputs = [Tensor_29712], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_29712: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13589: 38731776 Bnn)
Operation_14870: operator = LayernormBackward, inputs = [Tensor_29712, Tensor_27934, Tensor_27935, Tensor_27675, Tensor_27676, Tensor_27936, Tensor_27937], outputs = [Tensor_29714, Tensor_29715, Tensor_29719], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->23174
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_29714: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13112: 38731776 Bnn)
Tensor_29715: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (29718: 3072 Ban)
Operation_23174: operator = AllReduceDuplicated, inputs = [Tensor_29715, nullptr], outputs = [Tensor_29718, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 14870->this
  - Operand: [subop: SUM tag: 23173]
Operation_29717: operator = Consume, inputs = [Tensor_29715], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29718: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29718: 3072 Bnn)
Tensor_29719: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (29722: 3072 Ban)
Operation_23178: operator = AllReduceDuplicated, inputs = [Tensor_29719, nullptr], outputs = [Tensor_29722, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: SUM tag: 23177]
Operation_29721: operator = Consume, inputs = [Tensor_29719], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29722: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29722: 3072 Bnn)
Operation_14872: operator = ElemwiseBinaryForward, inputs = [Tensor_29587, Tensor_29714], outputs = [Tensor_29724], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_29724: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13748: 38731776 Bnn)
Operation_14879: operator = ElemwiseUnaryBackward, inputs = [Tensor_29724, nullptr, nullptr], outputs = [Tensor_29726], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 1]
Tensor_29726: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12633: 38731776 Bnn)
Operation_14881: operator = TypecastBackward, inputs = [Tensor_29724], outputs = [Tensor_29728], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_29728: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13215: 19365888 Bnn)
Operation_14883: operator = ElemwiseBinaryBackward, inputs = [Tensor_29728, Tensor_27932, Tensor_27933, nullptr], outputs = [Tensor_29730, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_MUL]
Tensor_29730: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13215: 19365888 Bnn)
Operation_14885: operator = ElemwiseUnaryBackward, inputs = [Tensor_29730, nullptr, nullptr], outputs = [Tensor_29732], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_DIV scal1_float: 0.945454538]
Tensor_29732: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13215: 19365888 Bnn)
Operation_14887: operator = ReshapeBackward, inputs = [Tensor_29732], outputs = [Tensor_29734], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 151296 stride: 768 stride: 1]
Tensor_29734: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13215: 19365888 Bnr)
Operation_14889: operator = LinearBackward, inputs = [Tensor_29734, Tensor_27930, Tensor_27931], outputs = [Tensor_29736, Tensor_29737, Tensor_29741], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->23196
  - Operand: []
Tensor_29736: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13073: 77463552 Bnn)
Tensor_29737: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (12752: 4718592 Bnn)
Operation_23196: operator = AllReduceDuplicated, inputs = [Tensor_29737, nullptr], outputs = [Tensor_29740, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 14889->this
  - Operand: [subop: SUM tag: 23195]
Operation_29739: operator = Consume, inputs = [Tensor_29737], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29740: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12752: 4718592 Bnr)
Tensor_29741: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (29744: 1536 Ban)
Operation_23200: operator = AllReduceDuplicated, inputs = [Tensor_29741, nullptr], outputs = [Tensor_29744, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: SUM tag: 23199]
Operation_29743: operator = Consume, inputs = [Tensor_29741], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29744: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29744: 1536 Bnr)
Operation_14893: operator = TypecastBackward, inputs = [Tensor_29736], outputs = [Tensor_29746], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_29746: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13554: 154927104 Bnn)
Operation_14895: operator = LayernormBackward, inputs = [Tensor_29746, Tensor_27926, Tensor_27927, Tensor_27669, Tensor_27670, Tensor_27928, Tensor_27929], outputs = [Tensor_29748, Tensor_29749, Tensor_29753], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->23208
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_29748: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13713: 154927104 Bnn)
Tensor_29749: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (29752: 12288 Ban)
Operation_23208: operator = AllReduceDuplicated, inputs = [Tensor_29749, nullptr], outputs = [Tensor_29752, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 14895->this
  - Operand: [subop: SUM tag: 23207]
Operation_29751: operator = Consume, inputs = [Tensor_29749], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29752: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29752: 12288 Bnn)
Tensor_29753: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (29756: 12288 Ban)
Operation_23212: operator = AllReduceDuplicated, inputs = [Tensor_29753, nullptr], outputs = [Tensor_29756, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: SUM tag: 23211]
Operation_29755: operator = Consume, inputs = [Tensor_29753], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29756: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29756: 12288 Bnn)
Operation_14899: operator = TypecastBackward, inputs = [Tensor_29748], outputs = [Tensor_29758], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_29758: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13510: 77463552 Bnn)
Operation_14901: operator = TypecastBackward, inputs = [Tensor_29758], outputs = [Tensor_29760], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_29760: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12757: 154927104 Bnn)
Operation_14903: operator = ElemwiseUnaryBackward, inputs = [Tensor_29760, Tensor_27924, Tensor_27925], outputs = [Tensor_29762], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_GELU]
Tensor_29762: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12757: 154927104 Bnn)
Operation_14905: operator = TypecastBackward, inputs = [Tensor_29762], outputs = [Tensor_29764], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_29764: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13669: 77463552 Bnn)
Operation_14907: operator = LinearBackward, inputs = [Tensor_29764, Tensor_27922, Tensor_27923], outputs = [Tensor_29766, Tensor_29767, Tensor_29771], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->23226
  - Operand: []
Tensor_29766: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13631: 19365888 Bnr)
Tensor_29767: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (13070: 4718592 Bnn)
Operation_23226: operator = AllReduceDuplicated, inputs = [Tensor_29767, nullptr], outputs = [Tensor_29770, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 14907->this
  - Operand: [subop: SUM tag: 23225]
Operation_29769: operator = Consume, inputs = [Tensor_29767], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29770: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13070: 4718592 Bnr)
Tensor_29771: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (29774: 6144 Ban)
Operation_23230: operator = AllReduceDuplicated, inputs = [Tensor_29771, nullptr], outputs = [Tensor_29774, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: SUM tag: 23229]
Operation_29773: operator = Consume, inputs = [Tensor_29771], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29774: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29774: 6144 Bnr)
Operation_14911: operator = TypecastBackward, inputs = [Tensor_29766], outputs = [Tensor_29776], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_29776: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12743: 38731776 Bnn)
Operation_14913: operator = ReshapeBackward, inputs = [Tensor_29776], outputs = [Tensor_29778], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 25216 shape: 768 stride: 768 stride: 1]
Tensor_29778: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12743: 38731776 Bnn)
Operation_14915: operator = LayernormBackward, inputs = [Tensor_29778, Tensor_27918, Tensor_27919, Tensor_27671, Tensor_27672, Tensor_27920, Tensor_27921], outputs = [Tensor_29780, Tensor_29781, Tensor_29785], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->23240
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_29780: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12709: 38731776 Bnn)
Tensor_29781: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (29784: 3072 Ban)
Operation_23240: operator = AllReduceDuplicated, inputs = [Tensor_29781, nullptr], outputs = [Tensor_29784, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 14915->this
  - Operand: [subop: SUM tag: 23239]
Operation_29783: operator = Consume, inputs = [Tensor_29781], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29784: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29784: 3072 Bnn)
Tensor_29785: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (29788: 3072 Ban)
Operation_23244: operator = AllReduceDuplicated, inputs = [Tensor_29785, nullptr], outputs = [Tensor_29788, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: SUM tag: 23243]
Operation_29787: operator = Consume, inputs = [Tensor_29785], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29788: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29788: 3072 Bnn)
Operation_14917: operator = ElemwiseBinaryForward, inputs = [Tensor_29726, Tensor_29780], outputs = [Tensor_29790], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_29790: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12633: 38731776 Bnn)
Operation_14924: operator = ElemwiseUnaryBackward, inputs = [Tensor_29790, nullptr, nullptr], outputs = [Tensor_29792], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 1]
Tensor_29792: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13190: 38731776 Bnn)
Operation_14926: operator = TypecastBackward, inputs = [Tensor_29790], outputs = [Tensor_29794], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_29794: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12657: 19365888 Bnn)
Operation_14928: operator = ElemwiseBinaryBackward, inputs = [Tensor_29794, Tensor_27916, Tensor_27917, nullptr], outputs = [Tensor_29796, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_MUL]
Tensor_29796: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12657: 19365888 Bnn)
Operation_14930: operator = ElemwiseUnaryBackward, inputs = [Tensor_29796, nullptr, nullptr], outputs = [Tensor_29798], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_DIV scal1_float: 0.945454538]
Tensor_29798: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12657: 19365888 Bnn)
Operation_14932: operator = LinearBackward, inputs = [Tensor_29798, Tensor_27914, Tensor_27915], outputs = [Tensor_29800, Tensor_29801, Tensor_29805], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->23260
  - Operand: []
Tensor_29800: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13460: 19365888 Bnn)
Tensor_29801: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (12651: 1179648 Bnn)
Operation_23260: operator = AllReduceDuplicated, inputs = [Tensor_29801, nullptr], outputs = [Tensor_29804, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 14932->this
  - Operand: [subop: SUM tag: 23259]
Operation_29803: operator = Consume, inputs = [Tensor_29801], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29804: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12651: 1179648 Bnr)
Tensor_29805: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (29808: 1536 Ban)
Operation_23264: operator = AllReduceDuplicated, inputs = [Tensor_29805, nullptr], outputs = [Tensor_29808, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: SUM tag: 23263]
Operation_29807: operator = Consume, inputs = [Tensor_29805], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29808: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29808: 1536 Bnr)
Operation_14936: operator = TypecastBackward, inputs = [Tensor_29800], outputs = [Tensor_29810], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_29810: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13110: 38731776 Bnn)
Tensor_29811: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13697: 38731776 Bnn)
Operation_14938: operator = LayernormBackward, inputs = [Tensor_29810, Tensor_29811, Tensor_27911, Tensor_27665, Tensor_27666, Tensor_27912, Tensor_27913], outputs = [Tensor_29813, Tensor_29814, Tensor_29818], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->23274
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_29813: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13504: 38731776 Bnn)
Tensor_29814: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (29817: 3072 Ban)
Operation_23274: operator = AllReduceDuplicated, inputs = [Tensor_29814, nullptr], outputs = [Tensor_29817, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 14938->this
  - Operand: [subop: SUM tag: 23273]
Operation_29816: operator = Consume, inputs = [Tensor_29814], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29817: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29817: 3072 Bnn)
Tensor_29818: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (29821: 3072 Ban)
Operation_23278: operator = AllReduceDuplicated, inputs = [Tensor_29818, nullptr], outputs = [Tensor_29821, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: SUM tag: 23277]
Operation_29820: operator = Consume, inputs = [Tensor_29818], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29821: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29821: 3072 Bnn)
Operation_23281: operator = AllGatherDuplicated, inputs = [Tensor_29813], outputs = [Tensor_29824], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->14942
  - Operand: [tag: 23280]
Operation_29823: operator = Consume, inputs = [Tensor_29813], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29824: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13510: 77463552 Bnn)
Operation_15703: operator = TypecastBackward, inputs = [Tensor_28715], outputs = [Tensor_30799], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Operation_15705: operator = TypecastBackward, inputs = [Tensor_28719], outputs = [Tensor_30800], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Operation_15707: operator = TypecastBackward, inputs = [Tensor_28745], outputs = [Tensor_30801], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Operation_15709: operator = TypecastBackward, inputs = [Tensor_28749], outputs = [Tensor_30802], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Operation_15711: operator = TypecastBackward, inputs = [Tensor_28779], outputs = [Tensor_30803], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Operation_14942: operator = TypecastBackward, inputs = [Tensor_29824], outputs = [Tensor_29831], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 23281->this
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_29831: shape = [128, 197, 768], stride = [768, 98304, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12584: 38731776 Bnn)
Operation_14946: operator = ReshapeBackward, inputs = [Tensor_29831], outputs = [Tensor_29833], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 197 shape: 128 shape: 768 stride: 98304 stride: 768 stride: 1]
Tensor_29833: shape = [197, 1536, 64], stride = [98304, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12584: 38731776 Bnn)
Operation_14948: operator = ContiguousBackward, inputs = [Tensor_29833], outputs = [Tensor_29835], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29835: shape = [197, 1536, 64], stride = [64, 12608, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12709: 38731776 Bnn)
Operation_14950: operator = ReshapeBackward, inputs = [Tensor_29835], outputs = [Tensor_29837], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 197 shape: 1536 shape: 64 stride: 64 stride: 12608 stride: 1]
Tensor_29837: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12709: 38731776 Bnn)
Operation_23291: operator = DistributeWithMemAllocForward, inputs = [Tensor_29837], outputs = [Tensor_29839], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29839: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12782: 19365888 Bnr)
Operation_14952: operator = BatchedMatmulBackward, inputs = [Tensor_29839, Tensor_27909, Tensor_27905], outputs = [Tensor_29841, Tensor_29842], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29841: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13482: 59610624 Bnn)
Tensor_29842: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12623: 19365888 Bnn)
Operation_14955: operator = TypecastBackward, inputs = [Tensor_29841], outputs = [Tensor_29844], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_29844: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11920: 119221248 Bnn)
Operation_14959: operator = SoftmaxBackward, inputs = [Tensor_29844, Tensor_27908], outputs = [Tensor_29846], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dim: 2]
Tensor_29846: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13490: 119221248 Bnn)
Operation_14957: operator = ReshapeBackward, inputs = [Tensor_29842], outputs = [Tensor_29848], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_29848: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12623: 19365888 Bnn)
Operation_14961: operator = ContiguousBackward, inputs = [Tensor_29848], outputs = [Tensor_29850], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29850: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13692: 19365888 Bnn)
Operation_14963: operator = TypecastBackward, inputs = [Tensor_29846], outputs = [Tensor_29852], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_29852: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12846: 59610624 Bnn)
Operation_14967: operator = ReshapeBackward, inputs = [Tensor_29852], outputs = [Tensor_29854], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 197 stride: 38809 stride: 197 stride: 1]
Tensor_29854: shape = [128, 12, 197, 197], stride = [465708, 38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12846: 59610624 Bnn)
Operation_14971: operator = MaskedFillBackward, inputs = [Tensor_29854, Tensor_27907], outputs = [Tensor_29856], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [value_float: -inf]
Tensor_29856: shape = [128, 12, 197, 197], stride = [465708, 38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12846: 59610624 Bnn)
Operation_14969: operator = ReshapeBackward, inputs = [Tensor_29850], outputs = [Tensor_29858], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 12 shape: 64 stride: 151296 stride: 768 stride: 64 stride: 1]
Tensor_29858: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13692: 19365888 Bnr)
Operation_14973: operator = LinearBackward, inputs = [Tensor_29858, Tensor_27902, Tensor_27903], outputs = [Tensor_29860, Tensor_29861, Tensor_29865], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->23316
  - Operand: []
Tensor_29860: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13150: 19365888 Bnn)
Tensor_29861: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (13446: 1179648 Bnn)
Operation_23316: operator = AllReduceDuplicated, inputs = [Tensor_29861, nullptr], outputs = [Tensor_29864, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 14973->this
  - Operand: [subop: SUM tag: 23315]
Operation_29863: operator = Consume, inputs = [Tensor_29861], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29864: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13446: 1179648 Bnr)
Tensor_29865: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (28719: 1536 Bnn)
Operation_23320: operator = AllReduceDuplicated, inputs = [Tensor_29865, nullptr], outputs = [Tensor_29868, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: SUM tag: 23319]
Operation_29867: operator = Consume, inputs = [Tensor_29865], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29868: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28719: 1536 Bnr)
Operation_14979: operator = TypecastBackward, inputs = [Tensor_29860], outputs = [Tensor_29870], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_29870: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12584: 38731776 Bnn)
Operation_14977: operator = ReshapeBackward, inputs = [Tensor_29856], outputs = [Tensor_29872], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 197 stride: 465708 stride: 38809 stride: 197 stride: 1]
Tensor_29872: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12846: 59610624 Bnn)
Operation_14981: operator = BatchedMatmulBackward, inputs = [Tensor_29872, Tensor_27904, Tensor_27906], outputs = [Tensor_29874, Tensor_29875], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29874: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13150: 19365888 Bnn)
Tensor_29875: shape = [1536, 64, 197], stride = [12608, 1, 64], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13583: 19365888 Bnn)
Operation_14984: operator = ReshapeBackward, inputs = [Tensor_29874], outputs = [Tensor_29877], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_29877: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13150: 19365888 Bnn)
Operation_14988: operator = ContiguousBackward, inputs = [Tensor_29877], outputs = [Tensor_29879], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29879: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12498: 19365888 Bnn)
Operation_14990: operator = ReshapeBackward, inputs = [Tensor_29875], outputs = [Tensor_29881], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_29881: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13583: 19365888 Bnn)
Operation_14994: operator = ContiguousBackward, inputs = [Tensor_29881], outputs = [Tensor_29883], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29883: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12512: 19365888 Bnn)
Operation_14996: operator = ReshapeBackward, inputs = [Tensor_29879], outputs = [Tensor_29885], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 12 shape: 64 stride: 151296 stride: 768 stride: 64 stride: 1]
Tensor_29885: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12498: 19365888 Bnn)
Operation_15000: operator = ElemwiseUnaryBackward, inputs = [Tensor_29885, nullptr, nullptr], outputs = [Tensor_29887], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 0.125]
Tensor_29887: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12498: 19365888 Bnn)
Operation_15004: operator = LinearBackward, inputs = [Tensor_29887, Tensor_27898, Tensor_27899], outputs = [Tensor_29889, Tensor_29890, Tensor_29894], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->23345
  - Operand: []
Tensor_29889: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13150: 19365888 Bnr)
Tensor_29890: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (12500: 1179648 Bnn)
Operation_23345: operator = AllReduceDuplicated, inputs = [Tensor_29890, nullptr], outputs = [Tensor_29893, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 15004->this
  - Operand: [subop: SUM tag: 23344]
Operation_29892: operator = Consume, inputs = [Tensor_29890], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29893: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12500: 1179648 Bnr)
Tensor_29894: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (29897: 1536 Ban)
Operation_23349: operator = AllReduceDuplicated, inputs = [Tensor_29894, nullptr], outputs = [Tensor_29897, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: SUM tag: 23348]
Operation_29896: operator = Consume, inputs = [Tensor_29894], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29897: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29897: 1536 Bnr)
Operation_15002: operator = ReshapeBackward, inputs = [Tensor_29883], outputs = [Tensor_29899], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 12 shape: 64 stride: 151296 stride: 768 stride: 64 stride: 1]
Tensor_29899: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12512: 19365888 Bnr)
Operation_15008: operator = LinearBackward, inputs = [Tensor_29899, Tensor_27900, Tensor_27901], outputs = [Tensor_29901, Tensor_29902, Tensor_29906], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->23357
  - Operand: []
Tensor_29901: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12506: 19365888 Bnr)
Tensor_29902: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (13597: 1179648 Bnn)
Operation_23357: operator = AllReduceDuplicated, inputs = [Tensor_29902, nullptr], outputs = [Tensor_29905, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 15008->this
  - Operand: [subop: SUM tag: 23356]
Operation_29904: operator = Consume, inputs = [Tensor_29902], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29905: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13597: 1179648 Bnr)
Tensor_29906: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (29909: 1536 Ban)
Operation_23361: operator = AllReduceDuplicated, inputs = [Tensor_29906, nullptr], outputs = [Tensor_29909, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: SUM tag: 23360]
Operation_29908: operator = Consume, inputs = [Tensor_29906], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29909: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29909: 1536 Bnr)
Operation_15012: operator = TypecastBackward, inputs = [Tensor_29889], outputs = [Tensor_29911], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_29911: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13027: 38731776 Bnn)
Operation_15014: operator = ElemwiseBinaryForward, inputs = [Tensor_29870, Tensor_29911], outputs = [Tensor_29913], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_29913: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12584: 38731776 Bnn)
Operation_15016: operator = TypecastBackward, inputs = [Tensor_29901], outputs = [Tensor_29915], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_29915: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13667: 38731776 Bnn)
Operation_15018: operator = ElemwiseBinaryForward, inputs = [Tensor_29913, Tensor_29915], outputs = [Tensor_29917], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_29917: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12584: 38731776 Bnn)
Operation_15020: operator = LayernormBackward, inputs = [Tensor_29917, Tensor_27894, Tensor_27895, Tensor_27667, Tensor_27668, Tensor_27896, Tensor_27897], outputs = [Tensor_29919, Tensor_29920, Tensor_29924], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->23375
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_29919: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13430: 38731776 Bnn)
Tensor_29920: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (29923: 3072 Ban)
Operation_23375: operator = AllReduceDuplicated, inputs = [Tensor_29920, nullptr], outputs = [Tensor_29923, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 15020->this
  - Operand: [subop: SUM tag: 23374]
Operation_29922: operator = Consume, inputs = [Tensor_29920], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29923: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29923: 3072 Bnn)
Tensor_29924: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (29927: 3072 Ban)
Operation_23379: operator = AllReduceDuplicated, inputs = [Tensor_29924, nullptr], outputs = [Tensor_29927, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: SUM tag: 23378]
Operation_29926: operator = Consume, inputs = [Tensor_29924], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29927: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29927: 3072 Bnn)
Operation_15022: operator = ElemwiseBinaryForward, inputs = [Tensor_29792, Tensor_29919], outputs = [Tensor_29929], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_29929: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13190: 38731776 Bnn)
Operation_15029: operator = ElemwiseUnaryBackward, inputs = [Tensor_29929, nullptr, nullptr], outputs = [Tensor_29931], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 1]
Tensor_29931: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13430: 38731776 Bnn)
Operation_15031: operator = TypecastBackward, inputs = [Tensor_29929], outputs = [Tensor_29933], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_29933: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13583: 19365888 Bnn)
Operation_15033: operator = ElemwiseBinaryBackward, inputs = [Tensor_29933, Tensor_27892, Tensor_27893, nullptr], outputs = [Tensor_29935, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_MUL]
Tensor_29935: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13583: 19365888 Bnn)
Operation_15035: operator = ElemwiseUnaryBackward, inputs = [Tensor_29935, nullptr, nullptr], outputs = [Tensor_29937], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_DIV scal1_float: 0.959090889]
Tensor_29937: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13583: 19365888 Bnn)
Operation_15037: operator = ReshapeBackward, inputs = [Tensor_29937], outputs = [Tensor_29939], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 151296 stride: 768 stride: 1]
Tensor_29939: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13583: 19365888 Bnr)
Operation_15039: operator = LinearBackward, inputs = [Tensor_29939, Tensor_27890, Tensor_27891], outputs = [Tensor_29941, Tensor_29942, Tensor_29946], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->23397
  - Operand: []
Tensor_29941: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12397: 77463552 Bnr)
Tensor_29942: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (13547: 4718592 Bnn)
Operation_23397: operator = AllReduceDuplicated, inputs = [Tensor_29942, nullptr], outputs = [Tensor_29945, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 15039->this
  - Operand: [subop: SUM tag: 23396]
Operation_29944: operator = Consume, inputs = [Tensor_29942], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29945: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13547: 4718592 Bnr)
Tensor_29946: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (29949: 1536 Ban)
Operation_23401: operator = AllReduceDuplicated, inputs = [Tensor_29946, nullptr], outputs = [Tensor_29949, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: SUM tag: 23400]
Operation_29948: operator = Consume, inputs = [Tensor_29946], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29949: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29949: 1536 Bnr)
Operation_15043: operator = TypecastBackward, inputs = [Tensor_29941], outputs = [Tensor_29951], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_29951: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12759: 154927104 Bnr)
Operation_15045: operator = LayernormBackward, inputs = [Tensor_29951, Tensor_27886, Tensor_27887, Tensor_27661, Tensor_27662, Tensor_27888, Tensor_27889], outputs = [Tensor_29953, Tensor_29954, Tensor_29958], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->23409
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_29953: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12916: 154927104 Bnr)
Tensor_29954: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (29957: 12288 Ban)
Operation_23409: operator = AllReduceDuplicated, inputs = [Tensor_29954, nullptr], outputs = [Tensor_29957, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 15045->this
  - Operand: [subop: SUM tag: 23408]
Operation_29956: operator = Consume, inputs = [Tensor_29954], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29957: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29957: 12288 Bnn)
Tensor_29958: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (29961: 12288 Ban)
Operation_23413: operator = AllReduceDuplicated, inputs = [Tensor_29958, nullptr], outputs = [Tensor_29961, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: SUM tag: 23412]
Operation_29960: operator = Consume, inputs = [Tensor_29958], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29961: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29961: 12288 Bnn)
Operation_15049: operator = TypecastBackward, inputs = [Tensor_29953], outputs = [Tensor_29963], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_29963: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12874: 77463552 Bnr)
Operation_15051: operator = TypecastBackward, inputs = [Tensor_29963], outputs = [Tensor_29965], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_29965: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12604: 154927104 Bnn)
Operation_15053: operator = ElemwiseUnaryBackward, inputs = [Tensor_29965, Tensor_27884, Tensor_27885], outputs = [Tensor_29967], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_GELU]
Tensor_29967: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12604: 154927104 Bnr)
Operation_15055: operator = TypecastBackward, inputs = [Tensor_29967], outputs = [Tensor_29969], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_29969: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13073: 77463552 Bnr)
Operation_15057: operator = LinearBackward, inputs = [Tensor_29969, Tensor_27882, Tensor_27883], outputs = [Tensor_29971, Tensor_29972, Tensor_29976], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->23427
  - Operand: []
Tensor_29971: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13056: 19365888 Bnr)
Tensor_29972: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (13407: 4718592 Bnn)
Operation_23427: operator = AllReduceDuplicated, inputs = [Tensor_29972, nullptr], outputs = [Tensor_29975, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 15057->this
  - Operand: [subop: SUM tag: 23426]
Operation_29974: operator = Consume, inputs = [Tensor_29972], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29975: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13407: 4718592 Bnr)
Tensor_29976: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (28749: 6144 Bnn)
Operation_23431: operator = AllReduceDuplicated, inputs = [Tensor_29976, nullptr], outputs = [Tensor_29979, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: SUM tag: 23430]
Operation_29978: operator = Consume, inputs = [Tensor_29976], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29979: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28749: 6144 Bnr)
Operation_15061: operator = TypecastBackward, inputs = [Tensor_29971], outputs = [Tensor_29981], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_29981: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12713: 38731776 Bnn)
Operation_15063: operator = ReshapeBackward, inputs = [Tensor_29981], outputs = [Tensor_29983], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 25216 shape: 768 stride: 768 stride: 1]
Tensor_29983: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12713: 38731776 Bnr)
Operation_15065: operator = LayernormBackward, inputs = [Tensor_29983, Tensor_27878, Tensor_27879, Tensor_27663, Tensor_27664, Tensor_27880, Tensor_27881], outputs = [Tensor_29985, Tensor_29986, Tensor_29990], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->23441
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_29985: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13271: 38731776 Bnr)
Tensor_29986: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (29989: 3072 Ban)
Operation_23441: operator = AllReduceDuplicated, inputs = [Tensor_29986, nullptr], outputs = [Tensor_29989, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 15065->this
  - Operand: [subop: SUM tag: 23440]
Operation_29988: operator = Consume, inputs = [Tensor_29986], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29989: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29989: 3072 Bnn)
Tensor_29990: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (29993: 3072 Ban)
Operation_23445: operator = AllReduceDuplicated, inputs = [Tensor_29990, nullptr], outputs = [Tensor_29993, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: SUM tag: 23444]
Operation_29992: operator = Consume, inputs = [Tensor_29990], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_29993: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29993: 3072 Bnn)
Operation_15067: operator = ElemwiseBinaryForward, inputs = [Tensor_29931, Tensor_29985], outputs = [Tensor_29995], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_29995: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13430: 38731776 Bnr)
Operation_15074: operator = ElemwiseUnaryBackward, inputs = [Tensor_29995, nullptr, nullptr], outputs = [Tensor_29997], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 1]
Tensor_29997: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13663: 38731776 Bnn)
Operation_15076: operator = TypecastBackward, inputs = [Tensor_29995], outputs = [Tensor_29999], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_29999: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13625: 19365888 Bnn)
Operation_15078: operator = ElemwiseBinaryBackward, inputs = [Tensor_29999, Tensor_27876, Tensor_27877, nullptr], outputs = [Tensor_30001, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_MUL]
Tensor_30001: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13625: 19365888 Bnn)
Operation_15080: operator = ElemwiseUnaryBackward, inputs = [Tensor_30001, nullptr, nullptr], outputs = [Tensor_30003], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_DIV scal1_float: 0.959090889]
Tensor_30003: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13625: 19365888 Bnr)
Operation_15082: operator = LinearBackward, inputs = [Tensor_30003, Tensor_27874, Tensor_27875], outputs = [Tensor_30005, Tensor_30006, Tensor_30010], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->23461
  - Operand: []
Tensor_30005: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13142: 19365888 Bnr)
Tensor_30006: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (12977: 1179648 Bnn)
Operation_23461: operator = AllReduceDuplicated, inputs = [Tensor_30006, nullptr], outputs = [Tensor_30009, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 15082->this
  - Operand: [subop: SUM tag: 23460]
Operation_30008: operator = Consume, inputs = [Tensor_30006], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_30009: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12977: 1179648 Bnr)
Tensor_30010: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (30013: 1536 Ban)
Operation_23465: operator = AllReduceDuplicated, inputs = [Tensor_30010, nullptr], outputs = [Tensor_30013, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: SUM tag: 23464]
Operation_30012: operator = Consume, inputs = [Tensor_30010], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_30013: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30013: 1536 Bnr)
Operation_15086: operator = TypecastBackward, inputs = [Tensor_30005], outputs = [Tensor_30015], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_30015: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13112: 38731776 Bnr)
Tensor_30016: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (28345: 38731776 Bnn)
Operation_15088: operator = LayernormBackward, inputs = [Tensor_30015, Tensor_30016, Tensor_27871, Tensor_27657, Tensor_27658, Tensor_27872, Tensor_27873], outputs = [Tensor_30018, Tensor_30019, Tensor_30023], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->23475
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_30018: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12635: 38731776 Bnr)
Tensor_30019: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (30022: 3072 Ban)
Operation_23475: operator = AllReduceDuplicated, inputs = [Tensor_30019, nullptr], outputs = [Tensor_30022, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 15088->this
  - Operand: [subop: SUM tag: 23474]
Operation_30021: operator = Consume, inputs = [Tensor_30019], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_30022: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30022: 3072 Bnn)
Tensor_30023: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (30026: 3072 Ban)
Operation_23479: operator = AllReduceDuplicated, inputs = [Tensor_30023, nullptr], outputs = [Tensor_30026, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: SUM tag: 23478]
Operation_30025: operator = Consume, inputs = [Tensor_30023], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_30026: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30026: 3072 Bnn)
Operation_23482: operator = AllGatherDuplicated, inputs = [Tensor_30018], outputs = [Tensor_30029], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->15092
  - Operand: [tag: 23481]
Operation_30028: operator = Consume, inputs = [Tensor_30018], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_30029: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12596: 77463552 Bnn)
Operation_15713: operator = TypecastBackward, inputs = [Tensor_28783], outputs = [Tensor_30804], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Operation_15715: operator = TypecastBackward, inputs = [Tensor_28839], outputs = [Tensor_30805], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Operation_15717: operator = TypecastBackward, inputs = [Tensor_28843], outputs = [Tensor_30806], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Operation_15719: operator = TypecastBackward, inputs = [Tensor_28868], outputs = [Tensor_30807], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Operation_15721: operator = TypecastBackward, inputs = [Tensor_28872], outputs = [Tensor_30808], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Operation_15092: operator = TypecastBackward, inputs = [Tensor_30029], outputs = [Tensor_30036], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 23482->this
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_30036: shape = [128, 197, 768], stride = [768, 98304, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13748: 38731776 Bnn)
Operation_15096: operator = ReshapeBackward, inputs = [Tensor_30036], outputs = [Tensor_30038], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 197 shape: 128 shape: 768 stride: 98304 stride: 768 stride: 1]
Tensor_30038: shape = [197, 1536, 64], stride = [98304, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13748: 38731776 Bnr)
Operation_15098: operator = ContiguousBackward, inputs = [Tensor_30038], outputs = [Tensor_30040], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_30040: shape = [197, 1536, 64], stride = [64, 12608, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13589: 38731776 Bnn)
Operation_15100: operator = ReshapeBackward, inputs = [Tensor_30040], outputs = [Tensor_30042], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 197 shape: 1536 shape: 64 stride: 64 stride: 12608 stride: 1]
Tensor_30042: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13589: 38731776 Bnn)
Operation_23492: operator = DistributeWithMemAllocForward, inputs = [Tensor_30042], outputs = [Tensor_30044], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_30044: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13603: 19365888 Bnn)
Operation_15102: operator = BatchedMatmulBackward, inputs = [Tensor_30044, Tensor_27869, Tensor_27865], outputs = [Tensor_30046, Tensor_30047], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_30046: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13164: 59610624 Bnn)
Tensor_30047: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13418: 19365888 Bnn)
Operation_15105: operator = TypecastBackward, inputs = [Tensor_30046], outputs = [Tensor_30049], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_30049: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12695: 119221248 Bnn)
Operation_15109: operator = SoftmaxBackward, inputs = [Tensor_30049, Tensor_27868], outputs = [Tensor_30051], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dim: 2]
Tensor_30051: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12536: 119221248 Bnn)
Operation_15107: operator = ReshapeBackward, inputs = [Tensor_30047], outputs = [Tensor_30053], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_30053: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13418: 19365888 Bnr)
Operation_15111: operator = ContiguousBackward, inputs = [Tensor_30053], outputs = [Tensor_30055], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_30055: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13460: 19365888 Bnn)
Operation_15113: operator = TypecastBackward, inputs = [Tensor_30051], outputs = [Tensor_30057], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_30057: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12687: 59610624 Bnn)
Operation_15117: operator = ReshapeBackward, inputs = [Tensor_30057], outputs = [Tensor_30059], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 197 stride: 38809 stride: 197 stride: 1]
Tensor_30059: shape = [128, 12, 197, 197], stride = [465708, 38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12687: 59610624 Bnn)
Operation_15121: operator = MaskedFillBackward, inputs = [Tensor_30059, Tensor_27867], outputs = [Tensor_30061], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [value_float: -inf]
Tensor_30061: shape = [128, 12, 197, 197], stride = [465708, 38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12687: 59610624 Bnn)
Operation_15119: operator = ReshapeBackward, inputs = [Tensor_30055], outputs = [Tensor_30063], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 12 shape: 64 stride: 151296 stride: 768 stride: 64 stride: 1]
Tensor_30063: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13460: 19365888 Bnn)
Operation_15123: operator = LinearBackward, inputs = [Tensor_30063, Tensor_27862, Tensor_27863], outputs = [Tensor_30065, Tensor_30066, Tensor_30070], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->23517
  - Operand: []
Tensor_30065: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13603: 19365888 Bnn)
Tensor_30066: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (13360: 1179648 Bnn)
Operation_23517: operator = AllReduceDuplicated, inputs = [Tensor_30066, nullptr], outputs = [Tensor_30069, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 15123->this
  - Operand: [subop: SUM tag: 23516]
Operation_30068: operator = Consume, inputs = [Tensor_30066], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_30069: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13360: 1179648 Bnr)
Tensor_30070: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (11821: 1536 Bnn)
Operation_23521: operator = AllReduceDuplicated, inputs = [Tensor_30070, nullptr], outputs = [Tensor_30073, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: SUM tag: 23520]
Operation_30072: operator = Consume, inputs = [Tensor_30070], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_30073: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11821: 1536 Bnr)
Operation_15129: operator = TypecastBackward, inputs = [Tensor_30065], outputs = [Tensor_30075], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_30075: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12709: 38731776 Bnn)
Operation_15127: operator = ReshapeBackward, inputs = [Tensor_30061], outputs = [Tensor_30077], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 197 stride: 465708 stride: 38809 stride: 197 stride: 1]
Tensor_30077: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12687: 59610624 Bnr)
Operation_15131: operator = BatchedMatmulBackward, inputs = [Tensor_30077, Tensor_27864, Tensor_27866], outputs = [Tensor_30079, Tensor_30080], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_30079: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13307: 19365888 Bnn)
Tensor_30080: shape = [1536, 64, 197], stride = [12608, 1, 64], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13603: 19365888 Bnn)
Operation_15134: operator = ReshapeBackward, inputs = [Tensor_30079], outputs = [Tensor_30082], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_30082: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13307: 19365888 Bnn)
Operation_15138: operator = ContiguousBackward, inputs = [Tensor_30082], outputs = [Tensor_30084], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_30084: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13460: 19365888 Bnn)
Operation_15140: operator = ReshapeBackward, inputs = [Tensor_30080], outputs = [Tensor_30086], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_30086: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13603: 19365888 Bnn)
Operation_15144: operator = ContiguousBackward, inputs = [Tensor_30086], outputs = [Tensor_30088], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_30088: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12353: 19365888 Bnn)
Operation_15146: operator = ReshapeBackward, inputs = [Tensor_30084], outputs = [Tensor_30090], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 12 shape: 64 stride: 151296 stride: 768 stride: 64 stride: 1]
Tensor_30090: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13460: 19365888 Bnn)
Operation_15150: operator = ElemwiseUnaryBackward, inputs = [Tensor_30090, nullptr, nullptr], outputs = [Tensor_30092], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 0.125]
Tensor_30092: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13460: 19365888 Bnr)
Operation_15154: operator = LinearBackward, inputs = [Tensor_30092, Tensor_27858, Tensor_27859], outputs = [Tensor_30094, Tensor_30095, Tensor_30099], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->23546
  - Operand: []
Tensor_30094: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13307: 19365888 Bnr)
Tensor_30095: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (12341: 1179648 Bnn)
Operation_23546: operator = AllReduceDuplicated, inputs = [Tensor_30095, nullptr], outputs = [Tensor_30098, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 15154->this
  - Operand: [subop: SUM tag: 23545]
Operation_30097: operator = Consume, inputs = [Tensor_30095], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_30098: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12341: 1179648 Bnr)
Tensor_30099: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (28872: 1536 Bnn)
Operation_23550: operator = AllReduceDuplicated, inputs = [Tensor_30099, nullptr], outputs = [Tensor_30102, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: SUM tag: 23549]
Operation_30101: operator = Consume, inputs = [Tensor_30099], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_30102: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28872: 1536 Bnr)
Operation_15152: operator = ReshapeBackward, inputs = [Tensor_30088], outputs = [Tensor_30104], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 12 shape: 64 stride: 151296 stride: 768 stride: 64 stride: 1]
Tensor_30104: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12353: 19365888 Bnn)
Operation_15158: operator = LinearBackward, inputs = [Tensor_30104, Tensor_27860, Tensor_27861], outputs = [Tensor_30106, Tensor_30107, Tensor_30111], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->23558
  - Operand: []
Tensor_30106: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13603: 19365888 Bnr)
Tensor_30107: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (12883: 1179648 Bnn)
Operation_23558: operator = AllReduceDuplicated, inputs = [Tensor_30107, nullptr], outputs = [Tensor_30110, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 15158->this
  - Operand: [subop: SUM tag: 23557]
Operation_30109: operator = Consume, inputs = [Tensor_30107], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_30110: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12883: 1179648 Bnr)
Tensor_30111: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (28783: 1536 Bnn)
Operation_23562: operator = AllReduceDuplicated, inputs = [Tensor_30111, nullptr], outputs = [Tensor_30114, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: SUM tag: 23561]
Operation_30113: operator = Consume, inputs = [Tensor_30111], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_30114: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28783: 1536 Bnr)
Operation_15162: operator = TypecastBackward, inputs = [Tensor_30094], outputs = [Tensor_30116], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_30116: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (28345: 38731776 Bnr)
Operation_15164: operator = ElemwiseBinaryForward, inputs = [Tensor_30075, Tensor_30116], outputs = [Tensor_30118], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_30118: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12709: 38731776 Bnn)
Operation_15166: operator = TypecastBackward, inputs = [Tensor_30106], outputs = [Tensor_30120], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_30120: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13538: 38731776 Bnr)
Operation_15168: operator = ElemwiseBinaryForward, inputs = [Tensor_30118, Tensor_30120], outputs = [Tensor_30122], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_30122: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12709: 38731776 Bnr)
Operation_15170: operator = LayernormBackward, inputs = [Tensor_30122, Tensor_27854, Tensor_27855, Tensor_27659, Tensor_27660, Tensor_27856, Tensor_27857], outputs = [Tensor_30124, Tensor_30125, Tensor_30129], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->23576
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_30124: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12633: 38731776 Bnr)
Tensor_30125: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (30128: 3072 Ban)
Operation_23576: operator = AllReduceDuplicated, inputs = [Tensor_30125, nullptr], outputs = [Tensor_30128, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 15170->this
  - Operand: [subop: SUM tag: 23575]
Operation_30127: operator = Consume, inputs = [Tensor_30125], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_30128: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30128: 3072 Bnn)
Tensor_30129: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (30132: 3072 Ban)
Operation_23580: operator = AllReduceDuplicated, inputs = [Tensor_30129, nullptr], outputs = [Tensor_30132, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: SUM tag: 23579]
Operation_30131: operator = Consume, inputs = [Tensor_30129], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_30132: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30132: 3072 Bnn)
Operation_15172: operator = ElemwiseBinaryForward, inputs = [Tensor_29997, Tensor_30124], outputs = [Tensor_30134], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_30134: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13663: 38731776 Bnr)
Operation_15179: operator = ElemwiseUnaryBackward, inputs = [Tensor_30134, nullptr, nullptr], outputs = [Tensor_30136], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 1]
Tensor_30136: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12317: 38731776 Bnn)
Operation_15181: operator = TypecastBackward, inputs = [Tensor_30134], outputs = [Tensor_30138], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_30138: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13265: 19365888 Bnn)
Operation_15183: operator = ElemwiseBinaryBackward, inputs = [Tensor_30138, Tensor_27852, Tensor_27853, nullptr], outputs = [Tensor_30140, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_MUL]
Tensor_30140: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13265: 19365888 Bnn)
Operation_15185: operator = ElemwiseUnaryBackward, inputs = [Tensor_30140, nullptr, nullptr], outputs = [Tensor_30142], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_DIV scal1_float: 0.972727299]
Tensor_30142: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13265: 19365888 Bnn)
Operation_15187: operator = ReshapeBackward, inputs = [Tensor_30142], outputs = [Tensor_30144], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 151296 stride: 768 stride: 1]
Tensor_30144: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13265: 19365888 Bnr)
Operation_15189: operator = LinearBackward, inputs = [Tensor_30144, Tensor_27850, Tensor_27851], outputs = [Tensor_30146, Tensor_30147, Tensor_30151], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->23598
  - Operand: []
Tensor_30146: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12715: 77463552 Bnr)
Tensor_30147: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (12453: 4718592 Bnn)
Operation_23598: operator = AllReduceDuplicated, inputs = [Tensor_30147, nullptr], outputs = [Tensor_30150, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 15189->this
  - Operand: [subop: SUM tag: 23597]
Operation_30149: operator = Consume, inputs = [Tensor_30147], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_30150: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12453: 4718592 Bnr)
Tensor_30151: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (30154: 1536 Ban)
Operation_23602: operator = AllReduceDuplicated, inputs = [Tensor_30151, nullptr], outputs = [Tensor_30154, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: SUM tag: 23601]
Operation_30153: operator = Consume, inputs = [Tensor_30151], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_30154: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30154: 1536 Bnr)
Operation_15193: operator = TypecastBackward, inputs = [Tensor_30146], outputs = [Tensor_30156], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_30156: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13554: 154927104 Bnr)
Operation_15195: operator = LayernormBackward, inputs = [Tensor_30156, Tensor_27846, Tensor_27847, Tensor_27653, Tensor_27654, Tensor_27848, Tensor_27849], outputs = [Tensor_30158, Tensor_30159, Tensor_30163], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->23610
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_30158: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12606: 154927104 Bnr)
Tensor_30159: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (30162: 12288 Ban)
Operation_23610: operator = AllReduceDuplicated, inputs = [Tensor_30159, nullptr], outputs = [Tensor_30162, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 15195->this
  - Operand: [subop: SUM tag: 23609]
Operation_30161: operator = Consume, inputs = [Tensor_30159], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_30162: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30162: 12288 Bnn)
Tensor_30163: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (30166: 12288 Ban)
Operation_23614: operator = AllReduceDuplicated, inputs = [Tensor_30163, nullptr], outputs = [Tensor_30166, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: SUM tag: 23613]
Operation_30165: operator = Consume, inputs = [Tensor_30163], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_30166: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30166: 12288 Bnn)
Operation_15199: operator = TypecastBackward, inputs = [Tensor_30158], outputs = [Tensor_30168], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_30168: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13669: 77463552 Bnn)
Operation_15201: operator = TypecastBackward, inputs = [Tensor_30168], outputs = [Tensor_30170], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_30170: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13713: 154927104 Bnn)
Operation_15203: operator = ElemwiseUnaryBackward, inputs = [Tensor_30170, Tensor_27844, Tensor_27845], outputs = [Tensor_30172], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_GELU]
Tensor_30172: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13713: 154927104 Bnn)
Operation_15205: operator = TypecastBackward, inputs = [Tensor_30172], outputs = [Tensor_30174], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_30174: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12914: 77463552 Bnr)
Operation_15207: operator = LinearBackward, inputs = [Tensor_30174, Tensor_27842, Tensor_27843], outputs = [Tensor_30176, Tensor_30177, Tensor_30181], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->23628
  - Operand: []
Tensor_30176: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12347: 19365888 Bnn)
Tensor_30177: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (12593: 4718592 Bnn)
Operation_23628: operator = AllReduceDuplicated, inputs = [Tensor_30177, nullptr], outputs = [Tensor_30180, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 15207->this
  - Operand: [subop: SUM tag: 23627]
Operation_30179: operator = Consume, inputs = [Tensor_30177], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_30180: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12593: 4718592 Bnr)
Tensor_30181: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (30184: 6144 Ban)
Operation_23632: operator = AllReduceDuplicated, inputs = [Tensor_30181, nullptr], outputs = [Tensor_30184, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: SUM tag: 23631]
Operation_30183: operator = Consume, inputs = [Tensor_30181], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_30184: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30184: 6144 Bnr)
Operation_15211: operator = TypecastBackward, inputs = [Tensor_30176], outputs = [Tensor_30186], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_30186: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12743: 38731776 Bnn)
Operation_15213: operator = ReshapeBackward, inputs = [Tensor_30186], outputs = [Tensor_30188], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 25216 shape: 768 stride: 768 stride: 1]
Tensor_30188: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12743: 38731776 Bnr)
Operation_15215: operator = LayernormBackward, inputs = [Tensor_30188, Tensor_27838, Tensor_27839, Tensor_27655, Tensor_27656, Tensor_27840, Tensor_27841], outputs = [Tensor_30190, Tensor_30191, Tensor_30195], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->23642
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_30190: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12550: 38731776 Bnn)
Tensor_30191: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (30194: 3072 Ban)
Operation_23642: operator = AllReduceDuplicated, inputs = [Tensor_30191, nullptr], outputs = [Tensor_30194, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 15215->this
  - Operand: [subop: SUM tag: 23641]
Operation_30193: operator = Consume, inputs = [Tensor_30191], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_30194: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30194: 3072 Bnn)
Tensor_30195: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (30198: 3072 Ban)
Operation_23646: operator = AllReduceDuplicated, inputs = [Tensor_30195, nullptr], outputs = [Tensor_30198, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: SUM tag: 23645]
Operation_30197: operator = Consume, inputs = [Tensor_30195], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_30198: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30198: 3072 Bnn)
Operation_15217: operator = ElemwiseBinaryForward, inputs = [Tensor_30136, Tensor_30190], outputs = [Tensor_30200], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_30200: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12317: 38731776 Bnr)
Operation_15224: operator = ElemwiseUnaryBackward, inputs = [Tensor_30200, nullptr, nullptr], outputs = [Tensor_30202], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 1]
Tensor_30202: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12554: 38731776 Bnn)
Operation_15226: operator = TypecastBackward, inputs = [Tensor_30200], outputs = [Tensor_30204], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_30204: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12353: 19365888 Bnn)
Operation_15228: operator = ElemwiseBinaryBackward, inputs = [Tensor_30204, Tensor_27836, Tensor_27837, nullptr], outputs = [Tensor_30206, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_MUL]
Tensor_30206: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12353: 19365888 Bnn)
Operation_15230: operator = ElemwiseUnaryBackward, inputs = [Tensor_30206, nullptr, nullptr], outputs = [Tensor_30208], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_DIV scal1_float: 0.972727299]
Tensor_30208: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12353: 19365888 Bnr)
Operation_15232: operator = LinearBackward, inputs = [Tensor_30208, Tensor_27834, Tensor_27835], outputs = [Tensor_30210, Tensor_30211, Tensor_30215], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->23662
  - Operand: []
Tensor_30210: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12734: 19365888 Bnr)
Tensor_30211: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (12325: 1179648 Bnn)
Operation_23662: operator = AllReduceDuplicated, inputs = [Tensor_30211, nullptr], outputs = [Tensor_30214, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 15232->this
  - Operand: [subop: SUM tag: 23661]
Operation_30213: operator = Consume, inputs = [Tensor_30211], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_30214: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12325: 1179648 Bnr)
Tensor_30215: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (30218: 1536 Ban)
Operation_23666: operator = AllReduceDuplicated, inputs = [Tensor_30215, nullptr], outputs = [Tensor_30218, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: SUM tag: 23665]
Operation_30217: operator = Consume, inputs = [Tensor_30215], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_30218: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30218: 1536 Bnr)
Operation_15236: operator = TypecastBackward, inputs = [Tensor_30210], outputs = [Tensor_30220], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_30220: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12550: 38731776 Bnr)
Tensor_30221: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13587: 38731776 Bnn)
Operation_15238: operator = LayernormBackward, inputs = [Tensor_30220, Tensor_30221, Tensor_27831, Tensor_27649, Tensor_27650, Tensor_27832, Tensor_27833], outputs = [Tensor_30223, Tensor_30224, Tensor_30228], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->23676
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_30223: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13110: 38731776 Bnr)
Tensor_30224: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (30227: 3072 Ban)
Operation_23676: operator = AllReduceDuplicated, inputs = [Tensor_30224, nullptr], outputs = [Tensor_30227, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 15238->this
  - Operand: [subop: SUM tag: 23675]
Operation_30226: operator = Consume, inputs = [Tensor_30224], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_30227: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30227: 3072 Bnn)
Tensor_30228: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (30231: 3072 Ban)
Operation_23680: operator = AllReduceDuplicated, inputs = [Tensor_30228, nullptr], outputs = [Tensor_30231, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: SUM tag: 23679]
Operation_30230: operator = Consume, inputs = [Tensor_30228], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_30231: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30231: 3072 Bnn)
Operation_23683: operator = AllGatherDuplicated, inputs = [Tensor_30223], outputs = [Tensor_30234], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->15242
  - Operand: [tag: 23682]
Operation_30233: operator = Consume, inputs = [Tensor_30223], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_30234: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13669: 77463552 Bnn)
Operation_15723: operator = TypecastBackward, inputs = [Tensor_28880], outputs = [Tensor_30809], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Operation_15725: operator = TypecastBackward, inputs = [Tensor_28884], outputs = [Tensor_30810], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Operation_15727: operator = TypecastBackward, inputs = [Tensor_28920], outputs = [Tensor_30811], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Operation_15729: operator = TypecastBackward, inputs = [Tensor_28924], outputs = [Tensor_30812], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Operation_15731: operator = TypecastBackward, inputs = [Tensor_28950], outputs = [Tensor_30813], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Operation_15242: operator = TypecastBackward, inputs = [Tensor_30234], outputs = [Tensor_30241], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 23683->this
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_30241: shape = [128, 197, 768], stride = [768, 98304, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13587: 38731776 Bnn)
Operation_15246: operator = ReshapeBackward, inputs = [Tensor_30241], outputs = [Tensor_30243], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 197 shape: 128 shape: 768 stride: 98304 stride: 768 stride: 1]
Tensor_30243: shape = [197, 1536, 64], stride = [98304, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13587: 38731776 Bnr)
Operation_15248: operator = ContiguousBackward, inputs = [Tensor_30243], outputs = [Tensor_30245], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_30245: shape = [197, 1536, 64], stride = [64, 12608, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13697: 38731776 Bnn)
Operation_15250: operator = ReshapeBackward, inputs = [Tensor_30245], outputs = [Tensor_30247], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 197 shape: 1536 shape: 64 stride: 64 stride: 12608 stride: 1]
Tensor_30247: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13697: 38731776 Bnr)
Operation_23693: operator = DistributeWithMemAllocForward, inputs = [Tensor_30247], outputs = [Tensor_30249], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_30249: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12657: 19365888 Bnn)
Operation_15252: operator = BatchedMatmulBackward, inputs = [Tensor_30249, Tensor_27829, Tensor_27825], outputs = [Tensor_30251, Tensor_30252], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_30251: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13164: 59610624 Bnn)
Tensor_30252: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13452: 19365888 Bnn)
Operation_15255: operator = TypecastBackward, inputs = [Tensor_30251], outputs = [Tensor_30254], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_30254: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11920: 119221248 Bnr)
Operation_15259: operator = SoftmaxBackward, inputs = [Tensor_30254, Tensor_27828], outputs = [Tensor_30256], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dim: 2]
Tensor_30256: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13490: 119221248 Bnr)
Operation_15257: operator = ReshapeBackward, inputs = [Tensor_30252], outputs = [Tensor_30258], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_30258: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13452: 19365888 Bnn)
Operation_15261: operator = ContiguousBackward, inputs = [Tensor_30258], outputs = [Tensor_30260], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_30260: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13259: 19365888 Bnn)
Operation_15263: operator = TypecastBackward, inputs = [Tensor_30256], outputs = [Tensor_30262], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_30262: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12210: 59610624 Bnn)
Operation_15267: operator = ReshapeBackward, inputs = [Tensor_30262], outputs = [Tensor_30264], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 197 stride: 38809 stride: 197 stride: 1]
Tensor_30264: shape = [128, 12, 197, 197], stride = [465708, 38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12210: 59610624 Bnn)
Operation_15271: operator = MaskedFillBackward, inputs = [Tensor_30264, Tensor_27827], outputs = [Tensor_30266], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [value_float: -inf]
Tensor_30266: shape = [128, 12, 197, 197], stride = [465708, 38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12210: 59610624 Bnn)
Operation_15269: operator = ReshapeBackward, inputs = [Tensor_30260], outputs = [Tensor_30268], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 12 shape: 64 stride: 151296 stride: 768 stride: 64 stride: 1]
Tensor_30268: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13259: 19365888 Bnr)
Operation_15273: operator = LinearBackward, inputs = [Tensor_30268, Tensor_27822, Tensor_27823], outputs = [Tensor_30270, Tensor_30271, Tensor_30275], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->23718
  - Operand: []
Tensor_30270: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13452: 19365888 Bnn)
Tensor_30271: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (12724: 1179648 Bnn)
Operation_23718: operator = AllReduceDuplicated, inputs = [Tensor_30271, nullptr], outputs = [Tensor_30274, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 15273->this
  - Operand: [subop: SUM tag: 23717]
Operation_30273: operator = Consume, inputs = [Tensor_30271], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_30274: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12724: 1179648 Bnr)
Tensor_30275: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (28924: 1536 Bnn)
Operation_23722: operator = AllReduceDuplicated, inputs = [Tensor_30275, nullptr], outputs = [Tensor_30278, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: SUM tag: 23721]
Operation_30277: operator = Consume, inputs = [Tensor_30275], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_30278: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28924: 1536 Bnr)
Operation_15279: operator = TypecastBackward, inputs = [Tensor_30270], outputs = [Tensor_30280], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_30280: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13504: 38731776 Bnn)
Operation_15277: operator = ReshapeBackward, inputs = [Tensor_30266], outputs = [Tensor_30282], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 197 stride: 465708 stride: 38809 stride: 197 stride: 1]
Tensor_30282: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12210: 59610624 Bnr)
Operation_15281: operator = BatchedMatmulBackward, inputs = [Tensor_30282, Tensor_27824, Tensor_27826], outputs = [Tensor_30284, Tensor_30285], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_30284: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13452: 19365888 Bnn)
Tensor_30285: shape = [1536, 64, 197], stride = [12608, 1, 64], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12257: 19365888 Bnn)
Operation_15284: operator = ReshapeBackward, inputs = [Tensor_30284], outputs = [Tensor_30287], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_30287: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13452: 19365888 Bnn)
Operation_15288: operator = ContiguousBackward, inputs = [Tensor_30287], outputs = [Tensor_30289], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_30289: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12832: 19365888 Bnn)
Operation_15290: operator = ReshapeBackward, inputs = [Tensor_30285], outputs = [Tensor_30291], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_30291: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12257: 19365888 Bnn)
Operation_15294: operator = ContiguousBackward, inputs = [Tensor_30291], outputs = [Tensor_30293], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_30293: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13466: 19365888 Bnn)
Operation_15296: operator = ReshapeBackward, inputs = [Tensor_30289], outputs = [Tensor_30295], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 12 shape: 64 stride: 151296 stride: 768 stride: 64 stride: 1]
Tensor_30295: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12832: 19365888 Bnn)
Operation_15300: operator = ElemwiseUnaryBackward, inputs = [Tensor_30295, nullptr, nullptr], outputs = [Tensor_30297], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 0.125]
Tensor_30297: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12832: 19365888 Bnn)
Operation_15304: operator = LinearBackward, inputs = [Tensor_30297, Tensor_27818, Tensor_27819], outputs = [Tensor_30299, Tensor_30300, Tensor_30304], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->23747
  - Operand: []
Tensor_30299: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13452: 19365888 Bnr)
Tensor_30300: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (12182: 1179648 Bnn)
Operation_23747: operator = AllReduceDuplicated, inputs = [Tensor_30300, nullptr], outputs = [Tensor_30303, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 15304->this
  - Operand: [subop: SUM tag: 23746]
Operation_30302: operator = Consume, inputs = [Tensor_30300], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_30303: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12182: 1179648 Bnr)
Tensor_30304: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (28884: 1536 Bnn)
Operation_23751: operator = AllReduceDuplicated, inputs = [Tensor_30304, nullptr], outputs = [Tensor_30307, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: SUM tag: 23750]
Operation_30306: operator = Consume, inputs = [Tensor_30304], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_30307: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28884: 1536 Bnr)
Operation_15302: operator = ReshapeBackward, inputs = [Tensor_30293], outputs = [Tensor_30309], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 12 shape: 64 stride: 151296 stride: 768 stride: 64 stride: 1]
Tensor_30309: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13466: 19365888 Bnr)
Operation_15308: operator = LinearBackward, inputs = [Tensor_30309, Tensor_27820, Tensor_27821], outputs = [Tensor_30311, Tensor_30312, Tensor_30316], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->23759
  - Operand: []
Tensor_30311: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12257: 19365888 Bnr)
Tensor_30312: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (12818: 1179648 Bnn)
Operation_23759: operator = AllReduceDuplicated, inputs = [Tensor_30312, nullptr], outputs = [Tensor_30315, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 15308->this
  - Operand: [subop: SUM tag: 23758]
Operation_30314: operator = Consume, inputs = [Tensor_30312], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_30315: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12818: 1179648 Bnr)
Tensor_30316: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (30319: 1536 Ban)
Operation_23763: operator = AllReduceDuplicated, inputs = [Tensor_30316, nullptr], outputs = [Tensor_30319, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: SUM tag: 23762]
Operation_30318: operator = Consume, inputs = [Tensor_30316], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_30319: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30319: 1536 Bnr)
Operation_15312: operator = TypecastBackward, inputs = [Tensor_30299], outputs = [Tensor_30321], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_30321: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13027: 38731776 Bnn)
Operation_15314: operator = ElemwiseBinaryForward, inputs = [Tensor_30280, Tensor_30321], outputs = [Tensor_30323], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_30323: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13504: 38731776 Bnn)
Operation_15316: operator = TypecastBackward, inputs = [Tensor_30311], outputs = [Tensor_30325], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_30325: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13027: 38731776 Bnn)
Operation_15318: operator = ElemwiseBinaryForward, inputs = [Tensor_30323, Tensor_30325], outputs = [Tensor_30327], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_30327: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13504: 38731776 Bnn)
Operation_15320: operator = LayernormBackward, inputs = [Tensor_30327, Tensor_27814, Tensor_27815, Tensor_27651, Tensor_27652, Tensor_27816, Tensor_27817], outputs = [Tensor_30329, Tensor_30330, Tensor_30334], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->23777
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_30329: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13027: 38731776 Bnn)
Tensor_30330: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (30333: 3072 Ban)
Operation_23777: operator = AllReduceDuplicated, inputs = [Tensor_30330, nullptr], outputs = [Tensor_30333, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 15320->this
  - Operand: [subop: SUM tag: 23776]
Operation_30332: operator = Consume, inputs = [Tensor_30330], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_30333: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30333: 3072 Bnn)
Tensor_30334: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (30337: 3072 Ban)
Operation_23781: operator = AllReduceDuplicated, inputs = [Tensor_30334, nullptr], outputs = [Tensor_30337, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: SUM tag: 23780]
Operation_30336: operator = Consume, inputs = [Tensor_30334], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_30337: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30337: 3072 Bnn)
Operation_15322: operator = ElemwiseBinaryForward, inputs = [Tensor_30202, Tensor_30329], outputs = [Tensor_30339], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_30339: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12554: 38731776 Bnn)
Operation_15329: operator = ElemwiseUnaryBackward, inputs = [Tensor_30339, nullptr, nullptr], outputs = [Tensor_30341], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 1]
Tensor_30341: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13504: 38731776 Bnn)
Operation_15331: operator = TypecastBackward, inputs = [Tensor_30339], outputs = [Tensor_30343], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_30343: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12832: 19365888 Bnn)
Operation_15333: operator = ElemwiseBinaryBackward, inputs = [Tensor_30343, Tensor_27812, Tensor_27813, nullptr], outputs = [Tensor_30345, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_MUL]
Tensor_30345: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12832: 19365888 Bnn)
Operation_15335: operator = ElemwiseUnaryBackward, inputs = [Tensor_30345, nullptr, nullptr], outputs = [Tensor_30347], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_DIV scal1_float: 0.986363649]
Tensor_30347: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12832: 19365888 Bnn)
Operation_15337: operator = ReshapeBackward, inputs = [Tensor_30347], outputs = [Tensor_30349], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 151296 stride: 768 stride: 1]
Tensor_30349: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12832: 19365888 Bnr)
Operation_15339: operator = LinearBackward, inputs = [Tensor_30349, Tensor_27810, Tensor_27811], outputs = [Tensor_30351, Tensor_30352, Tensor_30356], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->23799
  - Operand: []
Tensor_30351: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13669: 77463552 Bnr)
Tensor_30352: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (28305: 4718592 Bnn)
Operation_23799: operator = AllReduceDuplicated, inputs = [Tensor_30352, nullptr], outputs = [Tensor_30355, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 15339->this
  - Operand: [subop: SUM tag: 23798]
Operation_30354: operator = Consume, inputs = [Tensor_30352], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_30355: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28305: 4718592 Bnr)
Tensor_30356: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (30359: 1536 Ban)
Operation_23803: operator = AllReduceDuplicated, inputs = [Tensor_30356, nullptr], outputs = [Tensor_30359, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: SUM tag: 23802]
Operation_30358: operator = Consume, inputs = [Tensor_30356], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_30359: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30359: 1536 Bnr)
Operation_15343: operator = TypecastBackward, inputs = [Tensor_30351], outputs = [Tensor_30361], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_30361: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12600: 154927104 Bnr)
Operation_15345: operator = LayernormBackward, inputs = [Tensor_30361, Tensor_27806, Tensor_27807, Tensor_27645, Tensor_27646, Tensor_27808, Tensor_27809], outputs = [Tensor_30363, Tensor_30364, Tensor_30368], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->23811
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_30363: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12598: 154927104 Bnr)
Tensor_30364: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (30367: 12288 Ban)
Operation_23811: operator = AllReduceDuplicated, inputs = [Tensor_30364, nullptr], outputs = [Tensor_30367, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 15345->this
  - Operand: [subop: SUM tag: 23810]
Operation_30366: operator = Consume, inputs = [Tensor_30364], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_30367: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30367: 12288 Bnn)
Tensor_30368: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (30371: 12288 Ban)
Operation_23815: operator = AllReduceDuplicated, inputs = [Tensor_30368, nullptr], outputs = [Tensor_30371, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: SUM tag: 23814]
Operation_30370: operator = Consume, inputs = [Tensor_30368], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_30371: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30371: 12288 Bnn)
Operation_15349: operator = TypecastBackward, inputs = [Tensor_30363], outputs = [Tensor_30373], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_30373: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12437: 77463552 Bnr)
Operation_15351: operator = TypecastBackward, inputs = [Tensor_30373], outputs = [Tensor_30375], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_30375: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12757: 154927104 Bnn)
Operation_15353: operator = ElemwiseUnaryBackward, inputs = [Tensor_30375, Tensor_27804, Tensor_27805], outputs = [Tensor_30377], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_GELU]
Tensor_30377: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12757: 154927104 Bnn)
Operation_15355: operator = TypecastBackward, inputs = [Tensor_30377], outputs = [Tensor_30379], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_30379: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13510: 77463552 Bnn)
Operation_15357: operator = LinearBackward, inputs = [Tensor_30379, Tensor_27802, Tensor_27803], outputs = [Tensor_30381, Tensor_30382, Tensor_30386], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->23829
  - Operand: []
Tensor_30381: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12470: 19365888 Bnr)
Tensor_30382: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (12612: 4718592 Bnn)
Operation_23829: operator = AllReduceDuplicated, inputs = [Tensor_30382, nullptr], outputs = [Tensor_30385, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 15357->this
  - Operand: [subop: SUM tag: 23828]
Operation_30384: operator = Consume, inputs = [Tensor_30382], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_30385: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12612: 4718592 Bnr)
Tensor_30386: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (30389: 6144 Ban)
Operation_23833: operator = AllReduceDuplicated, inputs = [Tensor_30386, nullptr], outputs = [Tensor_30389, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: SUM tag: 23832]
Operation_30388: operator = Consume, inputs = [Tensor_30386], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_30389: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30389: 6144 Bnr)
Operation_15361: operator = TypecastBackward, inputs = [Tensor_30381], outputs = [Tensor_30391], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_30391: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13589: 38731776 Bnn)
Operation_15363: operator = ReshapeBackward, inputs = [Tensor_30391], outputs = [Tensor_30393], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 25216 shape: 768 stride: 768 stride: 1]
Tensor_30393: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13589: 38731776 Bnr)
Operation_15365: operator = LayernormBackward, inputs = [Tensor_30393, Tensor_27798, Tensor_27799, Tensor_27647, Tensor_27648, Tensor_27800, Tensor_27801], outputs = [Tensor_30395, Tensor_30396, Tensor_30400], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->23843
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_30395: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13667: 38731776 Bnr)
Tensor_30396: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (30399: 3072 Ban)
Operation_23843: operator = AllReduceDuplicated, inputs = [Tensor_30396, nullptr], outputs = [Tensor_30399, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 15365->this
  - Operand: [subop: SUM tag: 23842]
Operation_30398: operator = Consume, inputs = [Tensor_30396], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_30399: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30399: 3072 Bnn)
Tensor_30400: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (30403: 3072 Ban)
Operation_23847: operator = AllReduceDuplicated, inputs = [Tensor_30400, nullptr], outputs = [Tensor_30403, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: SUM tag: 23846]
Operation_30402: operator = Consume, inputs = [Tensor_30400], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_30403: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30403: 3072 Bnn)
Operation_15367: operator = ElemwiseBinaryForward, inputs = [Tensor_30341, Tensor_30395], outputs = [Tensor_30405], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_30405: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13504: 38731776 Bnr)
Operation_15374: operator = ElemwiseUnaryBackward, inputs = [Tensor_30405, nullptr, nullptr], outputs = [Tensor_30407], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 1]
Tensor_30407: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12476: 38731776 Bnn)
Operation_15376: operator = TypecastBackward, inputs = [Tensor_30405], outputs = [Tensor_30409], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_30409: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12498: 19365888 Bnn)
Operation_15378: operator = ElemwiseBinaryBackward, inputs = [Tensor_30409, Tensor_27796, Tensor_27797, nullptr], outputs = [Tensor_30411, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_MUL]
Tensor_30411: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12498: 19365888 Bnn)
Operation_15380: operator = ElemwiseUnaryBackward, inputs = [Tensor_30411, nullptr, nullptr], outputs = [Tensor_30413], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_DIV scal1_float: 0.986363649]
Tensor_30413: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12498: 19365888 Bnr)
Operation_15382: operator = LinearBackward, inputs = [Tensor_30413, Tensor_27794, Tensor_27795], outputs = [Tensor_30415, Tensor_30416, Tensor_30420], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->23863
  - Operand: []
Tensor_30415: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12623: 19365888 Bnr)
Tensor_30416: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (12166: 1179648 Bnn)
Operation_23863: operator = AllReduceDuplicated, inputs = [Tensor_30416, nullptr], outputs = [Tensor_30419, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 15382->this
  - Operand: [subop: SUM tag: 23862]
Operation_30418: operator = Consume, inputs = [Tensor_30416], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_30419: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12166: 1179648 Bnr)
Tensor_30420: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (30423: 1536 Ban)
Operation_23867: operator = AllReduceDuplicated, inputs = [Tensor_30420, nullptr], outputs = [Tensor_30423, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: SUM tag: 23866]
Operation_30422: operator = Consume, inputs = [Tensor_30420], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_30423: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30423: 1536 Bnr)
Operation_15386: operator = TypecastBackward, inputs = [Tensor_30415], outputs = [Tensor_30425], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_30425: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12584: 38731776 Bnr)
Tensor_30426: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13508: 38731776 Bnr)
Operation_15388: operator = LayernormBackward, inputs = [Tensor_30425, Tensor_30426, Tensor_27791, Tensor_27641, Tensor_27642, Tensor_27792, Tensor_27793], outputs = [Tensor_30428, Tensor_30429, Tensor_30433], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->23877
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_30428: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12474: 38731776 Bnr)
Tensor_30429: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (30432: 3072 Ban)
Operation_23877: operator = AllReduceDuplicated, inputs = [Tensor_30429, nullptr], outputs = [Tensor_30432, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 15388->this
  - Operand: [subop: SUM tag: 23876]
Operation_30431: operator = Consume, inputs = [Tensor_30429], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_30432: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30432: 3072 Bnn)
Tensor_30433: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (30436: 3072 Ban)
Operation_23881: operator = AllReduceDuplicated, inputs = [Tensor_30433, nullptr], outputs = [Tensor_30436, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: SUM tag: 23880]
Operation_30435: operator = Consume, inputs = [Tensor_30433], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_30436: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30436: 3072 Bnn)
Operation_23884: operator = AllGatherDuplicated, inputs = [Tensor_30428], outputs = [Tensor_30439], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->15392
  - Operand: [tag: 23883]
Operation_30438: operator = Consume, inputs = [Tensor_30428], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_30439: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13510: 77463552 Bnr)
Operation_15733: operator = TypecastBackward, inputs = [Tensor_28954], outputs = [Tensor_30814], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Operation_15735: operator = TypecastBackward, inputs = [Tensor_28984], outputs = [Tensor_30815], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Operation_15737: operator = TypecastBackward, inputs = [Tensor_28988], outputs = [Tensor_30816], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Operation_15739: operator = TypecastBackward, inputs = [Tensor_29044], outputs = [Tensor_30817], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Operation_15741: operator = TypecastBackward, inputs = [Tensor_29048], outputs = [Tensor_30818], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Operation_15392: operator = TypecastBackward, inputs = [Tensor_30439], outputs = [Tensor_30446], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 23884->this
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_30446: shape = [128, 197, 768], stride = [768, 98304, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13190: 38731776 Bnn)
Operation_15396: operator = ReshapeBackward, inputs = [Tensor_30446], outputs = [Tensor_30448], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 197 shape: 128 shape: 768 stride: 98304 stride: 768 stride: 1]
Tensor_30448: shape = [197, 1536, 64], stride = [98304, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13190: 38731776 Bnn)
Operation_15398: operator = ContiguousBackward, inputs = [Tensor_30448], outputs = [Tensor_30450], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_30450: shape = [197, 1536, 64], stride = [64, 12608, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12156: 38731776 Bnn)
Operation_15400: operator = ReshapeBackward, inputs = [Tensor_30450], outputs = [Tensor_30452], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 197 shape: 1536 shape: 64 stride: 64 stride: 12608 stride: 1]
Tensor_30452: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12156: 38731776 Bnn)
Operation_23894: operator = DistributeWithMemAllocForward, inputs = [Tensor_30452], outputs = [Tensor_30454], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_30454: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12098: 19365888 Bnr)
Operation_15402: operator = BatchedMatmulBackward, inputs = [Tensor_30454, Tensor_27789, Tensor_27785], outputs = [Tensor_30456, Tensor_30457], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_30456: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13482: 59610624 Bnn)
Tensor_30457: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12180: 19365888 Bnn)
Operation_15405: operator = TypecastBackward, inputs = [Tensor_30456], outputs = [Tensor_30459], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_30459: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12377: 119221248 Bnr)
Operation_15409: operator = SoftmaxBackward, inputs = [Tensor_30459, Tensor_27788], outputs = [Tensor_30461], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dim: 2]
Tensor_30461: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12695: 119221248 Bnr)
Operation_15407: operator = ReshapeBackward, inputs = [Tensor_30457], outputs = [Tensor_30463], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_30463: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12180: 19365888 Bnn)
Operation_15411: operator = ContiguousBackward, inputs = [Tensor_30463], outputs = [Tensor_30465], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_30465: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12347: 19365888 Bnn)
Operation_15413: operator = TypecastBackward, inputs = [Tensor_30461], outputs = [Tensor_30467], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_30467: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12846: 59610624 Bnn)
Operation_15417: operator = ReshapeBackward, inputs = [Tensor_30467], outputs = [Tensor_30469], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 197 stride: 38809 stride: 197 stride: 1]
Tensor_30469: shape = [128, 12, 197, 197], stride = [465708, 38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12846: 59610624 Bnn)
Operation_15421: operator = MaskedFillBackward, inputs = [Tensor_30469, Tensor_27787], outputs = [Tensor_30471], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [value_float: -inf]
Tensor_30471: shape = [128, 12, 197, 197], stride = [465708, 38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12846: 59610624 Bnn)
Operation_15419: operator = ReshapeBackward, inputs = [Tensor_30465], outputs = [Tensor_30473], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 12 shape: 64 stride: 151296 stride: 768 stride: 64 stride: 1]
Tensor_30473: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12347: 19365888 Bnn)
Operation_15423: operator = LinearBackward, inputs = [Tensor_30473, Tensor_27782, Tensor_27783], outputs = [Tensor_30475, Tensor_30476, Tensor_30480], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->23919
  - Operand: []
Tensor_30475: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12180: 19365888 Bnn)
Tensor_30476: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (12088: 1179648 Bnn)
Operation_23919: operator = AllReduceDuplicated, inputs = [Tensor_30476, nullptr], outputs = [Tensor_30479, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 15423->this
  - Operand: [subop: SUM tag: 23918]
Operation_30478: operator = Consume, inputs = [Tensor_30476], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_30479: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12088: 1179648 Bnr)
Tensor_30480: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (28433: 1536 Bnn)
Operation_23923: operator = AllReduceDuplicated, inputs = [Tensor_30480, nullptr], outputs = [Tensor_30483, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: SUM tag: 23922]
Operation_30482: operator = Consume, inputs = [Tensor_30480], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_30483: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28433: 1536 Bnr)
Operation_15429: operator = TypecastBackward, inputs = [Tensor_30475], outputs = [Tensor_30485], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_30485: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13190: 38731776 Bnn)
Operation_15427: operator = ReshapeBackward, inputs = [Tensor_30471], outputs = [Tensor_30487], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 197 stride: 465708 stride: 38809 stride: 197 stride: 1]
Tensor_30487: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12846: 59610624 Bnr)
Operation_15431: operator = BatchedMatmulBackward, inputs = [Tensor_30487, Tensor_27784, Tensor_27786], outputs = [Tensor_30489, Tensor_30490], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_30489: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12180: 19365888 Bnn)
Tensor_30490: shape = [1536, 64, 197], stride = [12608, 1, 64], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12347: 19365888 Bnn)
Operation_15434: operator = ReshapeBackward, inputs = [Tensor_30489], outputs = [Tensor_30492], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_30492: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12180: 19365888 Bnn)
Operation_15438: operator = ContiguousBackward, inputs = [Tensor_30492], outputs = [Tensor_30494], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_30494: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12021: 19365888 Bnn)
Operation_15440: operator = ReshapeBackward, inputs = [Tensor_30490], outputs = [Tensor_30496], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_30496: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12347: 19365888 Bnr)
Operation_15444: operator = ContiguousBackward, inputs = [Tensor_30496], outputs = [Tensor_30498], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_30498: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12035: 19365888 Bnn)
Operation_15446: operator = ReshapeBackward, inputs = [Tensor_30494], outputs = [Tensor_30500], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 12 shape: 64 stride: 151296 stride: 768 stride: 64 stride: 1]
Tensor_30500: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12021: 19365888 Bnn)
Operation_15450: operator = ElemwiseUnaryBackward, inputs = [Tensor_30500, nullptr, nullptr], outputs = [Tensor_30502], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 0.125]
Tensor_30502: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12021: 19365888 Bnr)
Operation_15454: operator = LinearBackward, inputs = [Tensor_30502, Tensor_27778, Tensor_27779], outputs = [Tensor_30504, Tensor_30505, Tensor_30509], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->23948
  - Operand: []
Tensor_30504: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12180: 19365888 Bnr)
Tensor_30505: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (12802: 1179648 Bnn)
Operation_23948: operator = AllReduceDuplicated, inputs = [Tensor_30505, nullptr], outputs = [Tensor_30508, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 15454->this
  - Operand: [subop: SUM tag: 23947]
Operation_30507: operator = Consume, inputs = [Tensor_30505], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_30508: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12802: 1179648 Bnr)
Tensor_30509: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (28988: 1536 Bnn)
Operation_23952: operator = AllReduceDuplicated, inputs = [Tensor_30509, nullptr], outputs = [Tensor_30512, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: SUM tag: 23951]
Operation_30511: operator = Consume, inputs = [Tensor_30509], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_30512: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28988: 1536 Bnr)
Operation_15452: operator = ReshapeBackward, inputs = [Tensor_30498], outputs = [Tensor_30514], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 12 shape: 64 stride: 151296 stride: 768 stride: 64 stride: 1]
Tensor_30514: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12035: 19365888 Bnr)
Operation_15458: operator = LinearBackward, inputs = [Tensor_30514, Tensor_27780, Tensor_27781], outputs = [Tensor_30516, Tensor_30517, Tensor_30521], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->23960
  - Operand: []
Tensor_30516: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12029: 19365888 Bnr)
Tensor_30517: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (12810: 1179648 Bnn)
Operation_23960: operator = AllReduceDuplicated, inputs = [Tensor_30517, nullptr], outputs = [Tensor_30520, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 15458->this
  - Operand: [subop: SUM tag: 23959]
Operation_30519: operator = Consume, inputs = [Tensor_30517], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_30520: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12810: 1179648 Bnr)
Tensor_30521: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (30524: 1536 Ban)
Operation_23964: operator = AllReduceDuplicated, inputs = [Tensor_30521, nullptr], outputs = [Tensor_30524, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: SUM tag: 23963]
Operation_30523: operator = Consume, inputs = [Tensor_30521], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_30524: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30524: 1536 Bnr)
Operation_15462: operator = TypecastBackward, inputs = [Tensor_30504], outputs = [Tensor_30526], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_30526: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13349: 38731776 Bnr)
Operation_15464: operator = ElemwiseBinaryForward, inputs = [Tensor_30485, Tensor_30526], outputs = [Tensor_30528], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_30528: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13190: 38731776 Bnn)
Operation_15466: operator = TypecastBackward, inputs = [Tensor_30516], outputs = [Tensor_30530], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_30530: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12156: 38731776 Bnn)
Operation_15468: operator = ElemwiseBinaryForward, inputs = [Tensor_30528, Tensor_30530], outputs = [Tensor_30532], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_30532: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13190: 38731776 Bnr)
Operation_15470: operator = LayernormBackward, inputs = [Tensor_30532, Tensor_27774, Tensor_27775, Tensor_27643, Tensor_27644, Tensor_27776, Tensor_27777], outputs = [Tensor_30534, Tensor_30535, Tensor_30539], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->23978
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_30534: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13027: 38731776 Bnn)
Tensor_30535: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (30538: 3072 Ban)
Operation_23978: operator = AllReduceDuplicated, inputs = [Tensor_30535, nullptr], outputs = [Tensor_30538, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 15470->this
  - Operand: [subop: SUM tag: 23977]
Operation_30537: operator = Consume, inputs = [Tensor_30535], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_30538: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30538: 3072 Bnn)
Tensor_30539: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (30542: 3072 Ban)
Operation_23982: operator = AllReduceDuplicated, inputs = [Tensor_30539, nullptr], outputs = [Tensor_30542, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: SUM tag: 23981]
Operation_30541: operator = Consume, inputs = [Tensor_30539], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_30542: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30542: 3072 Bnn)
Operation_15472: operator = ElemwiseBinaryForward, inputs = [Tensor_30407, Tensor_30534], outputs = [Tensor_30544], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_30544: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12476: 38731776 Bnr)
Operation_15479: operator = ElemwiseUnaryBackward, inputs = [Tensor_30544, nullptr, nullptr], outputs = [Tensor_30546], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 1]
Tensor_30546: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12156: 38731776 Bnn)
Operation_15481: operator = TypecastBackward, inputs = [Tensor_30544], outputs = [Tensor_30548], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_30548: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13468: 19365888 Bnn)
Operation_15483: operator = ReshapeBackward, inputs = [Tensor_30548], outputs = [Tensor_30550], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 151296 stride: 768 stride: 1]
Tensor_30550: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13468: 19365888 Bnr)
Operation_15485: operator = LinearBackward, inputs = [Tensor_30550, Tensor_27772, Tensor_27773], outputs = [Tensor_30552, Tensor_30553, Tensor_30557], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->23996
  - Operand: []
Tensor_30552: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12596: 77463552 Bnr)
Tensor_30553: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (13706: 4718592 Bnn)
Operation_23996: operator = AllReduceDuplicated, inputs = [Tensor_30553, nullptr], outputs = [Tensor_30556, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 15485->this
  - Operand: [subop: SUM tag: 23995]
Operation_30555: operator = Consume, inputs = [Tensor_30553], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_30556: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13706: 4718592 Bnr)
Tensor_30557: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (30560: 1536 Ban)
Operation_24000: operator = AllReduceDuplicated, inputs = [Tensor_30557, nullptr], outputs = [Tensor_30560, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: SUM tag: 23999]
Operation_30559: operator = Consume, inputs = [Tensor_30557], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_30560: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30560: 1536 Bnr)
Operation_15489: operator = TypecastBackward, inputs = [Tensor_30552], outputs = [Tensor_30562], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_30562: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13713: 154927104 Bnr)
Operation_15491: operator = LayernormBackward, inputs = [Tensor_30562, Tensor_27768, Tensor_27769, Tensor_27637, Tensor_27638, Tensor_27770, Tensor_27771], outputs = [Tensor_30564, Tensor_30565, Tensor_30569], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->24008
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_30564: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (28311: 154927104 Bnr)
Tensor_30565: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (30568: 12288 Ban)
Operation_24008: operator = AllReduceDuplicated, inputs = [Tensor_30565, nullptr], outputs = [Tensor_30568, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 15491->this
  - Operand: [subop: SUM tag: 24007]
Operation_30567: operator = Consume, inputs = [Tensor_30565], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_30568: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30568: 12288 Bnn)
Tensor_30569: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (30572: 12288 Ban)
Operation_24012: operator = AllReduceDuplicated, inputs = [Tensor_30569, nullptr], outputs = [Tensor_30572, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: SUM tag: 24011]
Operation_30571: operator = Consume, inputs = [Tensor_30569], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_30572: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30572: 12288 Bnn)
Operation_15495: operator = TypecastBackward, inputs = [Tensor_30564], outputs = [Tensor_30574], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_30574: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13192: 77463552 Bnn)
Operation_15497: operator = TypecastBackward, inputs = [Tensor_30574], outputs = [Tensor_30576], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_30576: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12757: 154927104 Bnn)
Operation_15499: operator = ElemwiseUnaryBackward, inputs = [Tensor_30576, Tensor_27766, Tensor_27767], outputs = [Tensor_30578], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_GELU]
Tensor_30578: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12757: 154927104 Bnr)
Operation_15501: operator = TypecastBackward, inputs = [Tensor_30578], outputs = [Tensor_30580], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_30580: shape = [25216, 3072], stride = [3072, 1], offset = 0, size = 77463552, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13550: 77463552 Bnr)
Operation_15503: operator = LinearBackward, inputs = [Tensor_30580, Tensor_27764, Tensor_27765], outputs = [Tensor_30582, Tensor_30583, Tensor_30587], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->24026
  - Operand: []
Tensor_30582: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12677: 19365888 Bnr)
Tensor_30583: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (11987: 4718592 Bnn)
Operation_24026: operator = AllReduceDuplicated, inputs = [Tensor_30583, nullptr], outputs = [Tensor_30586, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 15503->this
  - Operand: [subop: SUM tag: 24025]
Operation_30585: operator = Consume, inputs = [Tensor_30583], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_30586: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11987: 4718592 Bnr)
Tensor_30587: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (11969: 6144 Bnn)
Operation_24030: operator = AllReduceDuplicated, inputs = [Tensor_30587, nullptr], outputs = [Tensor_30590, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: SUM tag: 24029]
Operation_30589: operator = Consume, inputs = [Tensor_30587], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_30590: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11969: 6144 Bnr)
Operation_15507: operator = TypecastBackward, inputs = [Tensor_30582], outputs = [Tensor_30592], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_30592: shape = [25216, 768], stride = [768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13027: 38731776 Bnn)
Operation_15509: operator = ReshapeBackward, inputs = [Tensor_30592], outputs = [Tensor_30594], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 25216 shape: 768 stride: 768 stride: 1]
Tensor_30594: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13027: 38731776 Bnr)
Operation_15511: operator = LayernormBackward, inputs = [Tensor_30594, Tensor_27760, Tensor_27761, Tensor_27639, Tensor_27640, Tensor_27762, Tensor_27763], outputs = [Tensor_30596, Tensor_30597, Tensor_30601], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->24040
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_30596: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12554: 38731776 Bnn)
Tensor_30597: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (30600: 3072 Ban)
Operation_24040: operator = AllReduceDuplicated, inputs = [Tensor_30597, nullptr], outputs = [Tensor_30600, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 15511->this
  - Operand: [subop: SUM tag: 24039]
Operation_30599: operator = Consume, inputs = [Tensor_30597], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_30600: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30600: 3072 Bnn)
Tensor_30601: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (30604: 3072 Ban)
Operation_24044: operator = AllReduceDuplicated, inputs = [Tensor_30601, nullptr], outputs = [Tensor_30604, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: SUM tag: 24043]
Operation_30603: operator = Consume, inputs = [Tensor_30601], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_30604: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30604: 3072 Bnn)
Operation_15513: operator = ElemwiseBinaryForward, inputs = [Tensor_30546, Tensor_30596], outputs = [Tensor_30606], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_30606: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12156: 38731776 Bnn)
Operation_15520: operator = ElemwiseUnaryBackward, inputs = [Tensor_30606, nullptr, nullptr], outputs = [Tensor_30608], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 1]
Tensor_30608: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (28355: 38731776 Bnn)
Operation_15522: operator = TypecastBackward, inputs = [Tensor_30606], outputs = [Tensor_30610], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_30610: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12649: 19365888 Bnr)
Operation_15524: operator = LinearBackward, inputs = [Tensor_30610, Tensor_27758, Tensor_27759], outputs = [Tensor_30612, Tensor_30613, Tensor_30617], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->24056
  - Operand: []
Tensor_30612: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12013: 19365888 Bnn)
Tensor_30613: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (12007: 1179648 Bnn)
Operation_24056: operator = AllReduceDuplicated, inputs = [Tensor_30613, nullptr], outputs = [Tensor_30616, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 15524->this
  - Operand: [subop: SUM tag: 24055]
Operation_30615: operator = Consume, inputs = [Tensor_30613], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_30616: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12007: 1179648 Bnr)
Tensor_30617: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (30620: 1536 Ban)
Operation_24060: operator = AllReduceDuplicated, inputs = [Tensor_30617, nullptr], outputs = [Tensor_30620, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: SUM tag: 24059]
Operation_30619: operator = Consume, inputs = [Tensor_30617], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_30620: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30620: 1536 Bnr)
Operation_15528: operator = TypecastBackward, inputs = [Tensor_30612], outputs = [Tensor_30622], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_30622: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11959: 38731776 Bnr)
Tensor_30623: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (28402: 38731776 Bnr)
Operation_15530: operator = LayernormBackward, inputs = [Tensor_30622, Tensor_30623, Tensor_27755, Tensor_27633, Tensor_27634, Tensor_27756, Tensor_27757], outputs = [Tensor_30625, Tensor_30626, Tensor_30630], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->24070
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_30625: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12156: 38731776 Bnr)
Tensor_30626: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (30629: 3072 Ban)
Operation_24070: operator = AllReduceDuplicated, inputs = [Tensor_30626, nullptr], outputs = [Tensor_30629, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 15530->this
  - Operand: [subop: SUM tag: 24069]
Operation_30628: operator = Consume, inputs = [Tensor_30626], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_30629: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30629: 3072 Bnn)
Tensor_30630: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (30633: 3072 Ban)
Operation_24074: operator = AllReduceDuplicated, inputs = [Tensor_30630, nullptr], outputs = [Tensor_30633, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: SUM tag: 24073]
Operation_30632: operator = Consume, inputs = [Tensor_30630], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_30633: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30633: 3072 Bnn)
Operation_24077: operator = AllGatherDuplicated, inputs = [Tensor_30625], outputs = [Tensor_30636], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->15534
  - Operand: [tag: 24076]
Operation_30635: operator = Consume, inputs = [Tensor_30625], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_30636: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13192: 77463552 Bnr)
Operation_15743: operator = TypecastBackward, inputs = [Tensor_29073], outputs = [Tensor_30819], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Operation_15745: operator = TypecastBackward, inputs = [Tensor_29077], outputs = [Tensor_30820], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Operation_15747: operator = TypecastBackward, inputs = [Tensor_29085], outputs = [Tensor_30821], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Operation_15749: operator = TypecastBackward, inputs = [Tensor_29089], outputs = [Tensor_30822], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Operation_15751: operator = TypecastBackward, inputs = [Tensor_29125], outputs = [Tensor_30823], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Operation_15534: operator = TypecastBackward, inputs = [Tensor_30636], outputs = [Tensor_30643], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 24077->this
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_30643: shape = [128, 197, 768], stride = [768, 98304, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11936: 38731776 Bnn)
Operation_15538: operator = ReshapeBackward, inputs = [Tensor_30643], outputs = [Tensor_30645], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 197 shape: 128 shape: 768 stride: 98304 stride: 768 stride: 1]
Tensor_30645: shape = [197, 1536, 64], stride = [98304, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11936: 38731776 Bnn)
Operation_15540: operator = ContiguousBackward, inputs = [Tensor_30645], outputs = [Tensor_30647], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_30647: shape = [197, 1536, 64], stride = [64, 12608, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12554: 38731776 Bnn)
Operation_15542: operator = ReshapeBackward, inputs = [Tensor_30647], outputs = [Tensor_30649], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 197 shape: 1536 shape: 64 stride: 64 stride: 12608 stride: 1]
Tensor_30649: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12554: 38731776 Bnn)
Operation_24087: operator = DistributeWithMemAllocForward, inputs = [Tensor_30649], outputs = [Tensor_30651], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_30651: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12897: 19365888 Bnr)
Operation_15544: operator = BatchedMatmulBackward, inputs = [Tensor_30651, Tensor_27753, Tensor_27749], outputs = [Tensor_30653, Tensor_30654], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_30653: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13164: 59610624 Bnr)
Tensor_30654: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11902: 19365888 Bnn)
Operation_15547: operator = TypecastBackward, inputs = [Tensor_30653], outputs = [Tensor_30656], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_30656: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12536: 119221248 Bnr)
Operation_15551: operator = SoftmaxBackward, inputs = [Tensor_30656, Tensor_27752], outputs = [Tensor_30658], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dim: 2]
Tensor_30658: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13013: 119221248 Bnr)
Operation_15549: operator = ReshapeBackward, inputs = [Tensor_30654], outputs = [Tensor_30660], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_30660: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11902: 19365888 Bnn)
Operation_15553: operator = ContiguousBackward, inputs = [Tensor_30660], outputs = [Tensor_30662], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_30662: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12188: 19365888 Bnn)
Operation_15555: operator = TypecastBackward, inputs = [Tensor_30658], outputs = [Tensor_30664], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_30664: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13482: 59610624 Bnn)
Operation_15559: operator = ReshapeBackward, inputs = [Tensor_30664], outputs = [Tensor_30666], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 197 stride: 38809 stride: 197 stride: 1]
Tensor_30666: shape = [128, 12, 197, 197], stride = [465708, 38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13482: 59610624 Bnn)
Operation_15563: operator = MaskedFillBackward, inputs = [Tensor_30666, Tensor_27751], outputs = [Tensor_30668], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [value_float: -inf]
Tensor_30668: shape = [128, 12, 197, 197], stride = [465708, 38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13482: 59610624 Bnn)
Operation_15561: operator = ReshapeBackward, inputs = [Tensor_30662], outputs = [Tensor_30670], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 12 shape: 64 stride: 151296 stride: 768 stride: 64 stride: 1]
Tensor_30670: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12188: 19365888 Bnr)
Operation_15565: operator = LinearBackward, inputs = [Tensor_30670, Tensor_27746, Tensor_27747], outputs = [Tensor_30672, Tensor_30673, Tensor_30677], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->24112
  - Operand: []
Tensor_30672: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12013: 19365888 Bnn)
Tensor_30673: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (13136: 1179648 Bnn)
Operation_24112: operator = AllReduceDuplicated, inputs = [Tensor_30673, nullptr], outputs = [Tensor_30676, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 15565->this->15931
  - Operand: [subop: SUM tag: 24111]
Operation_30675: operator = Consume, inputs = [Tensor_30673], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_30676: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (13136: 1179648 Bnr)
Tensor_30677: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (29089: 1536 Bnn)
Operation_24116: operator = AllReduceDuplicated, inputs = [Tensor_30677, nullptr], outputs = [Tensor_30680, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->15933
  - Operand: [subop: SUM tag: 24115]
Operation_30679: operator = Consume, inputs = [Tensor_30677], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_30680: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29089: 1536 Bnr)
Operation_15571: operator = TypecastBackward, inputs = [Tensor_30672], outputs = [Tensor_30682], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_30682: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12554: 38731776 Bnn)
Operation_15569: operator = ReshapeBackward, inputs = [Tensor_30668], outputs = [Tensor_30684], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 12 shape: 197 shape: 197 stride: 465708 stride: 38809 stride: 197 stride: 1]
Tensor_30684: shape = [1536, 197, 197], stride = [38809, 197, 1], offset = 0, size = 59610624, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (13482: 59610624 Bnr)
Operation_15573: operator = BatchedMatmulBackward, inputs = [Tensor_30684, Tensor_27748, Tensor_27750], outputs = [Tensor_30686, Tensor_30687], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_30686: shape = [1536, 197, 64], stride = [12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12657: 19365888 Bnn)
Tensor_30687: shape = [1536, 64, 197], stride = [12608, 1, 64], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12013: 19365888 Bnn)
Operation_15576: operator = ReshapeBackward, inputs = [Tensor_30686], outputs = [Tensor_30689], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_30689: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12657: 19365888 Bnn)
Operation_15580: operator = ContiguousBackward, inputs = [Tensor_30689], outputs = [Tensor_30691], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_30691: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11882: 19365888 Bnn)
Operation_15582: operator = ReshapeBackward, inputs = [Tensor_30687], outputs = [Tensor_30693], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 1536 shape: 197 shape: 64 stride: 12608 stride: 64 stride: 1]
Tensor_30693: shape = [128, 12, 197, 64], stride = [151296, 12608, 64, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12013: 19365888 Bnr)
Operation_15586: operator = ContiguousBackward, inputs = [Tensor_30693], outputs = [Tensor_30695], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_30695: shape = [128, 12, 197, 64], stride = [151296, 64, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11902: 19365888 Bnn)
Operation_15588: operator = ReshapeBackward, inputs = [Tensor_30691], outputs = [Tensor_30697], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 12 shape: 64 stride: 151296 stride: 768 stride: 64 stride: 1]
Tensor_30697: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11882: 19365888 Bnn)
Operation_15592: operator = ElemwiseUnaryBackward, inputs = [Tensor_30697, nullptr, nullptr], outputs = [Tensor_30699], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 0.125]
Tensor_30699: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11882: 19365888 Bnr)
Operation_15596: operator = LinearBackward, inputs = [Tensor_30699, Tensor_27742, Tensor_27743], outputs = [Tensor_30701, Tensor_30702, Tensor_30706], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->24141
  - Operand: []
Tensor_30701: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12657: 19365888 Bnr)
Tensor_30702: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (11884: 1179648 Bnn)
Operation_24141: operator = AllReduceDuplicated, inputs = [Tensor_30702, nullptr], outputs = [Tensor_30705, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 15596->this->15935
  - Operand: [subop: SUM tag: 24140]
Operation_30704: operator = Consume, inputs = [Tensor_30702], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_30705: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11884: 1179648 Bnr)
Tensor_30706: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (28373: 1536 Bnn)
Operation_24145: operator = AllReduceDuplicated, inputs = [Tensor_30706, nullptr], outputs = [Tensor_30709, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->15937
  - Operand: [subop: SUM tag: 24144]
Operation_30708: operator = Consume, inputs = [Tensor_30706], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_30709: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28373: 1536 Bnr)
Operation_15594: operator = ReshapeBackward, inputs = [Tensor_30695], outputs = [Tensor_30711], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 12 shape: 64 stride: 151296 stride: 768 stride: 64 stride: 1]
Tensor_30711: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11902: 19365888 Bnr)
Operation_15600: operator = LinearBackward, inputs = [Tensor_30711, Tensor_27744, Tensor_27745], outputs = [Tensor_30713, Tensor_30714, Tensor_30718], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->24153
  - Operand: []
Tensor_30713: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11890: 19365888 Bnr)
Tensor_30714: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (12643: 1179648 Bnn)
Operation_24153: operator = AllReduceDuplicated, inputs = [Tensor_30714, nullptr], outputs = [Tensor_30717, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 15600->this->15939
  - Operand: [subop: SUM tag: 24152]
Operation_30716: operator = Consume, inputs = [Tensor_30714], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_30717: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (12643: 1179648 Bnr)
Tensor_30718: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (30721: 1536 Ban)
Operation_24157: operator = AllReduceDuplicated, inputs = [Tensor_30718, nullptr], outputs = [Tensor_30721, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->15941
  - Operand: [subop: SUM tag: 24156]
Operation_30720: operator = Consume, inputs = [Tensor_30718], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_30721: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30721: 1536 Bnr)
Operation_15604: operator = TypecastBackward, inputs = [Tensor_30701], outputs = [Tensor_30723], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_30723: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11936: 38731776 Bnr)
Operation_15606: operator = ElemwiseBinaryForward, inputs = [Tensor_30682, Tensor_30723], outputs = [Tensor_30725], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_30725: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12554: 38731776 Bnn)
Operation_15608: operator = TypecastBackward, inputs = [Tensor_30713], outputs = [Tensor_30727], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_30727: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (28786: 38731776 Bnr)
Operation_15610: operator = ElemwiseBinaryForward, inputs = [Tensor_30725, Tensor_30727], outputs = [Tensor_30729], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_30729: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (12554: 38731776 Bnr)
Operation_15612: operator = LayernormBackward, inputs = [Tensor_30729, Tensor_27738, Tensor_27739, Tensor_27635, Tensor_27636, Tensor_27740, Tensor_27741], outputs = [Tensor_30731, Tensor_30732, Tensor_30736], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->24171
  - Operand: [eps: 1e-05 normalized_dims: 1]
Tensor_30731: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (28991: 38731776 Bnr)
Tensor_30732: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (30735: 3072 Ban)
Operation_24171: operator = AllReduceDuplicated, inputs = [Tensor_30732, nullptr], outputs = [Tensor_30735, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 15612->this
  - Operand: [subop: SUM tag: 24170]
Operation_30734: operator = Consume, inputs = [Tensor_30732], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_30735: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30735: 3072 Bnn)
Tensor_30736: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (30739: 3072 Ban)
Operation_24175: operator = AllReduceDuplicated, inputs = [Tensor_30736, nullptr], outputs = [Tensor_30739, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: SUM tag: 24174]
Operation_30738: operator = Consume, inputs = [Tensor_30736], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_30739: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30739: 3072 Bnn)
Operation_15614: operator = ElemwiseBinaryForward, inputs = [Tensor_30608, Tensor_30731], outputs = [Tensor_30741], # backwardable inputs = 2, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_ADD]
Tensor_30741: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (28355: 38731776 Bnn)
Operation_15618: operator = ElemwiseBinaryBackward, inputs = [Tensor_30741, Tensor_27736, Tensor_27737, nullptr], outputs = [Tensor_30743, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_BINARY_MUL]
Tensor_30743: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (28355: 38731776 Bnn)
Operation_15623: operator = SumForward, inputs = [Tensor_30743], outputs = [Tensor_30745], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dims: 0]
Tensor_30745: shape = [197, 768], stride = [768, 1], offset = 0, size = 151296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (11848: 605184 Brn)
Operation_15627: operator = ElemwiseUnaryBackward, inputs = [Tensor_30743, nullptr, nullptr], outputs = [Tensor_30747], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_MUL scal1_float: 1]
Tensor_30747: shape = [128, 197, 768], stride = [151296, 768, 1], offset = 0, size = 19365888, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (28355: 38731776 Bnr)
Operation_15631: operator = CatBackward, inputs = [Tensor_30747, Tensor_27733, Tensor_27734], outputs = [Tensor_30749, Tensor_30750], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dim: 1]
Tensor_30749: shape = [128, 1, 768], stride = [768, 768, 1], offset = 0, size = 98304, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (28277: 196608 Bnr)
Tensor_30750: shape = [128, 196, 768], stride = [150528, 768, 1], offset = 0, size = 19267584, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11817: 38535168 Bnr)
Operation_15633: operator = SumForward, inputs = [Tensor_30749], outputs = [Tensor_30752], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dims: 0]
Tensor_30752: shape = [1, 768], stride = [768, 1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (30758: 3072 Ban)
Operation_15629: operator = ReshapeBackward, inputs = [Tensor_30745], outputs = [Tensor_30754], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 197 shape: 768 stride: 0 stride: 768 stride: 1]
Tensor_30754: shape = [1, 197, 768], stride = [151296, 768, 1], offset = 0, size = 151296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (11848: 605184 Bnr)
Operation_15638: operator = EmbeddingBackward, inputs = [Tensor_30754, Tensor_27735], outputs = [Tensor_30756], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [norm_type: 2 padding_idx: -1]
Tensor_30756: shape = [199, 768], stride = [768, 1], offset = 0, size = 152832, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (30756: 611328 Ban)
Operation_15640: operator = ReshapeBackward, inputs = [Tensor_30752], outputs = [Tensor_30758], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 1 shape: 768 stride: 0 stride: 768 stride: 1]
Tensor_30758: shape = [1, 1, 768], stride = [768, 768, 1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (30758: 3072 Bnn)
Operation_15642: operator = TypecastBackward, inputs = [Tensor_30750], outputs = [Tensor_30760], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT32]
Tensor_30760: shape = [128, 196, 768], stride = [150528, 1, 196], offset = 0, size = 19267584, datatype = float16, contiguous? = false, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11823: 19267584 Brn)
Operation_15646: operator = ReshapeBackward, inputs = [Tensor_30760], outputs = [Tensor_30762], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 128 shape: 768 shape: 196 stride: 150528 stride: 196 stride: 1]
Tensor_30762: shape = [128, 768, 14, 14], stride = [150528, 196, 14, 1], offset = 0, size = 19267584, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11823: 19267584 Bnr)
Tensor_30763: shape = [128, 3, 224, 224], stride = [150528, 50176, 224, 1], offset = 0, size = 19267584, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (div:128), attr<Storage> = (11829: 19267584 Brr)
Operation_15648: operator = Convolution2dBackward, inputs = [Tensor_30762, Tensor_30763, Tensor_27732], outputs = [nullptr, Tensor_30765, Tensor_30769], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->24205
  - Operand: [stride_h: 16 stride_w: 16 dilation_h: 1 dilation_w: 1 groups: 1]
Tensor_30765: shape = [768, 3, 16, 16], stride = [768, 256, 16, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (11868: 1179648 Bnn)
Operation_24205: operator = AllReduceDuplicated, inputs = [Tensor_30765, nullptr], outputs = [Tensor_30768, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 15648->this->15943
  - Operand: [subop: SUM tag: 24204]
Operation_30767: operator = Consume, inputs = [Tensor_30765], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_30768: shape = [768, 3, 16, 16], stride = [768, 256, 16, 1], offset = 0, size = 589824, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (11868: 1179648 Bnr)
Tensor_30769: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (30772: 1536 Ban)
Operation_24209: operator = AllReduceDuplicated, inputs = [Tensor_30769, nullptr], outputs = [Tensor_30772, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->15945
  - Operand: [subop: SUM tag: 24208]
Operation_30771: operator = Consume, inputs = [Tensor_30769], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_30772: shape = [768], stride = [1], offset = 0, size = 768, datatype = float16, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30772: 1536 Bnr)
Tensor_30773: shape = [1000, 768], stride = [768, 1], offset = 0, size = 768000, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30773: 3072000 Ban)
Tensor_30774: shape = [1000], stride = [1], offset = 0, size = 1000, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30774: 4000 Ban)
Tensor_30775: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30775: 9437184 Ban)
Tensor_30776: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30776: 3072 Ban)
Tensor_30777: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30777: 9437184 Ban)
Tensor_30778: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30778: 12288 Ban)
Tensor_30779: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30779: 2359296 Ban)
Tensor_30780: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30780: 3072 Ban)
Tensor_30781: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30781: 2359296 Ban)
Tensor_30782: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30782: 3072 Ban)
Tensor_30783: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30783: 2359296 Ban)
Tensor_30784: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30784: 3072 Ban)
Tensor_30785: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30785: 2359296 Ban)
Tensor_30786: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30786: 3072 Ban)
Tensor_30787: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30787: 9437184 Ban)
Tensor_30788: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30788: 3072 Ban)
Tensor_30789: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30789: 9437184 Ban)
Tensor_30790: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30790: 12288 Ban)
Tensor_30791: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30791: 2359296 Ban)
Tensor_30792: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30792: 3072 Ban)
Tensor_30793: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30793: 2359296 Ban)
Tensor_30794: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30794: 3072 Ban)
Tensor_30795: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30795: 2359296 Ban)
Tensor_30796: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30796: 3072 Ban)
Tensor_30797: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30797: 2359296 Ban)
Tensor_30798: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30798: 3072 Ban)
Tensor_30799: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30799: 9437184 Ban)
Tensor_30800: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30800: 3072 Ban)
Tensor_30801: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30801: 9437184 Ban)
Tensor_30802: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30802: 12288 Ban)
Tensor_30803: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30803: 2359296 Ban)
Tensor_30804: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30804: 3072 Ban)
Tensor_30805: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30805: 2359296 Ban)
Tensor_30806: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30806: 3072 Ban)
Tensor_30807: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30807: 2359296 Ban)
Tensor_30808: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30808: 3072 Ban)
Tensor_30809: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30809: 2359296 Ban)
Tensor_30810: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30810: 3072 Ban)
Tensor_30811: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30811: 9437184 Ban)
Tensor_30812: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30812: 3072 Ban)
Tensor_30813: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30813: 9437184 Ban)
Tensor_30814: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30814: 12288 Ban)
Tensor_30815: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30815: 2359296 Ban)
Tensor_30816: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30816: 3072 Ban)
Tensor_30817: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30817: 2359296 Ban)
Tensor_30818: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30818: 3072 Ban)
Tensor_30819: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30819: 2359296 Ban)
Tensor_30820: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30820: 3072 Ban)
Tensor_30821: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30821: 2359296 Ban)
Tensor_30822: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30822: 3072 Ban)
Tensor_30823: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30823: 9437184 Ban)
Operation_15753: operator = TypecastBackward, inputs = [Tensor_29129], outputs = [Tensor_30825], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_30825: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30825: 3072 Ban)
Operation_15755: operator = TypecastBackward, inputs = [Tensor_29155], outputs = [Tensor_30827], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_30827: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30827: 9437184 Ban)
Operation_15757: operator = TypecastBackward, inputs = [Tensor_29159], outputs = [Tensor_30829], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_30829: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30829: 12288 Ban)
Operation_15759: operator = TypecastBackward, inputs = [Tensor_29189], outputs = [Tensor_30831], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_30831: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30831: 2359296 Ban)
Operation_15761: operator = TypecastBackward, inputs = [Tensor_29193], outputs = [Tensor_30833], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_30833: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30833: 3072 Ban)
Operation_15763: operator = TypecastBackward, inputs = [Tensor_29249], outputs = [Tensor_30835], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_30835: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30835: 2359296 Ban)
Operation_15765: operator = TypecastBackward, inputs = [Tensor_29253], outputs = [Tensor_30837], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_30837: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30837: 3072 Ban)
Operation_15767: operator = TypecastBackward, inputs = [Tensor_29278], outputs = [Tensor_30839], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_30839: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30839: 2359296 Ban)
Operation_15769: operator = TypecastBackward, inputs = [Tensor_29282], outputs = [Tensor_30841], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_30841: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30841: 3072 Ban)
Operation_15771: operator = TypecastBackward, inputs = [Tensor_29290], outputs = [Tensor_30843], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_30843: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30843: 2359296 Ban)
Operation_15773: operator = TypecastBackward, inputs = [Tensor_29294], outputs = [Tensor_30845], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_30845: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30845: 3072 Ban)
Operation_15775: operator = TypecastBackward, inputs = [Tensor_29330], outputs = [Tensor_30847], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_30847: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30847: 9437184 Ban)
Operation_15777: operator = TypecastBackward, inputs = [Tensor_29334], outputs = [Tensor_30849], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_30849: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30849: 3072 Ban)
Operation_15779: operator = TypecastBackward, inputs = [Tensor_29360], outputs = [Tensor_30851], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_30851: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30851: 9437184 Ban)
Operation_15781: operator = TypecastBackward, inputs = [Tensor_29364], outputs = [Tensor_30853], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_30853: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30853: 12288 Ban)
Operation_15783: operator = TypecastBackward, inputs = [Tensor_29394], outputs = [Tensor_30855], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_30855: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30855: 2359296 Ban)
Operation_15785: operator = TypecastBackward, inputs = [Tensor_29398], outputs = [Tensor_30857], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_30857: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30857: 3072 Ban)
Operation_15787: operator = TypecastBackward, inputs = [Tensor_29454], outputs = [Tensor_30859], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_30859: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30859: 2359296 Ban)
Operation_15789: operator = TypecastBackward, inputs = [Tensor_29458], outputs = [Tensor_30861], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_30861: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30861: 3072 Ban)
Operation_15791: operator = TypecastBackward, inputs = [Tensor_29483], outputs = [Tensor_30863], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_30863: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30863: 2359296 Ban)
Operation_15793: operator = TypecastBackward, inputs = [Tensor_29487], outputs = [Tensor_30865], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_30865: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30865: 3072 Ban)
Operation_15795: operator = TypecastBackward, inputs = [Tensor_29495], outputs = [Tensor_30867], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_30867: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30867: 2359296 Ban)
Operation_15797: operator = TypecastBackward, inputs = [Tensor_29499], outputs = [Tensor_30869], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_30869: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30869: 3072 Ban)
Operation_15799: operator = TypecastBackward, inputs = [Tensor_29535], outputs = [Tensor_30871], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_30871: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30871: 9437184 Ban)
Operation_15801: operator = TypecastBackward, inputs = [Tensor_29539], outputs = [Tensor_30873], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_30873: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30873: 3072 Ban)
Operation_15803: operator = TypecastBackward, inputs = [Tensor_29565], outputs = [Tensor_30875], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_30875: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30875: 9437184 Ban)
Operation_15805: operator = TypecastBackward, inputs = [Tensor_29569], outputs = [Tensor_30877], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_30877: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30877: 12288 Ban)
Operation_15807: operator = TypecastBackward, inputs = [Tensor_29599], outputs = [Tensor_30879], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_30879: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30879: 2359296 Ban)
Operation_15809: operator = TypecastBackward, inputs = [Tensor_29603], outputs = [Tensor_30881], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_30881: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30881: 3072 Ban)
Operation_15811: operator = TypecastBackward, inputs = [Tensor_29659], outputs = [Tensor_30883], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_30883: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30883: 2359296 Ban)
Operation_15813: operator = TypecastBackward, inputs = [Tensor_29663], outputs = [Tensor_30885], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_30885: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30885: 3072 Ban)
Operation_15815: operator = TypecastBackward, inputs = [Tensor_29688], outputs = [Tensor_30887], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_30887: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30887: 2359296 Ban)
Operation_15817: operator = TypecastBackward, inputs = [Tensor_29692], outputs = [Tensor_30889], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_30889: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30889: 3072 Ban)
Operation_15819: operator = TypecastBackward, inputs = [Tensor_29700], outputs = [Tensor_30891], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_30891: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30891: 2359296 Ban)
Operation_15821: operator = TypecastBackward, inputs = [Tensor_29704], outputs = [Tensor_30893], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_30893: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30893: 3072 Ban)
Operation_15823: operator = TypecastBackward, inputs = [Tensor_29740], outputs = [Tensor_30895], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_30895: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30895: 9437184 Ban)
Operation_15825: operator = TypecastBackward, inputs = [Tensor_29744], outputs = [Tensor_30897], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_30897: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30897: 3072 Ban)
Operation_15827: operator = TypecastBackward, inputs = [Tensor_29770], outputs = [Tensor_30899], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_30899: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30899: 9437184 Ban)
Operation_15829: operator = TypecastBackward, inputs = [Tensor_29774], outputs = [Tensor_30901], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_30901: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30901: 12288 Ban)
Operation_15831: operator = TypecastBackward, inputs = [Tensor_29804], outputs = [Tensor_30903], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_30903: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30903: 2359296 Ban)
Operation_15833: operator = TypecastBackward, inputs = [Tensor_29808], outputs = [Tensor_30905], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_30905: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30905: 3072 Ban)
Operation_15835: operator = TypecastBackward, inputs = [Tensor_29864], outputs = [Tensor_30907], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_30907: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30907: 2359296 Ban)
Operation_15837: operator = TypecastBackward, inputs = [Tensor_29868], outputs = [Tensor_30909], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_30909: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30909: 3072 Ban)
Operation_15839: operator = TypecastBackward, inputs = [Tensor_29893], outputs = [Tensor_30911], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_30911: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30911: 2359296 Ban)
Operation_15841: operator = TypecastBackward, inputs = [Tensor_29897], outputs = [Tensor_30913], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_30913: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30913: 3072 Ban)
Operation_15843: operator = TypecastBackward, inputs = [Tensor_29905], outputs = [Tensor_30915], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_30915: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30915: 2359296 Ban)
Operation_15845: operator = TypecastBackward, inputs = [Tensor_29909], outputs = [Tensor_30917], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_30917: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30917: 3072 Ban)
Operation_15847: operator = TypecastBackward, inputs = [Tensor_29945], outputs = [Tensor_30919], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_30919: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30919: 9437184 Ban)
Operation_15849: operator = TypecastBackward, inputs = [Tensor_29949], outputs = [Tensor_30921], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_30921: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30921: 3072 Ban)
Operation_15851: operator = TypecastBackward, inputs = [Tensor_29975], outputs = [Tensor_30923], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_30923: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30923: 9437184 Ban)
Operation_15853: operator = TypecastBackward, inputs = [Tensor_29979], outputs = [Tensor_30925], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_30925: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30925: 12288 Ban)
Operation_15855: operator = TypecastBackward, inputs = [Tensor_30009], outputs = [Tensor_30927], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_30927: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30927: 2359296 Ban)
Operation_15857: operator = TypecastBackward, inputs = [Tensor_30013], outputs = [Tensor_30929], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_30929: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30929: 3072 Ban)
Operation_15859: operator = TypecastBackward, inputs = [Tensor_30069], outputs = [Tensor_30931], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_30931: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30931: 2359296 Ban)
Operation_15861: operator = TypecastBackward, inputs = [Tensor_30073], outputs = [Tensor_30933], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_30933: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30933: 3072 Ban)
Operation_15863: operator = TypecastBackward, inputs = [Tensor_30098], outputs = [Tensor_30935], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_30935: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30935: 2359296 Ban)
Operation_15865: operator = TypecastBackward, inputs = [Tensor_30102], outputs = [Tensor_30937], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_30937: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30937: 3072 Ban)
Operation_15867: operator = TypecastBackward, inputs = [Tensor_30110], outputs = [Tensor_30939], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_30939: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30939: 2359296 Ban)
Operation_15869: operator = TypecastBackward, inputs = [Tensor_30114], outputs = [Tensor_30941], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_30941: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30941: 3072 Ban)
Operation_15871: operator = TypecastBackward, inputs = [Tensor_30150], outputs = [Tensor_30943], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_30943: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30943: 9437184 Ban)
Operation_15873: operator = TypecastBackward, inputs = [Tensor_30154], outputs = [Tensor_30945], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_30945: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30945: 3072 Ban)
Operation_15875: operator = TypecastBackward, inputs = [Tensor_30180], outputs = [Tensor_30947], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_30947: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30947: 9437184 Ban)
Operation_15877: operator = TypecastBackward, inputs = [Tensor_30184], outputs = [Tensor_30949], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_30949: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30949: 12288 Ban)
Operation_15879: operator = TypecastBackward, inputs = [Tensor_30214], outputs = [Tensor_30951], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_30951: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30951: 2359296 Ban)
Operation_15881: operator = TypecastBackward, inputs = [Tensor_30218], outputs = [Tensor_30953], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_30953: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30953: 3072 Ban)
Operation_15883: operator = TypecastBackward, inputs = [Tensor_30274], outputs = [Tensor_30955], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_30955: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30955: 2359296 Ban)
Operation_15885: operator = TypecastBackward, inputs = [Tensor_30278], outputs = [Tensor_30957], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_30957: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30957: 3072 Ban)
Operation_15887: operator = TypecastBackward, inputs = [Tensor_30303], outputs = [Tensor_30959], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_30959: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30959: 2359296 Ban)
Operation_15889: operator = TypecastBackward, inputs = [Tensor_30307], outputs = [Tensor_30961], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_30961: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30961: 3072 Ban)
Operation_15891: operator = TypecastBackward, inputs = [Tensor_30315], outputs = [Tensor_30963], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_30963: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30963: 2359296 Ban)
Operation_15893: operator = TypecastBackward, inputs = [Tensor_30319], outputs = [Tensor_30965], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_30965: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30965: 3072 Ban)
Operation_15895: operator = TypecastBackward, inputs = [Tensor_30355], outputs = [Tensor_30967], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_30967: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30967: 9437184 Ban)
Operation_15897: operator = TypecastBackward, inputs = [Tensor_30359], outputs = [Tensor_30969], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_30969: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30969: 3072 Ban)
Operation_15899: operator = TypecastBackward, inputs = [Tensor_30385], outputs = [Tensor_30971], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_30971: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30971: 9437184 Ban)
Operation_15901: operator = TypecastBackward, inputs = [Tensor_30389], outputs = [Tensor_30973], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_30973: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30973: 12288 Ban)
Operation_15903: operator = TypecastBackward, inputs = [Tensor_30419], outputs = [Tensor_30975], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_30975: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30975: 2359296 Ban)
Operation_15905: operator = TypecastBackward, inputs = [Tensor_30423], outputs = [Tensor_30977], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_30977: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30977: 3072 Ban)
Operation_15907: operator = TypecastBackward, inputs = [Tensor_30479], outputs = [Tensor_30979], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_30979: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30979: 2359296 Ban)
Operation_15909: operator = TypecastBackward, inputs = [Tensor_30483], outputs = [Tensor_30981], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_30981: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30981: 3072 Ban)
Operation_15911: operator = TypecastBackward, inputs = [Tensor_30508], outputs = [Tensor_30983], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_30983: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30983: 2359296 Ban)
Operation_15913: operator = TypecastBackward, inputs = [Tensor_30512], outputs = [Tensor_30985], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_30985: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30985: 3072 Ban)
Operation_15915: operator = TypecastBackward, inputs = [Tensor_30520], outputs = [Tensor_30987], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_30987: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30987: 2359296 Ban)
Operation_15917: operator = TypecastBackward, inputs = [Tensor_30524], outputs = [Tensor_30989], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_30989: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30989: 3072 Ban)
Operation_15919: operator = TypecastBackward, inputs = [Tensor_30556], outputs = [Tensor_30991], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_30991: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30991: 9437184 Ban)
Operation_15921: operator = TypecastBackward, inputs = [Tensor_30560], outputs = [Tensor_30993], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_30993: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30993: 3072 Ban)
Operation_15923: operator = TypecastBackward, inputs = [Tensor_30586], outputs = [Tensor_30995], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_30995: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30995: 9437184 Ban)
Operation_15925: operator = TypecastBackward, inputs = [Tensor_30590], outputs = [Tensor_30997], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_30997: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30997: 12288 Ban)
Operation_15927: operator = TypecastBackward, inputs = [Tensor_30616], outputs = [Tensor_30999], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_30999: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30999: 2359296 Ban)
Operation_15929: operator = TypecastBackward, inputs = [Tensor_30620], outputs = [Tensor_31001], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_31001: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (31001: 3072 Ban)
Operation_15931: operator = TypecastBackward, inputs = [Tensor_30676], outputs = [Tensor_31003], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 24112->this
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_31003: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (31003: 2359296 Ban)
Operation_15933: operator = TypecastBackward, inputs = [Tensor_30680], outputs = [Tensor_31005], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 24116->this
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_31005: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (31005: 3072 Ban)
Operation_15935: operator = TypecastBackward, inputs = [Tensor_30705], outputs = [Tensor_31007], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 24141->this
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_31007: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (31007: 2359296 Ban)
Operation_15937: operator = TypecastBackward, inputs = [Tensor_30709], outputs = [Tensor_31009], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 24145->this
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_31009: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (31009: 3072 Ban)
Operation_15939: operator = TypecastBackward, inputs = [Tensor_30717], outputs = [Tensor_31011], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 24153->this
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_31011: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (31011: 2359296 Ban)
Operation_15941: operator = TypecastBackward, inputs = [Tensor_30721], outputs = [Tensor_31013], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 24157->this
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_31013: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (31013: 3072 Ban)
Operation_15943: operator = TypecastBackward, inputs = [Tensor_30768], outputs = [Tensor_31015], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 24205->this
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_31015: shape = [768, 3, 16, 16], stride = [768, 256, 16, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (31015: 2359296 Ban)
Operation_15945: operator = TypecastBackward, inputs = [Tensor_30772], outputs = [Tensor_31017], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = 24209->this
  - Operand: [dest_type: TYPECAST_FLOAT16]
Tensor_31017: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (31017: 3072 Ban)

Move-in tensors (source -> target):
Tensor_24 -> Tensor_27633
Tensor_25 -> Tensor_27634
Tensor_28 -> Tensor_27635
Tensor_29 -> Tensor_27636
Tensor_36 -> Tensor_27637
Tensor_37 -> Tensor_27638
Tensor_44 -> Tensor_27639
Tensor_45 -> Tensor_27640
Tensor_64 -> Tensor_27641
Tensor_65 -> Tensor_27642
Tensor_68 -> Tensor_27643
Tensor_69 -> Tensor_27644
Tensor_76 -> Tensor_27645
Tensor_77 -> Tensor_27646
Tensor_84 -> Tensor_27647
Tensor_85 -> Tensor_27648
Tensor_104 -> Tensor_27649
Tensor_105 -> Tensor_27650
Tensor_108 -> Tensor_27651
Tensor_109 -> Tensor_27652
Tensor_116 -> Tensor_27653
Tensor_117 -> Tensor_27654
Tensor_124 -> Tensor_27655
Tensor_125 -> Tensor_27656
Tensor_144 -> Tensor_27657
Tensor_145 -> Tensor_27658
Tensor_148 -> Tensor_27659
Tensor_149 -> Tensor_27660
Tensor_156 -> Tensor_27661
Tensor_157 -> Tensor_27662
Tensor_164 -> Tensor_27663
Tensor_165 -> Tensor_27664
Tensor_184 -> Tensor_27665
Tensor_185 -> Tensor_27666
Tensor_188 -> Tensor_27667
Tensor_189 -> Tensor_27668
Tensor_196 -> Tensor_27669
Tensor_197 -> Tensor_27670
Tensor_204 -> Tensor_27671
Tensor_205 -> Tensor_27672
Tensor_224 -> Tensor_27673
Tensor_225 -> Tensor_27674
Tensor_228 -> Tensor_27675
Tensor_229 -> Tensor_27676
Tensor_236 -> Tensor_27677
Tensor_237 -> Tensor_27678
Tensor_244 -> Tensor_27679
Tensor_245 -> Tensor_27680
Tensor_264 -> Tensor_27681
Tensor_265 -> Tensor_27682
Tensor_268 -> Tensor_27683
Tensor_269 -> Tensor_27684
Tensor_276 -> Tensor_27685
Tensor_277 -> Tensor_27686
Tensor_284 -> Tensor_27687
Tensor_285 -> Tensor_27688
Tensor_304 -> Tensor_27689
Tensor_305 -> Tensor_27690
Tensor_308 -> Tensor_27691
Tensor_309 -> Tensor_27692
Tensor_316 -> Tensor_27693
Tensor_317 -> Tensor_27694
Tensor_324 -> Tensor_27695
Tensor_325 -> Tensor_27696
Tensor_344 -> Tensor_27697
Tensor_345 -> Tensor_27698
Tensor_348 -> Tensor_27699
Tensor_349 -> Tensor_27700
Tensor_356 -> Tensor_27701
Tensor_357 -> Tensor_27702
Tensor_364 -> Tensor_27703
Tensor_365 -> Tensor_27704
Tensor_384 -> Tensor_27705
Tensor_385 -> Tensor_27706
Tensor_388 -> Tensor_27707
Tensor_389 -> Tensor_27708
Tensor_396 -> Tensor_27709
Tensor_397 -> Tensor_27710
Tensor_404 -> Tensor_27711
Tensor_405 -> Tensor_27712
Tensor_424 -> Tensor_27713
Tensor_425 -> Tensor_27714
Tensor_428 -> Tensor_27715
Tensor_429 -> Tensor_27716
Tensor_436 -> Tensor_27717
Tensor_437 -> Tensor_27718
Tensor_444 -> Tensor_27719
Tensor_445 -> Tensor_27720
Tensor_464 -> Tensor_27721
Tensor_465 -> Tensor_27722
Tensor_468 -> Tensor_27723
Tensor_469 -> Tensor_27724
Tensor_476 -> Tensor_27725
Tensor_477 -> Tensor_27726
Tensor_484 -> Tensor_27727
Tensor_485 -> Tensor_27728
Tensor_488 -> Tensor_27729
Tensor_489 -> Tensor_27730
Tensor_531 -> Tensor_27731
Tensor_533 -> Tensor_27732
Tensor_543 -> Tensor_27733
Tensor_545 -> Tensor_27734
Tensor_558 -> Tensor_27735
Tensor_564 -> Tensor_27736
Tensor_572 -> Tensor_27737
Tensor_574 -> Tensor_27738
Tensor_576 -> Tensor_27739
Tensor_577 -> Tensor_27740
Tensor_578 -> Tensor_27741
Tensor_580 -> Tensor_27742
Tensor_582 -> Tensor_27743
Tensor_588 -> Tensor_27744
Tensor_590 -> Tensor_27745
Tensor_596 -> Tensor_27746
Tensor_598 -> Tensor_27747
Tensor_620 -> Tensor_27748
Tensor_628 -> Tensor_27749
Tensor_630 -> Tensor_27750
Tensor_640 -> Tensor_27751
Tensor_648 -> Tensor_27752
Tensor_650 -> Tensor_27753
Tensor_662 -> Tensor_27754
Tensor_664 -> Tensor_27755
Tensor_665 -> Tensor_27756
Tensor_666 -> Tensor_27757
Tensor_668 -> Tensor_27758
Tensor_670 -> Tensor_27759
Tensor_684 -> Tensor_27760
Tensor_686 -> Tensor_27761
Tensor_687 -> Tensor_27762
Tensor_688 -> Tensor_27763
Tensor_692 -> Tensor_27764
Tensor_694 -> Tensor_27765
Tensor_700 -> Tensor_27766
Tensor_702 -> Tensor_27767
Tensor_706 -> Tensor_27768
Tensor_708 -> Tensor_27769
Tensor_709 -> Tensor_27770
Tensor_710 -> Tensor_27771
Tensor_712 -> Tensor_27772
Tensor_714 -> Tensor_27773
Tensor_726 -> Tensor_27774
Tensor_728 -> Tensor_27775
Tensor_729 -> Tensor_27776
Tensor_730 -> Tensor_27777
Tensor_732 -> Tensor_27778
Tensor_734 -> Tensor_27779
Tensor_740 -> Tensor_27780
Tensor_742 -> Tensor_27781
Tensor_748 -> Tensor_27782
Tensor_750 -> Tensor_27783
Tensor_772 -> Tensor_27784
Tensor_780 -> Tensor_27785
Tensor_782 -> Tensor_27786
Tensor_792 -> Tensor_27787
Tensor_800 -> Tensor_27788
Tensor_802 -> Tensor_27789
Tensor_814 -> Tensor_27790
Tensor_816 -> Tensor_27791
Tensor_817 -> Tensor_27792
Tensor_818 -> Tensor_27793
Tensor_820 -> Tensor_27794
Tensor_822 -> Tensor_27795
Tensor_837 -> Tensor_27796
Tensor_839 -> Tensor_27797
Tensor_847 -> Tensor_27798
Tensor_849 -> Tensor_27799
Tensor_850 -> Tensor_27800
Tensor_851 -> Tensor_27801
Tensor_855 -> Tensor_27802
Tensor_857 -> Tensor_27803
Tensor_863 -> Tensor_27804
Tensor_865 -> Tensor_27805
Tensor_869 -> Tensor_27806
Tensor_871 -> Tensor_27807
Tensor_872 -> Tensor_27808
Tensor_873 -> Tensor_27809
Tensor_875 -> Tensor_27810
Tensor_877 -> Tensor_27811
Tensor_890 -> Tensor_27812
Tensor_892 -> Tensor_27813
Tensor_900 -> Tensor_27814
Tensor_902 -> Tensor_27815
Tensor_903 -> Tensor_27816
Tensor_904 -> Tensor_27817
Tensor_906 -> Tensor_27818
Tensor_908 -> Tensor_27819
Tensor_914 -> Tensor_27820
Tensor_916 -> Tensor_27821
Tensor_922 -> Tensor_27822
Tensor_924 -> Tensor_27823
Tensor_946 -> Tensor_27824
Tensor_954 -> Tensor_27825
Tensor_956 -> Tensor_27826
Tensor_966 -> Tensor_27827
Tensor_974 -> Tensor_27828
Tensor_976 -> Tensor_27829
Tensor_988 -> Tensor_27830
Tensor_990 -> Tensor_27831
Tensor_991 -> Tensor_27832
Tensor_992 -> Tensor_27833
Tensor_994 -> Tensor_27834
Tensor_996 -> Tensor_27835
Tensor_1011 -> Tensor_27836
Tensor_1013 -> Tensor_27837
Tensor_1021 -> Tensor_27838
Tensor_1023 -> Tensor_27839
Tensor_1024 -> Tensor_27840
Tensor_1025 -> Tensor_27841
Tensor_1029 -> Tensor_27842
Tensor_1031 -> Tensor_27843
Tensor_1037 -> Tensor_27844
Tensor_1039 -> Tensor_27845
Tensor_1043 -> Tensor_27846
Tensor_1045 -> Tensor_27847
Tensor_1046 -> Tensor_27848
Tensor_1047 -> Tensor_27849
Tensor_1049 -> Tensor_27850
Tensor_1051 -> Tensor_27851
Tensor_1064 -> Tensor_27852
Tensor_1066 -> Tensor_27853
Tensor_1074 -> Tensor_27854
Tensor_1076 -> Tensor_27855
Tensor_1077 -> Tensor_27856
Tensor_1078 -> Tensor_27857
Tensor_1080 -> Tensor_27858
Tensor_1082 -> Tensor_27859
Tensor_1088 -> Tensor_27860
Tensor_1090 -> Tensor_27861
Tensor_1096 -> Tensor_27862
Tensor_1098 -> Tensor_27863
Tensor_1120 -> Tensor_27864
Tensor_1128 -> Tensor_27865
Tensor_1130 -> Tensor_27866
Tensor_1140 -> Tensor_27867
Tensor_1148 -> Tensor_27868
Tensor_1150 -> Tensor_27869
Tensor_1162 -> Tensor_27870
Tensor_1164 -> Tensor_27871
Tensor_1165 -> Tensor_27872
Tensor_1166 -> Tensor_27873
Tensor_1168 -> Tensor_27874
Tensor_1170 -> Tensor_27875
Tensor_1185 -> Tensor_27876
Tensor_1187 -> Tensor_27877
Tensor_1195 -> Tensor_27878
Tensor_1197 -> Tensor_27879
Tensor_1198 -> Tensor_27880
Tensor_1199 -> Tensor_27881
Tensor_1203 -> Tensor_27882
Tensor_1205 -> Tensor_27883
Tensor_1211 -> Tensor_27884
Tensor_1213 -> Tensor_27885
Tensor_1217 -> Tensor_27886
Tensor_1219 -> Tensor_27887
Tensor_1220 -> Tensor_27888
Tensor_1221 -> Tensor_27889
Tensor_1223 -> Tensor_27890
Tensor_1225 -> Tensor_27891
Tensor_1238 -> Tensor_27892
Tensor_1240 -> Tensor_27893
Tensor_1248 -> Tensor_27894
Tensor_1250 -> Tensor_27895
Tensor_1251 -> Tensor_27896
Tensor_1252 -> Tensor_27897
Tensor_1254 -> Tensor_27898
Tensor_1256 -> Tensor_27899
Tensor_1262 -> Tensor_27900
Tensor_1264 -> Tensor_27901
Tensor_1270 -> Tensor_27902
Tensor_1272 -> Tensor_27903
Tensor_1294 -> Tensor_27904
Tensor_1302 -> Tensor_27905
Tensor_1304 -> Tensor_27906
Tensor_1314 -> Tensor_27907
Tensor_1322 -> Tensor_27908
Tensor_1324 -> Tensor_27909
Tensor_1336 -> Tensor_27910
Tensor_1338 -> Tensor_27911
Tensor_1339 -> Tensor_27912
Tensor_1340 -> Tensor_27913
Tensor_1342 -> Tensor_27914
Tensor_1344 -> Tensor_27915
Tensor_1359 -> Tensor_27916
Tensor_1361 -> Tensor_27917
Tensor_1369 -> Tensor_27918
Tensor_1371 -> Tensor_27919
Tensor_1372 -> Tensor_27920
Tensor_1373 -> Tensor_27921
Tensor_1377 -> Tensor_27922
Tensor_1379 -> Tensor_27923
Tensor_1385 -> Tensor_27924
Tensor_1387 -> Tensor_27925
Tensor_1391 -> Tensor_27926
Tensor_1393 -> Tensor_27927
Tensor_1394 -> Tensor_27928
Tensor_1395 -> Tensor_27929
Tensor_1397 -> Tensor_27930
Tensor_1399 -> Tensor_27931
Tensor_1412 -> Tensor_27932
Tensor_1414 -> Tensor_27933
Tensor_1422 -> Tensor_27934
Tensor_1424 -> Tensor_27935
Tensor_1425 -> Tensor_27936
Tensor_1426 -> Tensor_27937
Tensor_1428 -> Tensor_27938
Tensor_1430 -> Tensor_27939
Tensor_1436 -> Tensor_27940
Tensor_1438 -> Tensor_27941
Tensor_1444 -> Tensor_27942
Tensor_1446 -> Tensor_27943
Tensor_1468 -> Tensor_27944
Tensor_1476 -> Tensor_27945
Tensor_1478 -> Tensor_27946
Tensor_1488 -> Tensor_27947
Tensor_1496 -> Tensor_27948
Tensor_1498 -> Tensor_27949
Tensor_1510 -> Tensor_27950
Tensor_1512 -> Tensor_27951
Tensor_1513 -> Tensor_27952
Tensor_1514 -> Tensor_27953
Tensor_1516 -> Tensor_27954
Tensor_1518 -> Tensor_27955
Tensor_1533 -> Tensor_27956
Tensor_1535 -> Tensor_27957
Tensor_1543 -> Tensor_27958
Tensor_1545 -> Tensor_27959
Tensor_1546 -> Tensor_27960
Tensor_1547 -> Tensor_27961
Tensor_1551 -> Tensor_27962
Tensor_1553 -> Tensor_27963
Tensor_1559 -> Tensor_27964
Tensor_1561 -> Tensor_27965
Tensor_1565 -> Tensor_27966
Tensor_1567 -> Tensor_27967
Tensor_1568 -> Tensor_27968
Tensor_1569 -> Tensor_27969
Tensor_1571 -> Tensor_27970
Tensor_1573 -> Tensor_27971
Tensor_1586 -> Tensor_27972
Tensor_1588 -> Tensor_27973
Tensor_1596 -> Tensor_27974
Tensor_1598 -> Tensor_27975
Tensor_1599 -> Tensor_27976
Tensor_1600 -> Tensor_27977
Tensor_1602 -> Tensor_27978
Tensor_1604 -> Tensor_27979
Tensor_1610 -> Tensor_27980
Tensor_1612 -> Tensor_27981
Tensor_1618 -> Tensor_27982
Tensor_1620 -> Tensor_27983
Tensor_1642 -> Tensor_27984
Tensor_1650 -> Tensor_27985
Tensor_1652 -> Tensor_27986
Tensor_1662 -> Tensor_27987
Tensor_1670 -> Tensor_27988
Tensor_1672 -> Tensor_27989
Tensor_1684 -> Tensor_27990
Tensor_1686 -> Tensor_27991
Tensor_1687 -> Tensor_27992
Tensor_1688 -> Tensor_27993
Tensor_1690 -> Tensor_27994
Tensor_1692 -> Tensor_27995
Tensor_1707 -> Tensor_27996
Tensor_1709 -> Tensor_27997
Tensor_1717 -> Tensor_27998
Tensor_1719 -> Tensor_27999
Tensor_1720 -> Tensor_28000
Tensor_1721 -> Tensor_28001
Tensor_1725 -> Tensor_28002
Tensor_1727 -> Tensor_28003
Tensor_1733 -> Tensor_28004
Tensor_1735 -> Tensor_28005
Tensor_1739 -> Tensor_28006
Tensor_1741 -> Tensor_28007
Tensor_1742 -> Tensor_28008
Tensor_1743 -> Tensor_28009
Tensor_1745 -> Tensor_28010
Tensor_1747 -> Tensor_28011
Tensor_1760 -> Tensor_28012
Tensor_1762 -> Tensor_28013
Tensor_1770 -> Tensor_28014
Tensor_1772 -> Tensor_28015
Tensor_1773 -> Tensor_28016
Tensor_1774 -> Tensor_28017
Tensor_1776 -> Tensor_28018
Tensor_1778 -> Tensor_28019
Tensor_1784 -> Tensor_28020
Tensor_1786 -> Tensor_28021
Tensor_1792 -> Tensor_28022
Tensor_1794 -> Tensor_28023
Tensor_1816 -> Tensor_28024
Tensor_1824 -> Tensor_28025
Tensor_1826 -> Tensor_28026
Tensor_1836 -> Tensor_28027
Tensor_1844 -> Tensor_28028
Tensor_1846 -> Tensor_28029
Tensor_1858 -> Tensor_28030
Tensor_1860 -> Tensor_28031
Tensor_1861 -> Tensor_28032
Tensor_1862 -> Tensor_28033
Tensor_1864 -> Tensor_28034
Tensor_1866 -> Tensor_28035
Tensor_1881 -> Tensor_28036
Tensor_1883 -> Tensor_28037
Tensor_1891 -> Tensor_28038
Tensor_1893 -> Tensor_28039
Tensor_1894 -> Tensor_28040
Tensor_1895 -> Tensor_28041
Tensor_1899 -> Tensor_28042
Tensor_1901 -> Tensor_28043
Tensor_1907 -> Tensor_28044
Tensor_1909 -> Tensor_28045
Tensor_1913 -> Tensor_28046
Tensor_1915 -> Tensor_28047
Tensor_1916 -> Tensor_28048
Tensor_1917 -> Tensor_28049
Tensor_1919 -> Tensor_28050
Tensor_1921 -> Tensor_28051
Tensor_1934 -> Tensor_28052
Tensor_1936 -> Tensor_28053
Tensor_1944 -> Tensor_28054
Tensor_1946 -> Tensor_28055
Tensor_1947 -> Tensor_28056
Tensor_1948 -> Tensor_28057
Tensor_1950 -> Tensor_28058
Tensor_1952 -> Tensor_28059
Tensor_1958 -> Tensor_28060
Tensor_1960 -> Tensor_28061
Tensor_1966 -> Tensor_28062
Tensor_1968 -> Tensor_28063
Tensor_1990 -> Tensor_28064
Tensor_1998 -> Tensor_28065
Tensor_2000 -> Tensor_28066
Tensor_2010 -> Tensor_28067
Tensor_2018 -> Tensor_28068
Tensor_2020 -> Tensor_28069
Tensor_2032 -> Tensor_28070
Tensor_2034 -> Tensor_28071
Tensor_2035 -> Tensor_28072
Tensor_2036 -> Tensor_28073
Tensor_2038 -> Tensor_28074
Tensor_2040 -> Tensor_28075
Tensor_2055 -> Tensor_28076
Tensor_2057 -> Tensor_28077
Tensor_2065 -> Tensor_28078
Tensor_2067 -> Tensor_28079
Tensor_2068 -> Tensor_28080
Tensor_2069 -> Tensor_28081
Tensor_2073 -> Tensor_28082
Tensor_2075 -> Tensor_28083
Tensor_2081 -> Tensor_28084
Tensor_2083 -> Tensor_28085
Tensor_2087 -> Tensor_28086
Tensor_2089 -> Tensor_28087
Tensor_2090 -> Tensor_28088
Tensor_2091 -> Tensor_28089
Tensor_2093 -> Tensor_28090
Tensor_2095 -> Tensor_28091
Tensor_2108 -> Tensor_28092
Tensor_2110 -> Tensor_28093
Tensor_2118 -> Tensor_28094
Tensor_2120 -> Tensor_28095
Tensor_2121 -> Tensor_28096
Tensor_2122 -> Tensor_28097
Tensor_2124 -> Tensor_28098
Tensor_2126 -> Tensor_28099
Tensor_2132 -> Tensor_28100
Tensor_2134 -> Tensor_28101
Tensor_2140 -> Tensor_28102
Tensor_2142 -> Tensor_28103
Tensor_2164 -> Tensor_28104
Tensor_2172 -> Tensor_28105
Tensor_2174 -> Tensor_28106
Tensor_2184 -> Tensor_28107
Tensor_2192 -> Tensor_28108
Tensor_2194 -> Tensor_28109
Tensor_2206 -> Tensor_28110
Tensor_2208 -> Tensor_28111
Tensor_2209 -> Tensor_28112
Tensor_2210 -> Tensor_28113
Tensor_2212 -> Tensor_28114
Tensor_2214 -> Tensor_28115
Tensor_2229 -> Tensor_28116
Tensor_2231 -> Tensor_28117
Tensor_2239 -> Tensor_28118
Tensor_2241 -> Tensor_28119
Tensor_2242 -> Tensor_28120
Tensor_2243 -> Tensor_28121
Tensor_2247 -> Tensor_28122
Tensor_2249 -> Tensor_28123
Tensor_2255 -> Tensor_28124
Tensor_2257 -> Tensor_28125
Tensor_2261 -> Tensor_28126
Tensor_2263 -> Tensor_28127
Tensor_2264 -> Tensor_28128
Tensor_2265 -> Tensor_28129
Tensor_2267 -> Tensor_28130
Tensor_2269 -> Tensor_28131
Tensor_2282 -> Tensor_28132
Tensor_2284 -> Tensor_28133
Tensor_2292 -> Tensor_28134
Tensor_2294 -> Tensor_28135
Tensor_2295 -> Tensor_28136
Tensor_2296 -> Tensor_28137
Tensor_2298 -> Tensor_28138
Tensor_2300 -> Tensor_28139
Tensor_2306 -> Tensor_28140
Tensor_2308 -> Tensor_28141
Tensor_2314 -> Tensor_28142
Tensor_2316 -> Tensor_28143
Tensor_2338 -> Tensor_28144
Tensor_2346 -> Tensor_28145
Tensor_2348 -> Tensor_28146
Tensor_2358 -> Tensor_28147
Tensor_2366 -> Tensor_28148
Tensor_2368 -> Tensor_28149
Tensor_2380 -> Tensor_28150
Tensor_2382 -> Tensor_28151
Tensor_2383 -> Tensor_28152
Tensor_2384 -> Tensor_28153
Tensor_2386 -> Tensor_28154
Tensor_2388 -> Tensor_28155
Tensor_2403 -> Tensor_28156
Tensor_2405 -> Tensor_28157
Tensor_2413 -> Tensor_28158
Tensor_2415 -> Tensor_28159
Tensor_2416 -> Tensor_28160
Tensor_2417 -> Tensor_28161
Tensor_2421 -> Tensor_28162
Tensor_2423 -> Tensor_28163
Tensor_2429 -> Tensor_28164
Tensor_2431 -> Tensor_28165
Tensor_2435 -> Tensor_28166
Tensor_2437 -> Tensor_28167
Tensor_2438 -> Tensor_28168
Tensor_2439 -> Tensor_28169
Tensor_2441 -> Tensor_28170
Tensor_2443 -> Tensor_28171
Tensor_2456 -> Tensor_28172
Tensor_2458 -> Tensor_28173
Tensor_2466 -> Tensor_28174
Tensor_2468 -> Tensor_28175
Tensor_2469 -> Tensor_28176
Tensor_2470 -> Tensor_28177
Tensor_2472 -> Tensor_28178
Tensor_2474 -> Tensor_28179
Tensor_2480 -> Tensor_28180
Tensor_2482 -> Tensor_28181
Tensor_2488 -> Tensor_28182
Tensor_2490 -> Tensor_28183
Tensor_2512 -> Tensor_28184
Tensor_2520 -> Tensor_28185
Tensor_2522 -> Tensor_28186
Tensor_2532 -> Tensor_28187
Tensor_2540 -> Tensor_28188
Tensor_2542 -> Tensor_28189
Tensor_2554 -> Tensor_28190
Tensor_2556 -> Tensor_28191
Tensor_2557 -> Tensor_28192
Tensor_2558 -> Tensor_28193
Tensor_2560 -> Tensor_28194
Tensor_2562 -> Tensor_28195
Tensor_2577 -> Tensor_28196
Tensor_2579 -> Tensor_28197
Tensor_2587 -> Tensor_28198
Tensor_2589 -> Tensor_28199
Tensor_2590 -> Tensor_28200
Tensor_2591 -> Tensor_28201
Tensor_2595 -> Tensor_28202
Tensor_2597 -> Tensor_28203
Tensor_2603 -> Tensor_28204
Tensor_2605 -> Tensor_28205
Tensor_2609 -> Tensor_28206
Tensor_2611 -> Tensor_28207
Tensor_2612 -> Tensor_28208
Tensor_2613 -> Tensor_28209
Tensor_2615 -> Tensor_28210
Tensor_2617 -> Tensor_28211
Tensor_2630 -> Tensor_28212
Tensor_2632 -> Tensor_28213
Tensor_2644 -> Tensor_28214
Tensor_2646 -> Tensor_28215
Tensor_2647 -> Tensor_28216
Tensor_2648 -> Tensor_28217
Tensor_2650 -> Tensor_28218
Tensor_2652 -> Tensor_28219
Tensor_2658 -> Tensor_28220
Tensor_2662 -> Tensor_28221
Tensor_2668 -> Tensor_28222
Tensor_13784 -> Tensor_28225
Tensor_13791 -> Tensor_28234

Move-out tensors (source <- target):
Tensor_24 <- Tensor_27633
Tensor_25 <- Tensor_27634
Tensor_28 <- Tensor_27635
Tensor_29 <- Tensor_27636
Tensor_36 <- Tensor_27637
Tensor_37 <- Tensor_27638
Tensor_44 <- Tensor_27639
Tensor_45 <- Tensor_27640
Tensor_64 <- Tensor_27641
Tensor_65 <- Tensor_27642
Tensor_68 <- Tensor_27643
Tensor_69 <- Tensor_27644
Tensor_76 <- Tensor_27645
Tensor_77 <- Tensor_27646
Tensor_84 <- Tensor_27647
Tensor_85 <- Tensor_27648
Tensor_104 <- Tensor_27649
Tensor_105 <- Tensor_27650
Tensor_108 <- Tensor_27651
Tensor_109 <- Tensor_27652
Tensor_116 <- Tensor_27653
Tensor_117 <- Tensor_27654
Tensor_124 <- Tensor_27655
Tensor_125 <- Tensor_27656
Tensor_144 <- Tensor_27657
Tensor_145 <- Tensor_27658
Tensor_148 <- Tensor_27659
Tensor_149 <- Tensor_27660
Tensor_156 <- Tensor_27661
Tensor_157 <- Tensor_27662
Tensor_164 <- Tensor_27663
Tensor_165 <- Tensor_27664
Tensor_184 <- Tensor_27665
Tensor_185 <- Tensor_27666
Tensor_188 <- Tensor_27667
Tensor_189 <- Tensor_27668
Tensor_196 <- Tensor_27669
Tensor_197 <- Tensor_27670
Tensor_204 <- Tensor_27671
Tensor_205 <- Tensor_27672
Tensor_224 <- Tensor_27673
Tensor_225 <- Tensor_27674
Tensor_228 <- Tensor_27675
Tensor_229 <- Tensor_27676
Tensor_236 <- Tensor_27677
Tensor_237 <- Tensor_27678
Tensor_244 <- Tensor_27679
Tensor_245 <- Tensor_27680
Tensor_264 <- Tensor_27681
Tensor_265 <- Tensor_27682
Tensor_268 <- Tensor_27683
Tensor_269 <- Tensor_27684
Tensor_276 <- Tensor_27685
Tensor_277 <- Tensor_27686
Tensor_284 <- Tensor_27687
Tensor_285 <- Tensor_27688
Tensor_304 <- Tensor_27689
Tensor_305 <- Tensor_27690
Tensor_308 <- Tensor_27691
Tensor_309 <- Tensor_27692
Tensor_316 <- Tensor_27693
Tensor_317 <- Tensor_27694
Tensor_324 <- Tensor_27695
Tensor_325 <- Tensor_27696
Tensor_344 <- Tensor_27697
Tensor_345 <- Tensor_27698
Tensor_348 <- Tensor_27699
Tensor_349 <- Tensor_27700
Tensor_356 <- Tensor_27701
Tensor_357 <- Tensor_27702
Tensor_364 <- Tensor_27703
Tensor_365 <- Tensor_27704
Tensor_384 <- Tensor_27705
Tensor_385 <- Tensor_27706
Tensor_388 <- Tensor_27707
Tensor_389 <- Tensor_27708
Tensor_396 <- Tensor_27709
Tensor_397 <- Tensor_27710
Tensor_404 <- Tensor_27711
Tensor_405 <- Tensor_27712
Tensor_424 <- Tensor_27713
Tensor_425 <- Tensor_27714
Tensor_428 <- Tensor_27715
Tensor_429 <- Tensor_27716
Tensor_436 <- Tensor_27717
Tensor_437 <- Tensor_27718
Tensor_444 <- Tensor_27719
Tensor_445 <- Tensor_27720
Tensor_464 <- Tensor_27721
Tensor_465 <- Tensor_27722
Tensor_468 <- Tensor_27723
Tensor_469 <- Tensor_27724
Tensor_476 <- Tensor_27725
Tensor_477 <- Tensor_27726
Tensor_484 <- Tensor_27727
Tensor_485 <- Tensor_27728
Tensor_488 <- Tensor_27729
Tensor_489 <- Tensor_27730
Tensor_13783 <- Tensor_28224
Tensor_13786 <- Tensor_28227
Tensor_13790 <- Tensor_28233
Tensor_13791 <- Tensor_28234
Tensor_13820 <- Tensor_28281
Tensor_13821 <- Tensor_28285
Tensor_13847 <- Tensor_28317
Tensor_13848 <- Tensor_28321
Tensor_13869 <- Tensor_28349
Tensor_13870 <- Tensor_28353
Tensor_13890 <- Tensor_28382
Tensor_13891 <- Tensor_28386
Tensor_13974 <- Tensor_28488
Tensor_13975 <- Tensor_28492
Tensor_13997 <- Tensor_28522
Tensor_13998 <- Tensor_28526
Tensor_14019 <- Tensor_28554
Tensor_14020 <- Tensor_28558
Tensor_14040 <- Tensor_28587
Tensor_14041 <- Tensor_28591
Tensor_14124 <- Tensor_28693
Tensor_14125 <- Tensor_28697
Tensor_14147 <- Tensor_28727
Tensor_14148 <- Tensor_28731
Tensor_14169 <- Tensor_28759
Tensor_14170 <- Tensor_28763
Tensor_14190 <- Tensor_28792
Tensor_14191 <- Tensor_28796
Tensor_14274 <- Tensor_28898
Tensor_14275 <- Tensor_28902
Tensor_14297 <- Tensor_28932
Tensor_14298 <- Tensor_28936
Tensor_14319 <- Tensor_28964
Tensor_14320 <- Tensor_28968
Tensor_14340 <- Tensor_28997
Tensor_14341 <- Tensor_29001
Tensor_14424 <- Tensor_29103
Tensor_14425 <- Tensor_29107
Tensor_14447 <- Tensor_29137
Tensor_14448 <- Tensor_29141
Tensor_14469 <- Tensor_29169
Tensor_14470 <- Tensor_29173
Tensor_14490 <- Tensor_29202
Tensor_14491 <- Tensor_29206
Tensor_14574 <- Tensor_29308
Tensor_14575 <- Tensor_29312
Tensor_14597 <- Tensor_29342
Tensor_14598 <- Tensor_29346
Tensor_14619 <- Tensor_29374
Tensor_14620 <- Tensor_29378
Tensor_14640 <- Tensor_29407
Tensor_14641 <- Tensor_29411
Tensor_14724 <- Tensor_29513
Tensor_14725 <- Tensor_29517
Tensor_14747 <- Tensor_29547
Tensor_14748 <- Tensor_29551
Tensor_14769 <- Tensor_29579
Tensor_14770 <- Tensor_29583
Tensor_14790 <- Tensor_29612
Tensor_14791 <- Tensor_29616
Tensor_14874 <- Tensor_29718
Tensor_14875 <- Tensor_29722
Tensor_14897 <- Tensor_29752
Tensor_14898 <- Tensor_29756
Tensor_14919 <- Tensor_29784
Tensor_14920 <- Tensor_29788
Tensor_14940 <- Tensor_29817
Tensor_14941 <- Tensor_29821
Tensor_15024 <- Tensor_29923
Tensor_15025 <- Tensor_29927
Tensor_15047 <- Tensor_29957
Tensor_15048 <- Tensor_29961
Tensor_15069 <- Tensor_29989
Tensor_15070 <- Tensor_29993
Tensor_15090 <- Tensor_30022
Tensor_15091 <- Tensor_30026
Tensor_15174 <- Tensor_30128
Tensor_15175 <- Tensor_30132
Tensor_15197 <- Tensor_30162
Tensor_15198 <- Tensor_30166
Tensor_15219 <- Tensor_30194
Tensor_15220 <- Tensor_30198
Tensor_15240 <- Tensor_30227
Tensor_15241 <- Tensor_30231
Tensor_15324 <- Tensor_30333
Tensor_15325 <- Tensor_30337
Tensor_15347 <- Tensor_30367
Tensor_15348 <- Tensor_30371
Tensor_15369 <- Tensor_30399
Tensor_15370 <- Tensor_30403
Tensor_15390 <- Tensor_30432
Tensor_15391 <- Tensor_30436
Tensor_15474 <- Tensor_30538
Tensor_15475 <- Tensor_30542
Tensor_15493 <- Tensor_30568
Tensor_15494 <- Tensor_30572
Tensor_15515 <- Tensor_30600
Tensor_15516 <- Tensor_30604
Tensor_15532 <- Tensor_30629
Tensor_15533 <- Tensor_30633
Tensor_15616 <- Tensor_30735
Tensor_15617 <- Tensor_30739
Tensor_15639 <- Tensor_30756
Tensor_15641 <- Tensor_30758
Tensor_15652 <- Tensor_30773
Tensor_15654 <- Tensor_30774
Tensor_15656 <- Tensor_30775
Tensor_15658 <- Tensor_30776
Tensor_15660 <- Tensor_30777
Tensor_15662 <- Tensor_30778
Tensor_15664 <- Tensor_30779
Tensor_15666 <- Tensor_30780
Tensor_15668 <- Tensor_30781
Tensor_15670 <- Tensor_30782
Tensor_15672 <- Tensor_30783
Tensor_15674 <- Tensor_30784
Tensor_15676 <- Tensor_30785
Tensor_15678 <- Tensor_30786
Tensor_15680 <- Tensor_30787
Tensor_15682 <- Tensor_30788
Tensor_15684 <- Tensor_30789
Tensor_15686 <- Tensor_30790
Tensor_15688 <- Tensor_30791
Tensor_15690 <- Tensor_30792
Tensor_15692 <- Tensor_30793
Tensor_15694 <- Tensor_30794
Tensor_15696 <- Tensor_30795
Tensor_15698 <- Tensor_30796
Tensor_15700 <- Tensor_30797
Tensor_15702 <- Tensor_30798
Tensor_15704 <- Tensor_30799
Tensor_15706 <- Tensor_30800
Tensor_15708 <- Tensor_30801
Tensor_15710 <- Tensor_30802
Tensor_15712 <- Tensor_30803
Tensor_15714 <- Tensor_30804
Tensor_15716 <- Tensor_30805
Tensor_15718 <- Tensor_30806
Tensor_15720 <- Tensor_30807
Tensor_15722 <- Tensor_30808
Tensor_15724 <- Tensor_30809
Tensor_15726 <- Tensor_30810
Tensor_15728 <- Tensor_30811
Tensor_15730 <- Tensor_30812
Tensor_15732 <- Tensor_30813
Tensor_15734 <- Tensor_30814
Tensor_15736 <- Tensor_30815
Tensor_15738 <- Tensor_30816
Tensor_15740 <- Tensor_30817
Tensor_15742 <- Tensor_30818
Tensor_15744 <- Tensor_30819
Tensor_15746 <- Tensor_30820
Tensor_15748 <- Tensor_30821
Tensor_15750 <- Tensor_30822
Tensor_15752 <- Tensor_30823
Tensor_15754 <- Tensor_30825
Tensor_15756 <- Tensor_30827
Tensor_15758 <- Tensor_30829
Tensor_15760 <- Tensor_30831
Tensor_15762 <- Tensor_30833
Tensor_15764 <- Tensor_30835
Tensor_15766 <- Tensor_30837
Tensor_15768 <- Tensor_30839
Tensor_15770 <- Tensor_30841
Tensor_15772 <- Tensor_30843
Tensor_15774 <- Tensor_30845
Tensor_15776 <- Tensor_30847
Tensor_15778 <- Tensor_30849
Tensor_15780 <- Tensor_30851
Tensor_15782 <- Tensor_30853
Tensor_15784 <- Tensor_30855
Tensor_15786 <- Tensor_30857
Tensor_15788 <- Tensor_30859
Tensor_15790 <- Tensor_30861
Tensor_15792 <- Tensor_30863
Tensor_15794 <- Tensor_30865
Tensor_15796 <- Tensor_30867
Tensor_15798 <- Tensor_30869
Tensor_15800 <- Tensor_30871
Tensor_15802 <- Tensor_30873
Tensor_15804 <- Tensor_30875
Tensor_15806 <- Tensor_30877
Tensor_15808 <- Tensor_30879
Tensor_15810 <- Tensor_30881
Tensor_15812 <- Tensor_30883
Tensor_15814 <- Tensor_30885
Tensor_15816 <- Tensor_30887
Tensor_15818 <- Tensor_30889
Tensor_15820 <- Tensor_30891
Tensor_15822 <- Tensor_30893
Tensor_15824 <- Tensor_30895
Tensor_15826 <- Tensor_30897
Tensor_15828 <- Tensor_30899
Tensor_15830 <- Tensor_30901
Tensor_15832 <- Tensor_30903
Tensor_15834 <- Tensor_30905
Tensor_15836 <- Tensor_30907
Tensor_15838 <- Tensor_30909
Tensor_15840 <- Tensor_30911
Tensor_15842 <- Tensor_30913
Tensor_15844 <- Tensor_30915
Tensor_15846 <- Tensor_30917
Tensor_15848 <- Tensor_30919
Tensor_15850 <- Tensor_30921
Tensor_15852 <- Tensor_30923
Tensor_15854 <- Tensor_30925
Tensor_15856 <- Tensor_30927
Tensor_15858 <- Tensor_30929
Tensor_15860 <- Tensor_30931
Tensor_15862 <- Tensor_30933
Tensor_15864 <- Tensor_30935
Tensor_15866 <- Tensor_30937
Tensor_15868 <- Tensor_30939
Tensor_15870 <- Tensor_30941
Tensor_15872 <- Tensor_30943
Tensor_15874 <- Tensor_30945
Tensor_15876 <- Tensor_30947
Tensor_15878 <- Tensor_30949
Tensor_15880 <- Tensor_30951
Tensor_15882 <- Tensor_30953
Tensor_15884 <- Tensor_30955
Tensor_15886 <- Tensor_30957
Tensor_15888 <- Tensor_30959
Tensor_15890 <- Tensor_30961
Tensor_15892 <- Tensor_30963
Tensor_15894 <- Tensor_30965
Tensor_15896 <- Tensor_30967
Tensor_15898 <- Tensor_30969
Tensor_15900 <- Tensor_30971
Tensor_15902 <- Tensor_30973
Tensor_15904 <- Tensor_30975
Tensor_15906 <- Tensor_30977
Tensor_15908 <- Tensor_30979
Tensor_15910 <- Tensor_30981
Tensor_15912 <- Tensor_30983
Tensor_15914 <- Tensor_30985
Tensor_15916 <- Tensor_30987
Tensor_15918 <- Tensor_30989
Tensor_15920 <- Tensor_30991
Tensor_15922 <- Tensor_30993
Tensor_15924 <- Tensor_30995
Tensor_15926 <- Tensor_30997
Tensor_15928 <- Tensor_30999
Tensor_15930 <- Tensor_31001
Tensor_15932 <- Tensor_31003
Tensor_15934 <- Tensor_31005
Tensor_15936 <- Tensor_31007
Tensor_15938 <- Tensor_31009
Tensor_15940 <- Tensor_31011
Tensor_15942 <- Tensor_31013
Tensor_15944 <- Tensor_31015
Tensor_15946 <- Tensor_31017

== Translation ==
Source graph:
Tensor_13820: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28281: 3072 Bnn)
Tensor_13821: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28285: 3072 Bnn)
Tensor_13847: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28317: 12288 Bnn)
Tensor_13848: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28321: 12288 Bnn)
Tensor_13869: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28349: 3072 Bnn)
Tensor_13870: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28353: 3072 Bnn)
Tensor_13890: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28382: 3072 Bnn)
Tensor_13891: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28386: 3072 Bnn)
Tensor_13974: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28488: 3072 Bnn)
Tensor_13975: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28492: 3072 Bnn)
Tensor_13997: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28522: 12288 Bnn)
Tensor_13998: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28526: 12288 Bnn)
Tensor_14019: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28554: 3072 Bnn)
Tensor_14020: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28558: 3072 Bnn)
Tensor_14040: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28587: 3072 Bnn)
Tensor_14041: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28591: 3072 Bnn)
Tensor_14124: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28693: 3072 Bnn)
Tensor_14125: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28697: 3072 Bnn)
Tensor_14147: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28727: 12288 Bnn)
Tensor_14148: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28731: 12288 Bnn)
Tensor_14169: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28759: 3072 Bnn)
Tensor_14170: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28763: 3072 Bnn)
Tensor_14190: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28792: 3072 Bnn)
Tensor_14191: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28796: 3072 Bnn)
Tensor_14274: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28898: 3072 Bnn)
Tensor_14275: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28902: 3072 Bnn)
Tensor_14297: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28932: 12288 Bnn)
Tensor_14298: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28936: 12288 Bnn)
Tensor_14319: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28964: 3072 Bnn)
Tensor_14320: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28968: 3072 Bnn)
Tensor_14340: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28997: 3072 Bnn)
Tensor_14341: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29001: 3072 Bnn)
Tensor_14424: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29103: 3072 Bnn)
Tensor_14425: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29107: 3072 Bnn)
Tensor_14447: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29137: 12288 Bnn)
Tensor_14448: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29141: 12288 Bnn)
Tensor_14469: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29169: 3072 Bnn)
Tensor_14470: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29173: 3072 Bnn)
Tensor_14490: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29202: 3072 Bnn)
Tensor_14491: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29206: 3072 Bnn)
Tensor_14574: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29308: 3072 Bnn)
Tensor_14575: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29312: 3072 Bnn)
Tensor_14597: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29342: 12288 Bnn)
Tensor_14598: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29346: 12288 Bnn)
Tensor_14619: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29374: 3072 Bnn)
Tensor_14620: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29378: 3072 Bnn)
Tensor_14640: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29407: 3072 Bnn)
Tensor_14641: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29411: 3072 Bnn)
Tensor_14724: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29513: 3072 Bnn)
Tensor_14725: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29517: 3072 Bnn)
Tensor_14747: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29547: 12288 Bnn)
Tensor_14748: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29551: 12288 Bnn)
Tensor_14769: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29579: 3072 Bnn)
Tensor_14770: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29583: 3072 Bnn)
Tensor_14790: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29612: 3072 Bnn)
Tensor_14791: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29616: 3072 Bnn)
Tensor_14874: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29718: 3072 Bnn)
Tensor_14875: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29722: 3072 Bnn)
Tensor_14897: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29752: 12288 Bnn)
Tensor_14898: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29756: 12288 Bnn)
Tensor_14919: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29784: 3072 Bnn)
Tensor_14920: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29788: 3072 Bnn)
Tensor_14940: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29817: 3072 Bnn)
Tensor_14941: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29821: 3072 Bnn)
Tensor_15024: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29923: 3072 Bnn)
Tensor_15025: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29927: 3072 Bnn)
Tensor_15047: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29957: 12288 Bnn)
Tensor_15048: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29961: 12288 Bnn)
Tensor_15069: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29989: 3072 Bnn)
Tensor_15070: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29993: 3072 Bnn)
Tensor_15090: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30022: 3072 Bnn)
Tensor_15091: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30026: 3072 Bnn)
Tensor_15174: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30128: 3072 Bnn)
Tensor_15175: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30132: 3072 Bnn)
Tensor_15197: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30162: 12288 Bnn)
Tensor_15198: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30166: 12288 Bnn)
Tensor_15219: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30194: 3072 Bnn)
Tensor_15220: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30198: 3072 Bnn)
Tensor_15240: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30227: 3072 Bnn)
Tensor_15241: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30231: 3072 Bnn)
Tensor_15324: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30333: 3072 Bnn)
Tensor_15325: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30337: 3072 Bnn)
Tensor_15347: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30367: 12288 Bnn)
Tensor_15348: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30371: 12288 Bnn)
Tensor_15369: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30399: 3072 Bnn)
Tensor_15370: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30403: 3072 Bnn)
Tensor_15390: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30432: 3072 Bnn)
Tensor_15391: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30436: 3072 Bnn)
Tensor_15474: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30538: 3072 Bnn)
Tensor_15475: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30542: 3072 Bnn)
Tensor_15493: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30568: 12288 Bnn)
Tensor_15494: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30572: 12288 Bnn)
Tensor_15515: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30600: 3072 Bnn)
Tensor_15516: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30604: 3072 Bnn)
Tensor_15532: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30629: 3072 Bnn)
Tensor_15533: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30633: 3072 Bnn)
Tensor_15616: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30735: 3072 Bnn)
Tensor_15617: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30739: 3072 Bnn)
Tensor_15639: shape = [199, 768], stride = [768, 1], offset = 0, size = 152832, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (30756: 611328 Bnn)
Tensor_15641: shape = [1, 1, 768], stride = [768, 768, 1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (30758: 3072 Bnn)
Tensor_15652: shape = [1000, 768], stride = [768, 1], offset = 0, size = 768000, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30773: 3072000 Bnn)
Tensor_15654: shape = [1000], stride = [1], offset = 0, size = 1000, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30774: 4000 Bnn)
Tensor_15656: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30775: 9437184 Bnn)
Tensor_15658: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30776: 3072 Bnn)
Tensor_15660: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30777: 9437184 Bnn)
Tensor_15662: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30778: 12288 Bnn)
Tensor_15664: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30779: 2359296 Bnn)
Tensor_15666: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30780: 3072 Bnn)
Tensor_15668: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30781: 2359296 Bnn)
Tensor_15670: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30782: 3072 Bnn)
Tensor_15672: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30783: 2359296 Bnn)
Tensor_15674: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30784: 3072 Bnn)
Tensor_15676: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30785: 2359296 Bnn)
Tensor_15678: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30786: 3072 Bnn)
Tensor_15680: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30787: 9437184 Bnn)
Tensor_15682: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30788: 3072 Bnn)
Tensor_15684: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30789: 9437184 Bnn)
Tensor_15686: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30790: 12288 Bnn)
Tensor_15688: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30791: 2359296 Bnn)
Tensor_15690: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30792: 3072 Bnn)
Tensor_15692: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30793: 2359296 Bnn)
Tensor_15694: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30794: 3072 Bnn)
Tensor_15696: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30795: 2359296 Bnn)
Tensor_15698: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30796: 3072 Bnn)
Tensor_15700: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30797: 2359296 Bnn)
Tensor_15702: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30798: 3072 Bnn)
Tensor_15704: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30799: 9437184 Bnn)
Tensor_15706: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30800: 3072 Bnn)
Tensor_15708: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30801: 9437184 Bnn)
Tensor_15710: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30802: 12288 Bnn)
Tensor_15712: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30803: 2359296 Bnn)
Tensor_15714: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30804: 3072 Bnn)
Tensor_15716: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30805: 2359296 Bnn)
Tensor_15718: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30806: 3072 Bnn)
Tensor_15720: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30807: 2359296 Bnn)
Tensor_15722: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30808: 3072 Bnn)
Tensor_15724: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30809: 2359296 Bnn)
Tensor_15726: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30810: 3072 Bnn)
Tensor_15728: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30811: 9437184 Bnn)
Tensor_15730: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30812: 3072 Bnn)
Tensor_15732: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30813: 9437184 Bnn)
Tensor_15734: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30814: 12288 Bnn)
Tensor_15736: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30815: 2359296 Bnn)
Tensor_15738: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30816: 3072 Bnn)
Tensor_15740: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30817: 2359296 Bnn)
Tensor_15742: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30818: 3072 Bnn)
Tensor_15744: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30819: 2359296 Bnn)
Tensor_15746: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30820: 3072 Bnn)
Tensor_15748: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30821: 2359296 Bnn)
Tensor_15750: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30822: 3072 Bnn)
Tensor_15752: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30823: 9437184 Bnn)
Tensor_15754: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30825: 3072 Bnn)
Tensor_15756: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30827: 9437184 Bnn)
Tensor_15758: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30829: 12288 Bnn)
Tensor_15760: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30831: 2359296 Bnn)
Tensor_15762: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30833: 3072 Bnn)
Tensor_15764: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30835: 2359296 Bnn)
Tensor_15766: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30837: 3072 Bnn)
Tensor_15768: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30839: 2359296 Bnn)
Tensor_15770: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30841: 3072 Bnn)
Tensor_15772: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30843: 2359296 Bnn)
Tensor_15774: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30845: 3072 Bnn)
Tensor_15776: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30847: 9437184 Bnn)
Tensor_15778: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30849: 3072 Bnn)
Tensor_15780: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30851: 9437184 Bnn)
Tensor_15782: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30853: 12288 Bnn)
Tensor_15784: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30855: 2359296 Bnn)
Tensor_15786: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30857: 3072 Bnn)
Tensor_15788: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30859: 2359296 Bnn)
Tensor_15790: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30861: 3072 Bnn)
Tensor_15792: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30863: 2359296 Bnn)
Tensor_15794: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30865: 3072 Bnn)
Tensor_15796: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30867: 2359296 Bnn)
Tensor_15798: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30869: 3072 Bnn)
Tensor_15800: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30871: 9437184 Bnn)
Tensor_15802: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30873: 3072 Bnn)
Tensor_15804: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30875: 9437184 Bnn)
Tensor_15806: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30877: 12288 Bnn)
Tensor_15808: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30879: 2359296 Bnn)
Tensor_15810: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30881: 3072 Bnn)
Tensor_15812: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30883: 2359296 Bnn)
Tensor_15814: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30885: 3072 Bnn)
Tensor_15816: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30887: 2359296 Bnn)
Tensor_15818: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30889: 3072 Bnn)
Tensor_15820: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30891: 2359296 Bnn)
Tensor_15822: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30893: 3072 Bnn)
Tensor_15824: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30895: 9437184 Bnn)
Tensor_15826: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30897: 3072 Bnn)
Tensor_15828: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30899: 9437184 Bnn)
Tensor_15830: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30901: 12288 Bnn)
Tensor_15832: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30903: 2359296 Bnn)
Tensor_15834: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30905: 3072 Bnn)
Tensor_15836: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30907: 2359296 Bnn)
Tensor_15838: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30909: 3072 Bnn)
Tensor_15840: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30911: 2359296 Bnn)
Tensor_15842: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30913: 3072 Bnn)
Tensor_15844: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30915: 2359296 Bnn)
Tensor_15846: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30917: 3072 Bnn)
Tensor_15848: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30919: 9437184 Bnn)
Tensor_15850: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30921: 3072 Bnn)
Tensor_15852: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30923: 9437184 Bnn)
Tensor_15854: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30925: 12288 Bnn)
Tensor_15856: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30927: 2359296 Bnn)
Tensor_15858: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30929: 3072 Bnn)
Tensor_15860: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30931: 2359296 Bnn)
Tensor_15862: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30933: 3072 Bnn)
Tensor_15864: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30935: 2359296 Bnn)
Tensor_15866: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30937: 3072 Bnn)
Tensor_15868: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30939: 2359296 Bnn)
Tensor_15870: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30941: 3072 Bnn)
Tensor_15872: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30943: 9437184 Bnn)
Tensor_15874: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30945: 3072 Bnn)
Tensor_15876: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30947: 9437184 Bnn)
Tensor_15878: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30949: 12288 Bnn)
Tensor_15880: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30951: 2359296 Bnn)
Tensor_15882: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30953: 3072 Bnn)
Tensor_15884: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30955: 2359296 Bnn)
Tensor_15886: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30957: 3072 Bnn)
Tensor_15888: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30959: 2359296 Bnn)
Tensor_15890: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30961: 3072 Bnn)
Tensor_15892: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30963: 2359296 Bnn)
Tensor_15894: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30965: 3072 Bnn)
Tensor_15896: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30967: 9437184 Bnn)
Tensor_15898: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30969: 3072 Bnn)
Tensor_15900: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30971: 9437184 Bnn)
Tensor_15902: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30973: 12288 Bnn)
Tensor_15904: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30975: 2359296 Bnn)
Tensor_15906: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30977: 3072 Bnn)
Tensor_15908: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30979: 2359296 Bnn)
Tensor_15910: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30981: 3072 Bnn)
Tensor_15912: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30983: 2359296 Bnn)
Tensor_15914: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30985: 3072 Bnn)
Tensor_15916: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30987: 2359296 Bnn)
Tensor_15918: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30989: 3072 Bnn)
Tensor_15920: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30991: 9437184 Bnn)
Tensor_15922: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30993: 3072 Bnn)
Tensor_15924: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30995: 9437184 Bnn)
Tensor_15926: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30997: 12288 Bnn)
Tensor_15928: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30999: 2359296 Bnn)
Tensor_15930: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (31001: 3072 Bnn)
Tensor_15932: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (31003: 2359296 Bnn)
Tensor_15934: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (31005: 3072 Bnn)
Tensor_15936: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (31007: 2359296 Bnn)
Tensor_15938: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (31009: 3072 Bnn)
Tensor_15940: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (31011: 2359296 Bnn)
Tensor_15942: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (31013: 3072 Bnn)
Tensor_15944: shape = [768, 3, 16, 16], stride = [768, 256, 16, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (31015: 2359296 Bnn)
Tensor_15946: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (31017: 3072 Bnn)
Operation_31024: operator = ApexAmpPackForward, inputs = [Tensor_15944, Tensor_15641, Tensor_15946, Tensor_15639, Tensor_15940, Tensor_15932, Tensor_15936, Tensor_15928, Tensor_15924, Tensor_15920, Tensor_15942, Tensor_15934, Tensor_15938, Tensor_15930, Tensor_15532, Tensor_15533, Tensor_15616, Tensor_15617, Tensor_15926, Tensor_15922, Tensor_15493, Tensor_15494, Tensor_15515, Tensor_15516, Tensor_15916, Tensor_15908, Tensor_15912, Tensor_15904, Tensor_15900, Tensor_15896, Tensor_15918, Tensor_15910, Tensor_15914, Tensor_15906, Tensor_15390, Tensor_15391, Tensor_15474, Tensor_15475, Tensor_15902, Tensor_15898, Tensor_15347, Tensor_15348, Tensor_15369, Tensor_15370, Tensor_15892, Tensor_15884, Tensor_15888, Tensor_15880, Tensor_15876, Tensor_15872, Tensor_15894, Tensor_15886, Tensor_15890, Tensor_15882, Tensor_15240, Tensor_15241, Tensor_15324, Tensor_15325, Tensor_15878, Tensor_15874, Tensor_15197, Tensor_15198, Tensor_15219, Tensor_15220, Tensor_15868, Tensor_15860, Tensor_15864, Tensor_15856, Tensor_15852, Tensor_15848, Tensor_15870, Tensor_15862, Tensor_15866, Tensor_15858, Tensor_15090, Tensor_15091, Tensor_15174, Tensor_15175, Tensor_15854, Tensor_15850, Tensor_15047, Tensor_15048, Tensor_15069, Tensor_15070, Tensor_15844, Tensor_15836, Tensor_15840, Tensor_15832, Tensor_15828, Tensor_15824, Tensor_15846, Tensor_15838, Tensor_15842, Tensor_15834, Tensor_14940, Tensor_14941, Tensor_15024, Tensor_15025, Tensor_15830, Tensor_15826, Tensor_14897, Tensor_14898, Tensor_14919, Tensor_14920, Tensor_15820, Tensor_15812, Tensor_15816, Tensor_15808, Tensor_15804, Tensor_15800, Tensor_15822, Tensor_15814, Tensor_15818, Tensor_15810, Tensor_14790, Tensor_14791, Tensor_14874, Tensor_14875, Tensor_15806, Tensor_15802, Tensor_14747, Tensor_14748, Tensor_14769, Tensor_14770, Tensor_15796, Tensor_15788, Tensor_15792, Tensor_15784, Tensor_15780, Tensor_15776, Tensor_15798, Tensor_15790, Tensor_15794, Tensor_15786, Tensor_14640, Tensor_14641, Tensor_14724, Tensor_14725, Tensor_15782, Tensor_15778, Tensor_14597, Tensor_14598, Tensor_14619, Tensor_14620, Tensor_15772, Tensor_15764, Tensor_15768, Tensor_15760, Tensor_15756, Tensor_15752, Tensor_15774, Tensor_15766, Tensor_15770, Tensor_15762, Tensor_14490, Tensor_14491, Tensor_14574, Tensor_14575, Tensor_15758, Tensor_15754, Tensor_14447, Tensor_14448, Tensor_14469, Tensor_14470, Tensor_15748, Tensor_15740, Tensor_15744, Tensor_15736, Tensor_15732, Tensor_15728, Tensor_15750, Tensor_15742, Tensor_15746, Tensor_15738, Tensor_14340, Tensor_14341, Tensor_14424, Tensor_14425, Tensor_15734, Tensor_15730, Tensor_14297, Tensor_14298, Tensor_14319, Tensor_14320, Tensor_15724, Tensor_15716, Tensor_15720, Tensor_15712, Tensor_15708, Tensor_15704, Tensor_15726, Tensor_15718, Tensor_15722, Tensor_15714, Tensor_14190, Tensor_14191, Tensor_14274, Tensor_14275, Tensor_15710, Tensor_15706, Tensor_14147, Tensor_14148, Tensor_14169, Tensor_14170, Tensor_15700, Tensor_15692, Tensor_15696, Tensor_15688, Tensor_15684, Tensor_15680, Tensor_15702, Tensor_15694, Tensor_15698, Tensor_15690, Tensor_14040, Tensor_14041, Tensor_14124, Tensor_14125, Tensor_15686, Tensor_15682, Tensor_13997, Tensor_13998, Tensor_14019, Tensor_14020, Tensor_15676, Tensor_15668, Tensor_15672, Tensor_15664, Tensor_15660, Tensor_15656, Tensor_15678, Tensor_15670, Tensor_15674, Tensor_15666, Tensor_13890, Tensor_13891, Tensor_13974, Tensor_13975, Tensor_15662, Tensor_15658, Tensor_13847, Tensor_13848, Tensor_13869, Tensor_13870, Tensor_13820, Tensor_13821, Tensor_15654, Tensor_15652], outputs = [Tensor_31025], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_31025: shape = [86661352], stride = [1], offset = 0, size = 86661352, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_31026: operator = ElemwiseUnaryForward, inputs = [Tensor_31025], outputs = [Tensor_31027], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_ISFINITE]
Tensor_31027: shape = [86661352], stride = [1], offset = 0, size = 86661352, datatype = boolean, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_31028: operator = AllForward, inputs = [Tensor_31027], outputs = [Tensor_31029], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dims: 0]
Tensor_31029: shape = [], stride = [], offset = 0, size = 1, datatype = uint8, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_31030: operator = TypecastForward, inputs = [Tensor_31029], outputs = [Tensor_31031], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_BOOLEAN]
Tensor_31031: shape = [], stride = [], offset = 0, size = 1, datatype = boolean, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_31032: operator = ElemwiseUnaryForward, inputs = [Tensor_31031], outputs = [Tensor_31033], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_BITWISEXOR scal1_bool: true]
Tensor_31033: shape = [], stride = [], offset = 0, size = 1, datatype = boolean, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_31034: operator = ReshapeForward, inputs = [Tensor_15641], outputs = [Tensor_31035], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 768 stride: 1]
Tensor_31035: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_31036: operator = NormForward, inputs = [Tensor_31035], outputs = [Tensor_31037], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_31037: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_31038: operator = ReshapeForward, inputs = [Tensor_15944], outputs = [Tensor_31039], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 589824 stride: 1]
Tensor_31039: shape = [589824], stride = [1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_31040: operator = NormForward, inputs = [Tensor_31039], outputs = [Tensor_31041], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_31041: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_31042: operator = NormForward, inputs = [Tensor_15946], outputs = [Tensor_31043], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_31043: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_31044: operator = ReshapeForward, inputs = [Tensor_15639], outputs = [Tensor_31045], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 152832 stride: 1]
Tensor_31045: shape = [152832], stride = [1], offset = 0, size = 152832, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_31046: operator = NormForward, inputs = [Tensor_31045], outputs = [Tensor_31047], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_31047: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_31048: operator = ReshapeForward, inputs = [Tensor_15940], outputs = [Tensor_31049], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 589824 stride: 1]
Tensor_31049: shape = [589824], stride = [1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_31050: operator = NormForward, inputs = [Tensor_31049], outputs = [Tensor_31051], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_31051: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_31052: operator = NormForward, inputs = [Tensor_15942], outputs = [Tensor_31053], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_31053: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_31054: operator = ReshapeForward, inputs = [Tensor_15932], outputs = [Tensor_31055], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 589824 stride: 1]
Tensor_31055: shape = [589824], stride = [1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_31056: operator = NormForward, inputs = [Tensor_31055], outputs = [Tensor_31057], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_31057: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_31058: operator = NormForward, inputs = [Tensor_15934], outputs = [Tensor_31059], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_31059: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_31060: operator = ReshapeForward, inputs = [Tensor_15936], outputs = [Tensor_31061], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 589824 stride: 1]
Tensor_31061: shape = [589824], stride = [1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_31062: operator = NormForward, inputs = [Tensor_31061], outputs = [Tensor_31063], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_31063: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_31064: operator = NormForward, inputs = [Tensor_15938], outputs = [Tensor_31065], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_31065: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_31066: operator = ReshapeForward, inputs = [Tensor_15928], outputs = [Tensor_31067], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 589824 stride: 1]
Tensor_31067: shape = [589824], stride = [1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_31068: operator = NormForward, inputs = [Tensor_31067], outputs = [Tensor_31069], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_31069: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_31070: operator = NormForward, inputs = [Tensor_15930], outputs = [Tensor_31071], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_31071: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_31072: operator = NormForward, inputs = [Tensor_15532], outputs = [Tensor_31073], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_31073: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_31074: operator = NormForward, inputs = [Tensor_15533], outputs = [Tensor_31075], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_31075: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_31076: operator = NormForward, inputs = [Tensor_15616], outputs = [Tensor_31077], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_31077: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_31078: operator = NormForward, inputs = [Tensor_15617], outputs = [Tensor_31079], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_31079: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_31080: operator = ReshapeForward, inputs = [Tensor_15924], outputs = [Tensor_31081], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 2359296 stride: 1]
Tensor_31081: shape = [2359296], stride = [1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_31082: operator = NormForward, inputs = [Tensor_31081], outputs = [Tensor_31083], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_31083: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_31084: operator = NormForward, inputs = [Tensor_15926], outputs = [Tensor_31085], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_31085: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_31086: operator = ReshapeForward, inputs = [Tensor_15920], outputs = [Tensor_31087], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 2359296 stride: 1]
Tensor_31087: shape = [2359296], stride = [1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_31088: operator = NormForward, inputs = [Tensor_31087], outputs = [Tensor_31089], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_31089: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_31090: operator = NormForward, inputs = [Tensor_15922], outputs = [Tensor_31091], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_31091: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_31092: operator = NormForward, inputs = [Tensor_15493], outputs = [Tensor_31093], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_31093: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_31094: operator = NormForward, inputs = [Tensor_15494], outputs = [Tensor_31095], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_31095: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_31096: operator = NormForward, inputs = [Tensor_15515], outputs = [Tensor_31097], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_31097: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_31098: operator = NormForward, inputs = [Tensor_15516], outputs = [Tensor_31099], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_31099: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_31100: operator = ReshapeForward, inputs = [Tensor_15916], outputs = [Tensor_31101], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 589824 stride: 1]
Tensor_31101: shape = [589824], stride = [1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_31102: operator = NormForward, inputs = [Tensor_31101], outputs = [Tensor_31103], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_31103: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_31104: operator = NormForward, inputs = [Tensor_15918], outputs = [Tensor_31105], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_31105: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_31106: operator = ReshapeForward, inputs = [Tensor_15908], outputs = [Tensor_31107], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 589824 stride: 1]
Tensor_31107: shape = [589824], stride = [1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_31108: operator = NormForward, inputs = [Tensor_31107], outputs = [Tensor_31109], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_31109: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_31110: operator = NormForward, inputs = [Tensor_15910], outputs = [Tensor_31111], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_31111: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_31112: operator = ReshapeForward, inputs = [Tensor_15912], outputs = [Tensor_31113], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 589824 stride: 1]
Tensor_31113: shape = [589824], stride = [1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_31114: operator = NormForward, inputs = [Tensor_31113], outputs = [Tensor_31115], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_31115: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_31116: operator = NormForward, inputs = [Tensor_15914], outputs = [Tensor_31117], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_31117: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_31118: operator = ReshapeForward, inputs = [Tensor_15904], outputs = [Tensor_31119], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 589824 stride: 1]
Tensor_31119: shape = [589824], stride = [1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_31120: operator = NormForward, inputs = [Tensor_31119], outputs = [Tensor_31121], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_31121: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_31122: operator = NormForward, inputs = [Tensor_15906], outputs = [Tensor_31123], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_31123: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_31124: operator = NormForward, inputs = [Tensor_15390], outputs = [Tensor_31125], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_31125: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_31126: operator = NormForward, inputs = [Tensor_15391], outputs = [Tensor_31127], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_31127: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_31128: operator = NormForward, inputs = [Tensor_15474], outputs = [Tensor_31129], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_31129: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_31130: operator = NormForward, inputs = [Tensor_15475], outputs = [Tensor_31131], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_31131: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_31132: operator = ReshapeForward, inputs = [Tensor_15900], outputs = [Tensor_31133], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 2359296 stride: 1]
Tensor_31133: shape = [2359296], stride = [1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_31134: operator = NormForward, inputs = [Tensor_31133], outputs = [Tensor_31135], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_31135: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_31136: operator = NormForward, inputs = [Tensor_15902], outputs = [Tensor_31137], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_31137: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_31138: operator = ReshapeForward, inputs = [Tensor_15896], outputs = [Tensor_31139], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 2359296 stride: 1]
Tensor_31139: shape = [2359296], stride = [1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_31140: operator = NormForward, inputs = [Tensor_31139], outputs = [Tensor_31141], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_31141: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_31142: operator = NormForward, inputs = [Tensor_15898], outputs = [Tensor_31143], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_31143: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_31144: operator = NormForward, inputs = [Tensor_15347], outputs = [Tensor_31145], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_31145: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_31146: operator = NormForward, inputs = [Tensor_15348], outputs = [Tensor_31147], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_31147: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_31148: operator = NormForward, inputs = [Tensor_15369], outputs = [Tensor_31149], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_31149: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_31150: operator = NormForward, inputs = [Tensor_15370], outputs = [Tensor_31151], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_31151: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_31152: operator = ReshapeForward, inputs = [Tensor_15892], outputs = [Tensor_31153], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 589824 stride: 1]
Tensor_31153: shape = [589824], stride = [1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_31154: operator = NormForward, inputs = [Tensor_31153], outputs = [Tensor_31155], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_31155: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_31156: operator = NormForward, inputs = [Tensor_15894], outputs = [Tensor_31157], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_31157: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_31158: operator = ReshapeForward, inputs = [Tensor_15884], outputs = [Tensor_31159], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 589824 stride: 1]
Tensor_31159: shape = [589824], stride = [1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_31160: operator = NormForward, inputs = [Tensor_31159], outputs = [Tensor_31161], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_31161: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_31162: operator = NormForward, inputs = [Tensor_15886], outputs = [Tensor_31163], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_31163: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_31164: operator = ReshapeForward, inputs = [Tensor_15888], outputs = [Tensor_31165], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 589824 stride: 1]
Tensor_31165: shape = [589824], stride = [1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_31166: operator = NormForward, inputs = [Tensor_31165], outputs = [Tensor_31167], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_31167: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_31168: operator = NormForward, inputs = [Tensor_15890], outputs = [Tensor_31169], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_31169: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_31170: operator = ReshapeForward, inputs = [Tensor_15880], outputs = [Tensor_31171], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 589824 stride: 1]
Tensor_31171: shape = [589824], stride = [1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_31172: operator = NormForward, inputs = [Tensor_31171], outputs = [Tensor_31173], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_31173: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_31174: operator = NormForward, inputs = [Tensor_15882], outputs = [Tensor_31175], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_31175: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_31176: operator = NormForward, inputs = [Tensor_15240], outputs = [Tensor_31177], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_31177: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_31178: operator = NormForward, inputs = [Tensor_15241], outputs = [Tensor_31179], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_31179: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_31180: operator = NormForward, inputs = [Tensor_15324], outputs = [Tensor_31181], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_31181: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_31182: operator = NormForward, inputs = [Tensor_15325], outputs = [Tensor_31183], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_31183: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_31184: operator = ReshapeForward, inputs = [Tensor_15876], outputs = [Tensor_31185], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 2359296 stride: 1]
Tensor_31185: shape = [2359296], stride = [1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_31186: operator = NormForward, inputs = [Tensor_31185], outputs = [Tensor_31187], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_31187: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_31188: operator = NormForward, inputs = [Tensor_15878], outputs = [Tensor_31189], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_31189: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_31190: operator = ReshapeForward, inputs = [Tensor_15872], outputs = [Tensor_31191], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 2359296 stride: 1]
Tensor_31191: shape = [2359296], stride = [1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_31192: operator = NormForward, inputs = [Tensor_31191], outputs = [Tensor_31193], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_31193: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_31194: operator = NormForward, inputs = [Tensor_15874], outputs = [Tensor_31195], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_31195: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_31196: operator = NormForward, inputs = [Tensor_15197], outputs = [Tensor_31197], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_31197: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_31198: operator = NormForward, inputs = [Tensor_15198], outputs = [Tensor_31199], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_31199: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)

Target (result) graph:
Tensor_32906: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28281: 3072 Bnn)
Tensor_32907: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28285: 3072 Bnn)
Tensor_32908: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28317: 12288 Bnn)
Tensor_32909: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28321: 12288 Bnn)
Tensor_32910: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28349: 3072 Bnn)
Tensor_32911: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28353: 3072 Bnn)
Tensor_32912: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28382: 3072 Bnn)
Tensor_32913: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28386: 3072 Bnn)
Tensor_32914: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28488: 3072 Bnn)
Tensor_32915: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28492: 3072 Bnn)
Tensor_32916: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28522: 12288 Bnn)
Tensor_32917: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28526: 12288 Bnn)
Tensor_32918: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28554: 3072 Bnn)
Tensor_32919: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28558: 3072 Bnn)
Tensor_32920: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28587: 3072 Bnn)
Tensor_32921: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28591: 3072 Bnn)
Tensor_32922: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28693: 3072 Bnn)
Tensor_32923: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28697: 3072 Bnn)
Tensor_32924: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28727: 12288 Bnn)
Tensor_32925: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28731: 12288 Bnn)
Tensor_32926: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28759: 3072 Bnn)
Tensor_32927: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28763: 3072 Bnn)
Tensor_32928: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28792: 3072 Bnn)
Tensor_32929: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28796: 3072 Bnn)
Tensor_32930: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28898: 3072 Bnn)
Tensor_32931: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28902: 3072 Bnn)
Tensor_32932: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28932: 12288 Bnn)
Tensor_32933: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28936: 12288 Bnn)
Tensor_32934: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28964: 3072 Bnn)
Tensor_32935: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28968: 3072 Bnn)
Tensor_32936: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28997: 3072 Bnn)
Tensor_32937: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29001: 3072 Bnn)
Tensor_32938: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29103: 3072 Bnn)
Tensor_32939: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29107: 3072 Bnn)
Tensor_32940: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29137: 12288 Bnn)
Tensor_32941: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29141: 12288 Bnn)
Tensor_32942: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29169: 3072 Bnn)
Tensor_32943: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29173: 3072 Bnn)
Tensor_32944: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29202: 3072 Bnn)
Tensor_32945: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29206: 3072 Bnn)
Tensor_32946: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29308: 3072 Bnn)
Tensor_32947: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29312: 3072 Bnn)
Tensor_32948: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29342: 12288 Bnn)
Tensor_32949: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29346: 12288 Bnn)
Tensor_32950: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29374: 3072 Bnn)
Tensor_32951: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29378: 3072 Bnn)
Tensor_32952: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29407: 3072 Bnn)
Tensor_32953: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29411: 3072 Bnn)
Tensor_32954: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29513: 3072 Bnn)
Tensor_32955: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29517: 3072 Bnn)
Tensor_32956: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29547: 12288 Bnn)
Tensor_32957: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29551: 12288 Bnn)
Tensor_32958: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29579: 3072 Bnn)
Tensor_32959: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29583: 3072 Bnn)
Tensor_32960: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29612: 3072 Bnn)
Tensor_32961: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29616: 3072 Bnn)
Tensor_32962: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29718: 3072 Bnn)
Tensor_32963: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29722: 3072 Bnn)
Tensor_32964: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29752: 12288 Bnn)
Tensor_32965: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29756: 12288 Bnn)
Tensor_32966: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29784: 3072 Bnn)
Tensor_32967: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29788: 3072 Bnn)
Tensor_32968: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29817: 3072 Bnn)
Tensor_32969: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29821: 3072 Bnn)
Tensor_32970: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29923: 3072 Bnn)
Tensor_32971: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29927: 3072 Bnn)
Tensor_32972: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29957: 12288 Bnn)
Tensor_32973: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29961: 12288 Bnn)
Tensor_32974: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29989: 3072 Bnn)
Tensor_32975: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29993: 3072 Bnn)
Tensor_32976: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30022: 3072 Bnn)
Tensor_32977: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30026: 3072 Bnn)
Tensor_32978: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30128: 3072 Bnn)
Tensor_32979: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30132: 3072 Bnn)
Tensor_32980: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30162: 12288 Bnn)
Tensor_32981: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30166: 12288 Bnn)
Tensor_32982: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30194: 3072 Bnn)
Tensor_32983: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30198: 3072 Bnn)
Tensor_32984: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30227: 3072 Bnn)
Tensor_32985: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30231: 3072 Bnn)
Tensor_32986: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30333: 3072 Bnn)
Tensor_32987: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30337: 3072 Bnn)
Tensor_32988: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30367: 12288 Bnn)
Tensor_32989: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30371: 12288 Bnn)
Tensor_32990: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30399: 3072 Bnn)
Tensor_32991: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30403: 3072 Bnn)
Tensor_32992: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30432: 3072 Bnn)
Tensor_32993: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30436: 3072 Bnn)
Tensor_32994: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30538: 3072 Bnn)
Tensor_32995: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30542: 3072 Bnn)
Tensor_32996: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30568: 12288 Bnn)
Tensor_32997: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30572: 12288 Bnn)
Tensor_32998: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30600: 3072 Bnn)
Tensor_32999: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30604: 3072 Bnn)
Tensor_33000: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30629: 3072 Bnn)
Tensor_33001: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30633: 3072 Bnn)
Tensor_33002: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30735: 3072 Bnn)
Tensor_33003: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30739: 3072 Bnn)
Tensor_33004: shape = [199, 768], stride = [768, 1], offset = 0, size = 152832, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (30756: 611328 Bnn)
Tensor_33005: shape = [1, 1, 768], stride = [768, 768, 1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup_sum), attr<Storage> = (30758: 3072 Bnn)
Tensor_33006: shape = [1000, 768], stride = [768, 1], offset = 0, size = 768000, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30773: 3072000 Bnn)
Tensor_33007: shape = [1000], stride = [1], offset = 0, size = 1000, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30774: 4000 Bnn)
Tensor_33008: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30775: 9437184 Bnn)
Tensor_33009: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30776: 3072 Bnn)
Tensor_33010: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30777: 9437184 Bnn)
Tensor_33011: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30778: 12288 Bnn)
Tensor_33012: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30779: 2359296 Bnn)
Tensor_33013: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30780: 3072 Bnn)
Tensor_33014: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30781: 2359296 Bnn)
Tensor_33015: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30782: 3072 Bnn)
Tensor_33016: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30783: 2359296 Bnn)
Tensor_33017: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30784: 3072 Bnn)
Tensor_33018: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30785: 2359296 Bnn)
Tensor_33019: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30786: 3072 Bnn)
Tensor_33020: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30787: 9437184 Bnn)
Tensor_33021: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30788: 3072 Bnn)
Tensor_33022: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30789: 9437184 Bnn)
Tensor_33023: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30790: 12288 Bnn)
Tensor_33024: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30791: 2359296 Bnn)
Tensor_33025: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30792: 3072 Bnn)
Tensor_33026: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30793: 2359296 Bnn)
Tensor_33027: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30794: 3072 Bnn)
Tensor_33028: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30795: 2359296 Bnn)
Tensor_33029: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30796: 3072 Bnn)
Tensor_33030: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30797: 2359296 Bnn)
Tensor_33031: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30798: 3072 Bnn)
Tensor_33032: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30799: 9437184 Bnn)
Tensor_33033: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30800: 3072 Bnn)
Tensor_33034: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30801: 9437184 Bnn)
Tensor_33035: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30802: 12288 Bnn)
Tensor_33036: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30803: 2359296 Bnn)
Tensor_33037: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30804: 3072 Bnn)
Tensor_33038: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30805: 2359296 Bnn)
Tensor_33039: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30806: 3072 Bnn)
Tensor_33040: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30807: 2359296 Bnn)
Tensor_33041: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30808: 3072 Bnn)
Tensor_33042: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30809: 2359296 Bnn)
Tensor_33043: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30810: 3072 Bnn)
Tensor_33044: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30811: 9437184 Bnn)
Tensor_33045: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30812: 3072 Bnn)
Tensor_33046: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30813: 9437184 Bnn)
Tensor_33047: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30814: 12288 Bnn)
Tensor_33048: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30815: 2359296 Bnn)
Tensor_33049: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30816: 3072 Bnn)
Tensor_33050: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30817: 2359296 Bnn)
Tensor_33051: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30818: 3072 Bnn)
Tensor_33052: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30819: 2359296 Bnn)
Tensor_33053: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30820: 3072 Bnn)
Tensor_33054: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30821: 2359296 Bnn)
Tensor_33055: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30822: 3072 Bnn)
Tensor_33056: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30823: 9437184 Bnn)
Tensor_33057: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30825: 3072 Bnn)
Tensor_33058: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30827: 9437184 Bnn)
Tensor_33059: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30829: 12288 Bnn)
Tensor_33060: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30831: 2359296 Bnn)
Tensor_33061: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30833: 3072 Bnn)
Tensor_33062: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30835: 2359296 Bnn)
Tensor_33063: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30837: 3072 Bnn)
Tensor_33064: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30839: 2359296 Bnn)
Tensor_33065: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30841: 3072 Bnn)
Tensor_33066: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30843: 2359296 Bnn)
Tensor_33067: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30845: 3072 Bnn)
Tensor_33068: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30847: 9437184 Bnn)
Tensor_33069: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30849: 3072 Bnn)
Tensor_33070: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30851: 9437184 Bnn)
Tensor_33071: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30853: 12288 Bnn)
Tensor_33072: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30855: 2359296 Bnn)
Tensor_33073: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30857: 3072 Bnn)
Tensor_33074: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30859: 2359296 Bnn)
Tensor_33075: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30861: 3072 Bnn)
Tensor_33076: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30863: 2359296 Bnn)
Tensor_33077: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30865: 3072 Bnn)
Tensor_33078: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30867: 2359296 Bnn)
Tensor_33079: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30869: 3072 Bnn)
Tensor_33080: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30871: 9437184 Bnn)
Tensor_33081: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30873: 3072 Bnn)
Tensor_33082: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30875: 9437184 Bnn)
Tensor_33083: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30877: 12288 Bnn)
Tensor_33084: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30879: 2359296 Bnn)
Tensor_33085: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30881: 3072 Bnn)
Tensor_33086: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30883: 2359296 Bnn)
Tensor_33087: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30885: 3072 Bnn)
Tensor_33088: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30887: 2359296 Bnn)
Tensor_33089: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30889: 3072 Bnn)
Tensor_33090: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30891: 2359296 Bnn)
Tensor_33091: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30893: 3072 Bnn)
Tensor_33092: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30895: 9437184 Bnn)
Tensor_33093: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30897: 3072 Bnn)
Tensor_33094: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30899: 9437184 Bnn)
Tensor_33095: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30901: 12288 Bnn)
Tensor_33096: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30903: 2359296 Bnn)
Tensor_33097: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30905: 3072 Bnn)
Tensor_33098: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30907: 2359296 Bnn)
Tensor_33099: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30909: 3072 Bnn)
Tensor_33100: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30911: 2359296 Bnn)
Tensor_33101: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30913: 3072 Bnn)
Tensor_33102: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30915: 2359296 Bnn)
Tensor_33103: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30917: 3072 Bnn)
Tensor_33104: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30919: 9437184 Bnn)
Tensor_33105: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30921: 3072 Bnn)
Tensor_33106: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30923: 9437184 Bnn)
Tensor_33107: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30925: 12288 Bnn)
Tensor_33108: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30927: 2359296 Bnn)
Tensor_33109: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30929: 3072 Bnn)
Tensor_33110: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30931: 2359296 Bnn)
Tensor_33111: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30933: 3072 Bnn)
Tensor_33112: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30935: 2359296 Bnn)
Tensor_33113: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30937: 3072 Bnn)
Tensor_33114: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30939: 2359296 Bnn)
Tensor_33115: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30941: 3072 Bnn)
Tensor_33116: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30943: 9437184 Bnn)
Tensor_33117: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30945: 3072 Bnn)
Tensor_33118: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30947: 9437184 Bnn)
Tensor_33119: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30949: 12288 Bnn)
Tensor_33120: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30951: 2359296 Bnn)
Tensor_33121: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30953: 3072 Bnn)
Tensor_33122: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30955: 2359296 Bnn)
Tensor_33123: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30957: 3072 Bnn)
Tensor_33124: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30959: 2359296 Bnn)
Tensor_33125: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30961: 3072 Bnn)
Tensor_33126: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30963: 2359296 Bnn)
Tensor_33127: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30965: 3072 Bnn)
Tensor_33128: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30967: 9437184 Bnn)
Tensor_33129: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30969: 3072 Bnn)
Tensor_33130: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30971: 9437184 Bnn)
Tensor_33131: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30973: 12288 Bnn)
Tensor_33132: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30975: 2359296 Bnn)
Tensor_33133: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30977: 3072 Bnn)
Tensor_33134: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30979: 2359296 Bnn)
Tensor_33135: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30981: 3072 Bnn)
Tensor_33136: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30983: 2359296 Bnn)
Tensor_33137: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30985: 3072 Bnn)
Tensor_33138: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30987: 2359296 Bnn)
Tensor_33139: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30989: 3072 Bnn)
Tensor_33140: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30991: 9437184 Bnn)
Tensor_33141: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30993: 3072 Bnn)
Tensor_33142: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30995: 9437184 Bnn)
Tensor_33143: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30997: 12288 Bnn)
Tensor_33144: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30999: 2359296 Bnn)
Tensor_33145: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (31001: 3072 Bnn)
Tensor_33146: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (31003: 2359296 Bnn)
Tensor_33147: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (31005: 3072 Bnn)
Tensor_33148: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (31007: 2359296 Bnn)
Tensor_33149: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (31009: 3072 Bnn)
Tensor_33150: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (31011: 2359296 Bnn)
Tensor_33151: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (31013: 3072 Bnn)
Tensor_33152: shape = [768, 3, 16, 16], stride = [768, 256, 16, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (31015: 2359296 Bnn)
Tensor_33153: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (31017: 3072 Bnn)
Operation_32297: operator = AllReduceDuplicated, inputs = [Tensor_33005, nullptr], outputs = [Tensor_33156, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = all->this->31034
  - Operand: [subop: SUM tag: 32296]
Operation_33155: operator = Consume, inputs = [Tensor_33005], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_33156: shape = [1, 1, 768], stride = [768, 768, 1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30758: 3072 Bnn)
Operation_32300: operator = AllReduceDuplicated, inputs = [Tensor_33004, nullptr], outputs = [Tensor_33159, nullptr], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = this->31044
  - Operand: [subop: SUM tag: 32299]
Operation_33158: operator = Consume, inputs = [Tensor_33004], outputs = [], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_33159: shape = [199, 768], stride = [768, 1], offset = 0, size = 152832, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30756: 611328 Bnn)
Operation_31034: operator = ReshapeForward, inputs = [Tensor_33156], outputs = [Tensor_33175], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = 32297->this
  - Operand: [shape: 768 stride: 1]
Operation_31038: operator = ReshapeForward, inputs = [Tensor_33152], outputs = [Tensor_33178], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 589824 stride: 1]
Operation_31042: operator = NormForward, inputs = [Tensor_33153], outputs = [Tensor_33181], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Operation_31044: operator = ReshapeForward, inputs = [Tensor_33159], outputs = [Tensor_33182], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = 32300->this
  - Operand: [shape: 152832 stride: 1]
Operation_31048: operator = ReshapeForward, inputs = [Tensor_33150], outputs = [Tensor_33185], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 589824 stride: 1]
Operation_31024: operator = ApexAmpPackForward, inputs = [Tensor_33152, Tensor_33156, Tensor_33153, Tensor_33159, Tensor_33150, Tensor_33146, Tensor_33148, Tensor_33144, Tensor_33142, Tensor_33140, Tensor_33151, Tensor_33147, Tensor_33149, Tensor_33145, Tensor_33000, Tensor_33001, Tensor_33002, Tensor_33003, Tensor_33143, Tensor_33141, Tensor_32996, Tensor_32997, Tensor_32998, Tensor_32999, Tensor_33138, Tensor_33134, Tensor_33136, Tensor_33132, Tensor_33130, Tensor_33128, Tensor_33139, Tensor_33135, Tensor_33137, Tensor_33133, Tensor_32992, Tensor_32993, Tensor_32994, Tensor_32995, Tensor_33131, Tensor_33129, Tensor_32988, Tensor_32989, Tensor_32990, Tensor_32991, Tensor_33126, Tensor_33122, Tensor_33124, Tensor_33120, Tensor_33118, Tensor_33116, Tensor_33127, Tensor_33123, Tensor_33125, Tensor_33121, Tensor_32984, Tensor_32985, Tensor_32986, Tensor_32987, Tensor_33119, Tensor_33117, Tensor_32980, Tensor_32981, Tensor_32982, Tensor_32983, Tensor_33114, Tensor_33110, Tensor_33112, Tensor_33108, Tensor_33106, Tensor_33104, Tensor_33115, Tensor_33111, Tensor_33113, Tensor_33109, Tensor_32976, Tensor_32977, Tensor_32978, Tensor_32979, Tensor_33107, Tensor_33105, Tensor_32972, Tensor_32973, Tensor_32974, Tensor_32975, Tensor_33102, Tensor_33098, Tensor_33100, Tensor_33096, Tensor_33094, Tensor_33092, Tensor_33103, Tensor_33099, Tensor_33101, Tensor_33097, Tensor_32968, Tensor_32969, Tensor_32970, Tensor_32971, Tensor_33095, Tensor_33093, Tensor_32964, Tensor_32965, Tensor_32966, Tensor_32967, Tensor_33090, Tensor_33086, Tensor_33088, Tensor_33084, Tensor_33082, Tensor_33080, Tensor_33091, Tensor_33087, Tensor_33089, Tensor_33085, Tensor_32960, Tensor_32961, Tensor_32962, Tensor_32963, Tensor_33083, Tensor_33081, Tensor_32956, Tensor_32957, Tensor_32958, Tensor_32959, Tensor_33078, Tensor_33074, Tensor_33076, Tensor_33072, Tensor_33070, Tensor_33068, Tensor_33079, Tensor_33075, Tensor_33077, Tensor_33073, Tensor_32952, Tensor_32953, Tensor_32954, Tensor_32955, Tensor_33071, Tensor_33069, Tensor_32948, Tensor_32949, Tensor_32950, Tensor_32951, Tensor_33066, Tensor_33062, Tensor_33064, Tensor_33060, Tensor_33058, Tensor_33056, Tensor_33067, Tensor_33063, Tensor_33065, Tensor_33061, Tensor_32944, Tensor_32945, Tensor_32946, Tensor_32947, Tensor_33059, Tensor_33057, Tensor_32940, Tensor_32941, Tensor_32942, Tensor_32943, Tensor_33054, Tensor_33050, Tensor_33052, Tensor_33048, Tensor_33046, Tensor_33044, Tensor_33055, Tensor_33051, Tensor_33053, Tensor_33049, Tensor_32936, Tensor_32937, Tensor_32938, Tensor_32939, Tensor_33047, Tensor_33045, Tensor_32932, Tensor_32933, Tensor_32934, Tensor_32935, Tensor_33042, Tensor_33038, Tensor_33040, Tensor_33036, Tensor_33034, Tensor_33032, Tensor_33043, Tensor_33039, Tensor_33041, Tensor_33037, Tensor_32928, Tensor_32929, Tensor_32930, Tensor_32931, Tensor_33035, Tensor_33033, Tensor_32924, Tensor_32925, Tensor_32926, Tensor_32927, Tensor_33030, Tensor_33026, Tensor_33028, Tensor_33024, Tensor_33022, Tensor_33020, Tensor_33031, Tensor_33027, Tensor_33029, Tensor_33025, Tensor_32920, Tensor_32921, Tensor_32922, Tensor_32923, Tensor_33023, Tensor_33021, Tensor_32916, Tensor_32917, Tensor_32918, Tensor_32919, Tensor_33018, Tensor_33014, Tensor_33016, Tensor_33012, Tensor_33010, Tensor_33008, Tensor_33019, Tensor_33015, Tensor_33017, Tensor_33013, Tensor_32912, Tensor_32913, Tensor_32914, Tensor_32915, Tensor_33011, Tensor_33009, Tensor_32908, Tensor_32909, Tensor_32910, Tensor_32911, Tensor_32906, Tensor_32907, Tensor_33007, Tensor_33006], outputs = [Tensor_33166], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: []
Tensor_33166: shape = [86661352], stride = [1], offset = 0, size = 86661352, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (33166: 346645408 Bar)
Operation_31026: operator = ElemwiseUnaryForward, inputs = [Tensor_33166], outputs = [Tensor_33168], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_ISFINITE]
Tensor_33168: shape = [86661352], stride = [1], offset = 0, size = 86661352, datatype = boolean, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (33168: 86661352 Bar)
Operation_31028: operator = AllForward, inputs = [Tensor_33168], outputs = [Tensor_33170], # backwardable inputs = 0, state = executable, attr<ExplicitSync> = ()
  - Operand: [dims: 0]
Tensor_33170: shape = [], stride = [], offset = 0, size = 1, datatype = uint8, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (33170: 1 Bar)
Operation_31030: operator = TypecastForward, inputs = [Tensor_33170], outputs = [Tensor_33172], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [dest_type: TYPECAST_BOOLEAN]
Tensor_33172: shape = [], stride = [], offset = 0, size = 1, datatype = boolean, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (33174: 1 Ban)
Operation_31032: operator = ElemwiseUnaryForward, inputs = [Tensor_33172], outputs = [Tensor_33174], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [subop: ELEMWISE_UNARY_BITWISEXOR scal1_bool: true]
Tensor_33174: shape = [], stride = [], offset = 0, size = 1, datatype = boolean, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (33174: 1 Bnn)
Tensor_33175: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30758: 3072 Bnn)
Operation_31036: operator = NormForward, inputs = [Tensor_33175], outputs = [Tensor_33177], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33177: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28234: 4 Brn)
Tensor_33178: shape = [589824], stride = [1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (31015: 2359296 Bnn)
Operation_31040: operator = NormForward, inputs = [Tensor_33178], outputs = [Tensor_33180], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33180: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28233: 4 Brn)
Tensor_33181: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28231: 4 Brn)
Tensor_33182: shape = [152832], stride = [1], offset = 0, size = 152832, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30756: 611328 Bnn)
Operation_31046: operator = NormForward, inputs = [Tensor_33182], outputs = [Tensor_33184], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33184: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28238: 4 Brn)
Tensor_33185: shape = [589824], stride = [1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (31011: 2359296 Bnn)
Operation_31050: operator = NormForward, inputs = [Tensor_33185], outputs = [Tensor_33187], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33187: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (33187: 4 Ban)
Operation_31052: operator = NormForward, inputs = [Tensor_33151], outputs = [Tensor_33189], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33189: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (33189: 4 Ban)
Operation_31054: operator = ReshapeForward, inputs = [Tensor_33146], outputs = [Tensor_33191], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 589824 stride: 1]
Tensor_33191: shape = [589824], stride = [1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (31003: 2359296 Bnn)
Operation_31056: operator = NormForward, inputs = [Tensor_33191], outputs = [Tensor_33193], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33193: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (33193: 4 Ban)
Operation_31058: operator = NormForward, inputs = [Tensor_33147], outputs = [Tensor_33195], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33195: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (33195: 4 Ban)
Operation_31060: operator = ReshapeForward, inputs = [Tensor_33148], outputs = [Tensor_33197], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 589824 stride: 1]
Tensor_33197: shape = [589824], stride = [1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (31007: 2359296 Bnn)
Operation_31062: operator = NormForward, inputs = [Tensor_33197], outputs = [Tensor_33199], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33199: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (33199: 4 Ban)
Operation_31064: operator = NormForward, inputs = [Tensor_33149], outputs = [Tensor_33201], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33201: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (33201: 4 Ban)
Operation_31066: operator = ReshapeForward, inputs = [Tensor_33144], outputs = [Tensor_33203], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 589824 stride: 1]
Tensor_33203: shape = [589824], stride = [1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30999: 2359296 Bnn)
Operation_31068: operator = NormForward, inputs = [Tensor_33203], outputs = [Tensor_33205], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33205: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (33205: 4 Ban)
Operation_31070: operator = NormForward, inputs = [Tensor_33145], outputs = [Tensor_33207], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33207: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (33207: 4 Ban)
Operation_31072: operator = NormForward, inputs = [Tensor_33000], outputs = [Tensor_33209], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33209: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (33209: 4 Ban)
Operation_31074: operator = NormForward, inputs = [Tensor_33001], outputs = [Tensor_33211], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33211: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (33211: 4 Ban)
Operation_31076: operator = NormForward, inputs = [Tensor_33002], outputs = [Tensor_33213], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33213: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (33213: 4 Ban)
Operation_31078: operator = NormForward, inputs = [Tensor_33003], outputs = [Tensor_33215], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33215: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (33215: 4 Ban)
Operation_31080: operator = ReshapeForward, inputs = [Tensor_33142], outputs = [Tensor_33217], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 2359296 stride: 1]
Tensor_33217: shape = [2359296], stride = [1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30995: 9437184 Bnn)
Operation_31082: operator = NormForward, inputs = [Tensor_33217], outputs = [Tensor_33219], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33219: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (33219: 4 Ban)
Operation_31084: operator = NormForward, inputs = [Tensor_33143], outputs = [Tensor_33221], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33221: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (33221: 4 Ban)
Operation_31086: operator = ReshapeForward, inputs = [Tensor_33140], outputs = [Tensor_33223], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 2359296 stride: 1]
Tensor_33223: shape = [2359296], stride = [1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30991: 9437184 Bnn)
Operation_31088: operator = NormForward, inputs = [Tensor_33223], outputs = [Tensor_33225], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33225: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (33225: 4 Ban)
Operation_31090: operator = NormForward, inputs = [Tensor_33141], outputs = [Tensor_33227], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33227: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (33227: 4 Ban)
Operation_31092: operator = NormForward, inputs = [Tensor_32996], outputs = [Tensor_33229], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33229: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (33229: 4 Ban)
Operation_31094: operator = NormForward, inputs = [Tensor_32997], outputs = [Tensor_33231], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33231: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (33231: 4 Ban)
Operation_31096: operator = NormForward, inputs = [Tensor_32998], outputs = [Tensor_33233], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33233: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (33233: 4 Ban)
Operation_31098: operator = NormForward, inputs = [Tensor_32999], outputs = [Tensor_33235], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33235: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (33235: 4 Ban)
Operation_31100: operator = ReshapeForward, inputs = [Tensor_33138], outputs = [Tensor_33237], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 589824 stride: 1]
Tensor_33237: shape = [589824], stride = [1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30987: 2359296 Bnn)
Operation_31102: operator = NormForward, inputs = [Tensor_33237], outputs = [Tensor_33239], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33239: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (33239: 4 Ban)
Operation_31104: operator = NormForward, inputs = [Tensor_33139], outputs = [Tensor_33241], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33241: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (33241: 4 Ban)
Operation_31106: operator = ReshapeForward, inputs = [Tensor_33134], outputs = [Tensor_33243], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 589824 stride: 1]
Tensor_33243: shape = [589824], stride = [1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30979: 2359296 Bnn)
Operation_31108: operator = NormForward, inputs = [Tensor_33243], outputs = [Tensor_33245], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33245: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (33245: 4 Ban)
Operation_31110: operator = NormForward, inputs = [Tensor_33135], outputs = [Tensor_33247], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33247: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (33247: 4 Ban)
Operation_31112: operator = ReshapeForward, inputs = [Tensor_33136], outputs = [Tensor_33249], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 589824 stride: 1]
Tensor_33249: shape = [589824], stride = [1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30983: 2359296 Bnn)
Operation_31114: operator = NormForward, inputs = [Tensor_33249], outputs = [Tensor_33251], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33251: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (33251: 4 Ban)
Operation_31116: operator = NormForward, inputs = [Tensor_33137], outputs = [Tensor_33253], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33253: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (33253: 4 Ban)
Operation_31118: operator = ReshapeForward, inputs = [Tensor_33132], outputs = [Tensor_33255], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 589824 stride: 1]
Tensor_33255: shape = [589824], stride = [1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30975: 2359296 Bnn)
Operation_31120: operator = NormForward, inputs = [Tensor_33255], outputs = [Tensor_33257], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33257: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (33257: 4 Ban)
Operation_31122: operator = NormForward, inputs = [Tensor_33133], outputs = [Tensor_33259], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33259: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (33259: 4 Ban)
Operation_31124: operator = NormForward, inputs = [Tensor_32992], outputs = [Tensor_33261], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33261: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (33261: 4 Ban)
Operation_31126: operator = NormForward, inputs = [Tensor_32993], outputs = [Tensor_33263], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33263: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (33263: 4 Ban)
Operation_31128: operator = NormForward, inputs = [Tensor_32994], outputs = [Tensor_33265], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33265: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (33265: 4 Ban)
Operation_31130: operator = NormForward, inputs = [Tensor_32995], outputs = [Tensor_33267], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33267: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (33267: 4 Ban)
Operation_31132: operator = ReshapeForward, inputs = [Tensor_33130], outputs = [Tensor_33269], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 2359296 stride: 1]
Tensor_33269: shape = [2359296], stride = [1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30971: 9437184 Bnn)
Operation_31134: operator = NormForward, inputs = [Tensor_33269], outputs = [Tensor_33271], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33271: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (33271: 4 Ban)
Operation_31136: operator = NormForward, inputs = [Tensor_33131], outputs = [Tensor_33273], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33273: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (33273: 4 Ban)
Operation_31138: operator = ReshapeForward, inputs = [Tensor_33128], outputs = [Tensor_33275], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 2359296 stride: 1]
Tensor_33275: shape = [2359296], stride = [1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30967: 9437184 Bnn)
Operation_31140: operator = NormForward, inputs = [Tensor_33275], outputs = [Tensor_33277], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33277: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (33277: 4 Ban)
Operation_31142: operator = NormForward, inputs = [Tensor_33129], outputs = [Tensor_33279], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33279: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (33279: 4 Ban)
Operation_31144: operator = NormForward, inputs = [Tensor_32988], outputs = [Tensor_33281], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33281: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (33281: 4 Ban)
Operation_31146: operator = NormForward, inputs = [Tensor_32989], outputs = [Tensor_33283], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33283: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (33283: 4 Ban)
Operation_31148: operator = NormForward, inputs = [Tensor_32990], outputs = [Tensor_33285], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33285: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (33285: 4 Ban)
Operation_31150: operator = NormForward, inputs = [Tensor_32991], outputs = [Tensor_33287], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33287: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (33287: 4 Ban)
Operation_31152: operator = ReshapeForward, inputs = [Tensor_33126], outputs = [Tensor_33289], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 589824 stride: 1]
Tensor_33289: shape = [589824], stride = [1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30963: 2359296 Bnn)
Operation_31154: operator = NormForward, inputs = [Tensor_33289], outputs = [Tensor_33291], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33291: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (33291: 4 Ban)
Operation_31156: operator = NormForward, inputs = [Tensor_33127], outputs = [Tensor_33293], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33293: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (33293: 4 Ban)
Operation_31158: operator = ReshapeForward, inputs = [Tensor_33122], outputs = [Tensor_33295], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 589824 stride: 1]
Tensor_33295: shape = [589824], stride = [1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30955: 2359296 Bnn)
Operation_31160: operator = NormForward, inputs = [Tensor_33295], outputs = [Tensor_33297], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33297: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (33297: 4 Ban)
Operation_31162: operator = NormForward, inputs = [Tensor_33123], outputs = [Tensor_33299], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33299: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (33299: 4 Ban)
Operation_31164: operator = ReshapeForward, inputs = [Tensor_33124], outputs = [Tensor_33301], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 589824 stride: 1]
Tensor_33301: shape = [589824], stride = [1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30959: 2359296 Bnn)
Operation_31166: operator = NormForward, inputs = [Tensor_33301], outputs = [Tensor_33303], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33303: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (33303: 4 Ban)
Operation_31168: operator = NormForward, inputs = [Tensor_33125], outputs = [Tensor_33305], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33305: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (33305: 4 Ban)
Operation_31170: operator = ReshapeForward, inputs = [Tensor_33120], outputs = [Tensor_33307], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 589824 stride: 1]
Tensor_33307: shape = [589824], stride = [1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30951: 2359296 Bnn)
Operation_31172: operator = NormForward, inputs = [Tensor_33307], outputs = [Tensor_33309], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33309: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (33309: 4 Ban)
Operation_31174: operator = NormForward, inputs = [Tensor_33121], outputs = [Tensor_33311], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33311: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (33311: 4 Ban)
Operation_31176: operator = NormForward, inputs = [Tensor_32984], outputs = [Tensor_33313], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33313: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (33313: 4 Ban)
Operation_31178: operator = NormForward, inputs = [Tensor_32985], outputs = [Tensor_33315], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33315: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (33315: 4 Ban)
Operation_31180: operator = NormForward, inputs = [Tensor_32986], outputs = [Tensor_33317], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33317: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (33317: 4 Ban)
Operation_31182: operator = NormForward, inputs = [Tensor_32987], outputs = [Tensor_33319], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33319: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (33319: 4 Ban)
Operation_31184: operator = ReshapeForward, inputs = [Tensor_33118], outputs = [Tensor_33321], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 2359296 stride: 1]
Tensor_33321: shape = [2359296], stride = [1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30947: 9437184 Bnn)
Operation_31186: operator = NormForward, inputs = [Tensor_33321], outputs = [Tensor_33323], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33323: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (33323: 4 Ban)
Operation_31188: operator = NormForward, inputs = [Tensor_33119], outputs = [Tensor_33325], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33325: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (33325: 4 Ban)
Operation_31190: operator = ReshapeForward, inputs = [Tensor_33116], outputs = [Tensor_33327], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 2359296 stride: 1]
Tensor_33327: shape = [2359296], stride = [1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30943: 9437184 Bnn)
Operation_31192: operator = NormForward, inputs = [Tensor_33327], outputs = [Tensor_33329], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33329: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (33329: 4 Ban)
Operation_31194: operator = NormForward, inputs = [Tensor_33117], outputs = [Tensor_33331], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33331: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (33331: 4 Ban)
Operation_31196: operator = NormForward, inputs = [Tensor_32980], outputs = [Tensor_33333], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33333: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (33333: 4 Ban)
Operation_31198: operator = NormForward, inputs = [Tensor_32981], outputs = [Tensor_33335], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33335: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (33335: 4 Ban)

Move-in tensors (source -> target):
Tensor_13820 -> Tensor_32906
Tensor_13821 -> Tensor_32907
Tensor_13847 -> Tensor_32908
Tensor_13848 -> Tensor_32909
Tensor_13869 -> Tensor_32910
Tensor_13870 -> Tensor_32911
Tensor_13890 -> Tensor_32912
Tensor_13891 -> Tensor_32913
Tensor_13974 -> Tensor_32914
Tensor_13975 -> Tensor_32915
Tensor_13997 -> Tensor_32916
Tensor_13998 -> Tensor_32917
Tensor_14019 -> Tensor_32918
Tensor_14020 -> Tensor_32919
Tensor_14040 -> Tensor_32920
Tensor_14041 -> Tensor_32921
Tensor_14124 -> Tensor_32922
Tensor_14125 -> Tensor_32923
Tensor_14147 -> Tensor_32924
Tensor_14148 -> Tensor_32925
Tensor_14169 -> Tensor_32926
Tensor_14170 -> Tensor_32927
Tensor_14190 -> Tensor_32928
Tensor_14191 -> Tensor_32929
Tensor_14274 -> Tensor_32930
Tensor_14275 -> Tensor_32931
Tensor_14297 -> Tensor_32932
Tensor_14298 -> Tensor_32933
Tensor_14319 -> Tensor_32934
Tensor_14320 -> Tensor_32935
Tensor_14340 -> Tensor_32936
Tensor_14341 -> Tensor_32937
Tensor_14424 -> Tensor_32938
Tensor_14425 -> Tensor_32939
Tensor_14447 -> Tensor_32940
Tensor_14448 -> Tensor_32941
Tensor_14469 -> Tensor_32942
Tensor_14470 -> Tensor_32943
Tensor_14490 -> Tensor_32944
Tensor_14491 -> Tensor_32945
Tensor_14574 -> Tensor_32946
Tensor_14575 -> Tensor_32947
Tensor_14597 -> Tensor_32948
Tensor_14598 -> Tensor_32949
Tensor_14619 -> Tensor_32950
Tensor_14620 -> Tensor_32951
Tensor_14640 -> Tensor_32952
Tensor_14641 -> Tensor_32953
Tensor_14724 -> Tensor_32954
Tensor_14725 -> Tensor_32955
Tensor_14747 -> Tensor_32956
Tensor_14748 -> Tensor_32957
Tensor_14769 -> Tensor_32958
Tensor_14770 -> Tensor_32959
Tensor_14790 -> Tensor_32960
Tensor_14791 -> Tensor_32961
Tensor_14874 -> Tensor_32962
Tensor_14875 -> Tensor_32963
Tensor_14897 -> Tensor_32964
Tensor_14898 -> Tensor_32965
Tensor_14919 -> Tensor_32966
Tensor_14920 -> Tensor_32967
Tensor_14940 -> Tensor_32968
Tensor_14941 -> Tensor_32969
Tensor_15024 -> Tensor_32970
Tensor_15025 -> Tensor_32971
Tensor_15047 -> Tensor_32972
Tensor_15048 -> Tensor_32973
Tensor_15069 -> Tensor_32974
Tensor_15070 -> Tensor_32975
Tensor_15090 -> Tensor_32976
Tensor_15091 -> Tensor_32977
Tensor_15174 -> Tensor_32978
Tensor_15175 -> Tensor_32979
Tensor_15197 -> Tensor_32980
Tensor_15198 -> Tensor_32981
Tensor_15219 -> Tensor_32982
Tensor_15220 -> Tensor_32983
Tensor_15240 -> Tensor_32984
Tensor_15241 -> Tensor_32985
Tensor_15324 -> Tensor_32986
Tensor_15325 -> Tensor_32987
Tensor_15347 -> Tensor_32988
Tensor_15348 -> Tensor_32989
Tensor_15369 -> Tensor_32990
Tensor_15370 -> Tensor_32991
Tensor_15390 -> Tensor_32992
Tensor_15391 -> Tensor_32993
Tensor_15474 -> Tensor_32994
Tensor_15475 -> Tensor_32995
Tensor_15493 -> Tensor_32996
Tensor_15494 -> Tensor_32997
Tensor_15515 -> Tensor_32998
Tensor_15516 -> Tensor_32999
Tensor_15532 -> Tensor_33000
Tensor_15533 -> Tensor_33001
Tensor_15616 -> Tensor_33002
Tensor_15617 -> Tensor_33003
Tensor_15639 -> Tensor_33004
Tensor_15641 -> Tensor_33005
Tensor_15652 -> Tensor_33006
Tensor_15654 -> Tensor_33007
Tensor_15656 -> Tensor_33008
Tensor_15658 -> Tensor_33009
Tensor_15660 -> Tensor_33010
Tensor_15662 -> Tensor_33011
Tensor_15664 -> Tensor_33012
Tensor_15666 -> Tensor_33013
Tensor_15668 -> Tensor_33014
Tensor_15670 -> Tensor_33015
Tensor_15672 -> Tensor_33016
Tensor_15674 -> Tensor_33017
Tensor_15676 -> Tensor_33018
Tensor_15678 -> Tensor_33019
Tensor_15680 -> Tensor_33020
Tensor_15682 -> Tensor_33021
Tensor_15684 -> Tensor_33022
Tensor_15686 -> Tensor_33023
Tensor_15688 -> Tensor_33024
Tensor_15690 -> Tensor_33025
Tensor_15692 -> Tensor_33026
Tensor_15694 -> Tensor_33027
Tensor_15696 -> Tensor_33028
Tensor_15698 -> Tensor_33029
Tensor_15700 -> Tensor_33030
Tensor_15702 -> Tensor_33031
Tensor_15704 -> Tensor_33032
Tensor_15706 -> Tensor_33033
Tensor_15708 -> Tensor_33034
Tensor_15710 -> Tensor_33035
Tensor_15712 -> Tensor_33036
Tensor_15714 -> Tensor_33037
Tensor_15716 -> Tensor_33038
Tensor_15718 -> Tensor_33039
Tensor_15720 -> Tensor_33040
Tensor_15722 -> Tensor_33041
Tensor_15724 -> Tensor_33042
Tensor_15726 -> Tensor_33043
Tensor_15728 -> Tensor_33044
Tensor_15730 -> Tensor_33045
Tensor_15732 -> Tensor_33046
Tensor_15734 -> Tensor_33047
Tensor_15736 -> Tensor_33048
Tensor_15738 -> Tensor_33049
Tensor_15740 -> Tensor_33050
Tensor_15742 -> Tensor_33051
Tensor_15744 -> Tensor_33052
Tensor_15746 -> Tensor_33053
Tensor_15748 -> Tensor_33054
Tensor_15750 -> Tensor_33055
Tensor_15752 -> Tensor_33056
Tensor_15754 -> Tensor_33057
Tensor_15756 -> Tensor_33058
Tensor_15758 -> Tensor_33059
Tensor_15760 -> Tensor_33060
Tensor_15762 -> Tensor_33061
Tensor_15764 -> Tensor_33062
Tensor_15766 -> Tensor_33063
Tensor_15768 -> Tensor_33064
Tensor_15770 -> Tensor_33065
Tensor_15772 -> Tensor_33066
Tensor_15774 -> Tensor_33067
Tensor_15776 -> Tensor_33068
Tensor_15778 -> Tensor_33069
Tensor_15780 -> Tensor_33070
Tensor_15782 -> Tensor_33071
Tensor_15784 -> Tensor_33072
Tensor_15786 -> Tensor_33073
Tensor_15788 -> Tensor_33074
Tensor_15790 -> Tensor_33075
Tensor_15792 -> Tensor_33076
Tensor_15794 -> Tensor_33077
Tensor_15796 -> Tensor_33078
Tensor_15798 -> Tensor_33079
Tensor_15800 -> Tensor_33080
Tensor_15802 -> Tensor_33081
Tensor_15804 -> Tensor_33082
Tensor_15806 -> Tensor_33083
Tensor_15808 -> Tensor_33084
Tensor_15810 -> Tensor_33085
Tensor_15812 -> Tensor_33086
Tensor_15814 -> Tensor_33087
Tensor_15816 -> Tensor_33088
Tensor_15818 -> Tensor_33089
Tensor_15820 -> Tensor_33090
Tensor_15822 -> Tensor_33091
Tensor_15824 -> Tensor_33092
Tensor_15826 -> Tensor_33093
Tensor_15828 -> Tensor_33094
Tensor_15830 -> Tensor_33095
Tensor_15832 -> Tensor_33096
Tensor_15834 -> Tensor_33097
Tensor_15836 -> Tensor_33098
Tensor_15838 -> Tensor_33099
Tensor_15840 -> Tensor_33100
Tensor_15842 -> Tensor_33101
Tensor_15844 -> Tensor_33102
Tensor_15846 -> Tensor_33103
Tensor_15848 -> Tensor_33104
Tensor_15850 -> Tensor_33105
Tensor_15852 -> Tensor_33106
Tensor_15854 -> Tensor_33107
Tensor_15856 -> Tensor_33108
Tensor_15858 -> Tensor_33109
Tensor_15860 -> Tensor_33110
Tensor_15862 -> Tensor_33111
Tensor_15864 -> Tensor_33112
Tensor_15866 -> Tensor_33113
Tensor_15868 -> Tensor_33114
Tensor_15870 -> Tensor_33115
Tensor_15872 -> Tensor_33116
Tensor_15874 -> Tensor_33117
Tensor_15876 -> Tensor_33118
Tensor_15878 -> Tensor_33119
Tensor_15880 -> Tensor_33120
Tensor_15882 -> Tensor_33121
Tensor_15884 -> Tensor_33122
Tensor_15886 -> Tensor_33123
Tensor_15888 -> Tensor_33124
Tensor_15890 -> Tensor_33125
Tensor_15892 -> Tensor_33126
Tensor_15894 -> Tensor_33127
Tensor_15896 -> Tensor_33128
Tensor_15898 -> Tensor_33129
Tensor_15900 -> Tensor_33130
Tensor_15902 -> Tensor_33131
Tensor_15904 -> Tensor_33132
Tensor_15906 -> Tensor_33133
Tensor_15908 -> Tensor_33134
Tensor_15910 -> Tensor_33135
Tensor_15912 -> Tensor_33136
Tensor_15914 -> Tensor_33137
Tensor_15916 -> Tensor_33138
Tensor_15918 -> Tensor_33139
Tensor_15920 -> Tensor_33140
Tensor_15922 -> Tensor_33141
Tensor_15924 -> Tensor_33142
Tensor_15926 -> Tensor_33143
Tensor_15928 -> Tensor_33144
Tensor_15930 -> Tensor_33145
Tensor_15932 -> Tensor_33146
Tensor_15934 -> Tensor_33147
Tensor_15936 -> Tensor_33148
Tensor_15938 -> Tensor_33149
Tensor_15940 -> Tensor_33150
Tensor_15942 -> Tensor_33151
Tensor_15944 -> Tensor_33152
Tensor_15946 -> Tensor_33153

Move-out tensors (source <- target):
Tensor_13820 <- Tensor_32906
Tensor_13821 <- Tensor_32907
Tensor_13847 <- Tensor_32908
Tensor_13848 <- Tensor_32909
Tensor_13869 <- Tensor_32910
Tensor_13870 <- Tensor_32911
Tensor_13890 <- Tensor_32912
Tensor_13891 <- Tensor_32913
Tensor_13974 <- Tensor_32914
Tensor_13975 <- Tensor_32915
Tensor_13997 <- Tensor_32916
Tensor_13998 <- Tensor_32917
Tensor_14019 <- Tensor_32918
Tensor_14020 <- Tensor_32919
Tensor_14040 <- Tensor_32920
Tensor_14041 <- Tensor_32921
Tensor_14124 <- Tensor_32922
Tensor_14125 <- Tensor_32923
Tensor_14147 <- Tensor_32924
Tensor_14148 <- Tensor_32925
Tensor_14169 <- Tensor_32926
Tensor_14170 <- Tensor_32927
Tensor_14190 <- Tensor_32928
Tensor_14191 <- Tensor_32929
Tensor_14274 <- Tensor_32930
Tensor_14275 <- Tensor_32931
Tensor_14297 <- Tensor_32932
Tensor_14298 <- Tensor_32933
Tensor_14319 <- Tensor_32934
Tensor_14320 <- Tensor_32935
Tensor_14340 <- Tensor_32936
Tensor_14341 <- Tensor_32937
Tensor_14424 <- Tensor_32938
Tensor_14425 <- Tensor_32939
Tensor_14447 <- Tensor_32940
Tensor_14448 <- Tensor_32941
Tensor_14469 <- Tensor_32942
Tensor_14470 <- Tensor_32943
Tensor_14490 <- Tensor_32944
Tensor_14491 <- Tensor_32945
Tensor_14574 <- Tensor_32946
Tensor_14575 <- Tensor_32947
Tensor_14597 <- Tensor_32948
Tensor_14598 <- Tensor_32949
Tensor_14619 <- Tensor_32950
Tensor_14620 <- Tensor_32951
Tensor_14640 <- Tensor_32952
Tensor_14641 <- Tensor_32953
Tensor_14724 <- Tensor_32954
Tensor_14725 <- Tensor_32955
Tensor_14747 <- Tensor_32956
Tensor_14748 <- Tensor_32957
Tensor_14769 <- Tensor_32958
Tensor_14770 <- Tensor_32959
Tensor_14790 <- Tensor_32960
Tensor_14791 <- Tensor_32961
Tensor_14874 <- Tensor_32962
Tensor_14875 <- Tensor_32963
Tensor_14897 <- Tensor_32964
Tensor_14898 <- Tensor_32965
Tensor_14919 <- Tensor_32966
Tensor_14920 <- Tensor_32967
Tensor_14940 <- Tensor_32968
Tensor_14941 <- Tensor_32969
Tensor_15024 <- Tensor_32970
Tensor_15025 <- Tensor_32971
Tensor_15047 <- Tensor_32972
Tensor_15048 <- Tensor_32973
Tensor_15069 <- Tensor_32974
Tensor_15070 <- Tensor_32975
Tensor_15090 <- Tensor_32976
Tensor_15091 <- Tensor_32977
Tensor_15174 <- Tensor_32978
Tensor_15175 <- Tensor_32979
Tensor_15197 <- Tensor_32980
Tensor_15198 <- Tensor_32981
Tensor_15219 <- Tensor_32982
Tensor_15220 <- Tensor_32983
Tensor_15240 <- Tensor_32984
Tensor_15241 <- Tensor_32985
Tensor_15324 <- Tensor_32986
Tensor_15325 <- Tensor_32987
Tensor_15347 <- Tensor_32988
Tensor_15348 <- Tensor_32989
Tensor_15369 <- Tensor_32990
Tensor_15370 <- Tensor_32991
Tensor_15390 <- Tensor_32992
Tensor_15391 <- Tensor_32993
Tensor_15474 <- Tensor_32994
Tensor_15475 <- Tensor_32995
Tensor_15493 <- Tensor_32996
Tensor_15494 <- Tensor_32997
Tensor_15515 <- Tensor_32998
Tensor_15516 <- Tensor_32999
Tensor_15532 <- Tensor_33000
Tensor_15533 <- Tensor_33001
Tensor_15616 <- Tensor_33002
Tensor_15617 <- Tensor_33003
Tensor_15639 <- Tensor_33159
Tensor_15641 <- Tensor_33156
Tensor_15652 <- Tensor_33006
Tensor_15654 <- Tensor_33007
Tensor_15656 <- Tensor_33008
Tensor_15658 <- Tensor_33009
Tensor_15660 <- Tensor_33010
Tensor_15662 <- Tensor_33011
Tensor_15664 <- Tensor_33012
Tensor_15666 <- Tensor_33013
Tensor_15668 <- Tensor_33014
Tensor_15670 <- Tensor_33015
Tensor_15672 <- Tensor_33016
Tensor_15674 <- Tensor_33017
Tensor_15676 <- Tensor_33018
Tensor_15678 <- Tensor_33019
Tensor_15680 <- Tensor_33020
Tensor_15682 <- Tensor_33021
Tensor_15684 <- Tensor_33022
Tensor_15686 <- Tensor_33023
Tensor_15688 <- Tensor_33024
Tensor_15690 <- Tensor_33025
Tensor_15692 <- Tensor_33026
Tensor_15694 <- Tensor_33027
Tensor_15696 <- Tensor_33028
Tensor_15698 <- Tensor_33029
Tensor_15700 <- Tensor_33030
Tensor_15702 <- Tensor_33031
Tensor_15704 <- Tensor_33032
Tensor_15706 <- Tensor_33033
Tensor_15708 <- Tensor_33034
Tensor_15710 <- Tensor_33035
Tensor_15712 <- Tensor_33036
Tensor_15714 <- Tensor_33037
Tensor_15716 <- Tensor_33038
Tensor_15718 <- Tensor_33039
Tensor_15720 <- Tensor_33040
Tensor_15722 <- Tensor_33041
Tensor_15724 <- Tensor_33042
Tensor_15726 <- Tensor_33043
Tensor_15728 <- Tensor_33044
Tensor_15730 <- Tensor_33045
Tensor_15732 <- Tensor_33046
Tensor_15734 <- Tensor_33047
Tensor_15736 <- Tensor_33048
Tensor_15738 <- Tensor_33049
Tensor_15740 <- Tensor_33050
Tensor_15742 <- Tensor_33051
Tensor_15744 <- Tensor_33052
Tensor_15746 <- Tensor_33053
Tensor_15748 <- Tensor_33054
Tensor_15750 <- Tensor_33055
Tensor_15752 <- Tensor_33056
Tensor_15754 <- Tensor_33057
Tensor_15756 <- Tensor_33058
Tensor_15758 <- Tensor_33059
Tensor_15760 <- Tensor_33060
Tensor_15762 <- Tensor_33061
Tensor_15764 <- Tensor_33062
Tensor_15766 <- Tensor_33063
Tensor_15768 <- Tensor_33064
Tensor_15770 <- Tensor_33065
Tensor_15772 <- Tensor_33066
Tensor_15774 <- Tensor_33067
Tensor_15776 <- Tensor_33068
Tensor_15778 <- Tensor_33069
Tensor_15780 <- Tensor_33070
Tensor_15782 <- Tensor_33071
Tensor_15784 <- Tensor_33072
Tensor_15786 <- Tensor_33073
Tensor_15788 <- Tensor_33074
Tensor_15790 <- Tensor_33075
Tensor_15792 <- Tensor_33076
Tensor_15794 <- Tensor_33077
Tensor_15796 <- Tensor_33078
Tensor_15798 <- Tensor_33079
Tensor_15800 <- Tensor_33080
Tensor_15802 <- Tensor_33081
Tensor_15804 <- Tensor_33082
Tensor_15806 <- Tensor_33083
Tensor_15808 <- Tensor_33084
Tensor_15810 <- Tensor_33085
Tensor_15812 <- Tensor_33086
Tensor_15814 <- Tensor_33087
Tensor_15816 <- Tensor_33088
Tensor_15818 <- Tensor_33089
Tensor_15820 <- Tensor_33090
Tensor_15822 <- Tensor_33091
Tensor_15824 <- Tensor_33092
Tensor_15826 <- Tensor_33093
Tensor_15828 <- Tensor_33094
Tensor_15830 <- Tensor_33095
Tensor_15832 <- Tensor_33096
Tensor_15834 <- Tensor_33097
Tensor_15836 <- Tensor_33098
Tensor_15838 <- Tensor_33099
Tensor_15840 <- Tensor_33100
Tensor_15842 <- Tensor_33101
Tensor_15844 <- Tensor_33102
Tensor_15846 <- Tensor_33103
Tensor_15848 <- Tensor_33104
Tensor_15850 <- Tensor_33105
Tensor_15852 <- Tensor_33106
Tensor_15854 <- Tensor_33107
Tensor_15856 <- Tensor_33108
Tensor_15858 <- Tensor_33109
Tensor_15860 <- Tensor_33110
Tensor_15862 <- Tensor_33111
Tensor_15864 <- Tensor_33112
Tensor_15866 <- Tensor_33113
Tensor_15868 <- Tensor_33114
Tensor_15870 <- Tensor_33115
Tensor_15872 <- Tensor_33116
Tensor_15874 <- Tensor_33117
Tensor_15876 <- Tensor_33118
Tensor_15878 <- Tensor_33119
Tensor_15880 <- Tensor_33120
Tensor_15882 <- Tensor_33121
Tensor_15884 <- Tensor_33122
Tensor_15886 <- Tensor_33123
Tensor_15888 <- Tensor_33124
Tensor_15890 <- Tensor_33125
Tensor_15892 <- Tensor_33126
Tensor_15894 <- Tensor_33127
Tensor_15896 <- Tensor_33128
Tensor_15898 <- Tensor_33129
Tensor_15900 <- Tensor_33130
Tensor_15902 <- Tensor_33131
Tensor_15904 <- Tensor_33132
Tensor_15906 <- Tensor_33133
Tensor_15908 <- Tensor_33134
Tensor_15910 <- Tensor_33135
Tensor_15912 <- Tensor_33136
Tensor_15914 <- Tensor_33137
Tensor_15916 <- Tensor_33138
Tensor_15918 <- Tensor_33139
Tensor_15920 <- Tensor_33140
Tensor_15922 <- Tensor_33141
Tensor_15924 <- Tensor_33142
Tensor_15926 <- Tensor_33143
Tensor_15928 <- Tensor_33144
Tensor_15930 <- Tensor_33145
Tensor_15932 <- Tensor_33146
Tensor_15934 <- Tensor_33147
Tensor_15936 <- Tensor_33148
Tensor_15938 <- Tensor_33149
Tensor_15940 <- Tensor_33150
Tensor_15942 <- Tensor_33151
Tensor_15944 <- Tensor_33152
Tensor_15946 <- Tensor_33153
Tensor_31033 <- Tensor_33174
Tensor_31037 <- Tensor_33177
Tensor_31041 <- Tensor_33180
Tensor_31043 <- Tensor_33181
Tensor_31047 <- Tensor_33184
Tensor_31051 <- Tensor_33187
Tensor_31053 <- Tensor_33189
Tensor_31057 <- Tensor_33193
Tensor_31059 <- Tensor_33195
Tensor_31063 <- Tensor_33199
Tensor_31065 <- Tensor_33201
Tensor_31069 <- Tensor_33205
Tensor_31071 <- Tensor_33207
Tensor_31073 <- Tensor_33209
Tensor_31075 <- Tensor_33211
Tensor_31077 <- Tensor_33213
Tensor_31079 <- Tensor_33215
Tensor_31083 <- Tensor_33219
Tensor_31085 <- Tensor_33221
Tensor_31089 <- Tensor_33225
Tensor_31091 <- Tensor_33227
Tensor_31093 <- Tensor_33229
Tensor_31095 <- Tensor_33231
Tensor_31097 <- Tensor_33233
Tensor_31099 <- Tensor_33235
Tensor_31103 <- Tensor_33239
Tensor_31105 <- Tensor_33241
Tensor_31109 <- Tensor_33245
Tensor_31111 <- Tensor_33247
Tensor_31115 <- Tensor_33251
Tensor_31117 <- Tensor_33253
Tensor_31121 <- Tensor_33257
Tensor_31123 <- Tensor_33259
Tensor_31125 <- Tensor_33261
Tensor_31127 <- Tensor_33263
Tensor_31129 <- Tensor_33265
Tensor_31131 <- Tensor_33267
Tensor_31135 <- Tensor_33271
Tensor_31137 <- Tensor_33273
Tensor_31141 <- Tensor_33277
Tensor_31143 <- Tensor_33279
Tensor_31145 <- Tensor_33281
Tensor_31147 <- Tensor_33283
Tensor_31149 <- Tensor_33285
Tensor_31151 <- Tensor_33287
Tensor_31155 <- Tensor_33291
Tensor_31157 <- Tensor_33293
Tensor_31161 <- Tensor_33297
Tensor_31163 <- Tensor_33299
Tensor_31167 <- Tensor_33303
Tensor_31169 <- Tensor_33305
Tensor_31173 <- Tensor_33309
Tensor_31175 <- Tensor_33311
Tensor_31177 <- Tensor_33313
Tensor_31179 <- Tensor_33315
Tensor_31181 <- Tensor_33317
Tensor_31183 <- Tensor_33319
Tensor_31187 <- Tensor_33323
Tensor_31189 <- Tensor_33325
Tensor_31193 <- Tensor_33329
Tensor_31195 <- Tensor_33331
Tensor_31197 <- Tensor_33333
Tensor_31199 <- Tensor_33335

== Translation ==
Source graph:
Tensor_14297: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28932: 12288 Bnn)
Tensor_14298: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28936: 12288 Bnn)
Tensor_14340: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28997: 3072 Bnn)
Tensor_14341: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29001: 3072 Bnn)
Tensor_14424: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29103: 3072 Bnn)
Tensor_14425: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29107: 3072 Bnn)
Tensor_14447: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29137: 12288 Bnn)
Tensor_14448: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29141: 12288 Bnn)
Tensor_14469: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29169: 3072 Bnn)
Tensor_14470: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29173: 3072 Bnn)
Tensor_14490: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29202: 3072 Bnn)
Tensor_14491: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29206: 3072 Bnn)
Tensor_14574: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29308: 3072 Bnn)
Tensor_14575: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29312: 3072 Bnn)
Tensor_14597: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29342: 12288 Bnn)
Tensor_14598: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29346: 12288 Bnn)
Tensor_14619: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29374: 3072 Bnn)
Tensor_14620: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29378: 3072 Bnn)
Tensor_14640: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29407: 3072 Bnn)
Tensor_14641: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29411: 3072 Bnn)
Tensor_14724: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29513: 3072 Bnn)
Tensor_14725: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29517: 3072 Bnn)
Tensor_14747: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29547: 12288 Bnn)
Tensor_14748: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29551: 12288 Bnn)
Tensor_14769: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29579: 3072 Bnn)
Tensor_14770: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29583: 3072 Bnn)
Tensor_14790: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29612: 3072 Bnn)
Tensor_14791: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29616: 3072 Bnn)
Tensor_14874: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29718: 3072 Bnn)
Tensor_14875: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29722: 3072 Bnn)
Tensor_14897: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29752: 12288 Bnn)
Tensor_14898: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29756: 12288 Bnn)
Tensor_14919: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29784: 3072 Bnn)
Tensor_14920: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29788: 3072 Bnn)
Tensor_14940: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29817: 3072 Bnn)
Tensor_14941: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29821: 3072 Bnn)
Tensor_15024: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29923: 3072 Bnn)
Tensor_15025: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29927: 3072 Bnn)
Tensor_15047: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29957: 12288 Bnn)
Tensor_15048: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29961: 12288 Bnn)
Tensor_15069: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29989: 3072 Bnn)
Tensor_15070: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29993: 3072 Bnn)
Tensor_15090: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30022: 3072 Bnn)
Tensor_15091: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30026: 3072 Bnn)
Tensor_15174: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30128: 3072 Bnn)
Tensor_15175: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30132: 3072 Bnn)
Tensor_15219: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30194: 3072 Bnn)
Tensor_15220: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30198: 3072 Bnn)
Tensor_15728: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30811: 9437184 Bnn)
Tensor_15730: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30812: 3072 Bnn)
Tensor_15732: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30813: 9437184 Bnn)
Tensor_15734: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30814: 12288 Bnn)
Tensor_15736: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30815: 2359296 Bnn)
Tensor_15738: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30816: 3072 Bnn)
Tensor_15740: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30817: 2359296 Bnn)
Tensor_15742: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30818: 3072 Bnn)
Tensor_15744: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30819: 2359296 Bnn)
Tensor_15746: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30820: 3072 Bnn)
Tensor_15748: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30821: 2359296 Bnn)
Tensor_15750: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30822: 3072 Bnn)
Tensor_15752: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30823: 9437184 Bnn)
Tensor_15754: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30825: 3072 Bnn)
Tensor_15756: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30827: 9437184 Bnn)
Tensor_15758: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30829: 12288 Bnn)
Tensor_15760: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30831: 2359296 Bnn)
Tensor_15762: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30833: 3072 Bnn)
Tensor_15764: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30835: 2359296 Bnn)
Tensor_15766: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30837: 3072 Bnn)
Tensor_15768: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30839: 2359296 Bnn)
Tensor_15770: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30841: 3072 Bnn)
Tensor_15772: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30843: 2359296 Bnn)
Tensor_15774: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30845: 3072 Bnn)
Tensor_15776: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30847: 9437184 Bnn)
Tensor_15778: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30849: 3072 Bnn)
Tensor_15780: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30851: 9437184 Bnn)
Tensor_15782: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30853: 12288 Bnn)
Tensor_15784: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30855: 2359296 Bnn)
Tensor_15786: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30857: 3072 Bnn)
Tensor_15788: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30859: 2359296 Bnn)
Tensor_15790: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30861: 3072 Bnn)
Tensor_15792: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30863: 2359296 Bnn)
Tensor_15794: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30865: 3072 Bnn)
Tensor_15796: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30867: 2359296 Bnn)
Tensor_15798: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30869: 3072 Bnn)
Tensor_15800: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30871: 9437184 Bnn)
Tensor_15802: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30873: 3072 Bnn)
Tensor_15804: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30875: 9437184 Bnn)
Tensor_15806: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30877: 12288 Bnn)
Tensor_15808: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30879: 2359296 Bnn)
Tensor_15810: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30881: 3072 Bnn)
Tensor_15812: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30883: 2359296 Bnn)
Tensor_15814: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30885: 3072 Bnn)
Tensor_15816: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30887: 2359296 Bnn)
Tensor_15818: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30889: 3072 Bnn)
Tensor_15820: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30891: 2359296 Bnn)
Tensor_15822: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30893: 3072 Bnn)
Tensor_15824: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30895: 9437184 Bnn)
Tensor_15826: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30897: 3072 Bnn)
Tensor_15828: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30899: 9437184 Bnn)
Tensor_15830: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30901: 12288 Bnn)
Tensor_15832: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30903: 2359296 Bnn)
Tensor_15834: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30905: 3072 Bnn)
Tensor_15836: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30907: 2359296 Bnn)
Tensor_15838: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30909: 3072 Bnn)
Tensor_15840: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30911: 2359296 Bnn)
Tensor_15842: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30913: 3072 Bnn)
Tensor_15844: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30915: 2359296 Bnn)
Tensor_15846: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30917: 3072 Bnn)
Tensor_15848: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30919: 9437184 Bnn)
Tensor_15850: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30921: 3072 Bnn)
Tensor_15852: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30923: 9437184 Bnn)
Tensor_15854: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30925: 12288 Bnn)
Tensor_15856: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30927: 2359296 Bnn)
Tensor_15858: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30929: 3072 Bnn)
Tensor_15860: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30931: 2359296 Bnn)
Tensor_15862: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30933: 3072 Bnn)
Tensor_15864: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30935: 2359296 Bnn)
Tensor_15866: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30937: 3072 Bnn)
Tensor_15868: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30939: 2359296 Bnn)
Tensor_15870: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30941: 3072 Bnn)
Operation_33336: operator = NormForward, inputs = [Tensor_15219], outputs = [Tensor_33337], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33337: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33338: operator = NormForward, inputs = [Tensor_15220], outputs = [Tensor_33339], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33339: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33340: operator = ReshapeForward, inputs = [Tensor_15868], outputs = [Tensor_33341], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 589824 stride: 1]
Tensor_33341: shape = [589824], stride = [1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33342: operator = NormForward, inputs = [Tensor_33341], outputs = [Tensor_33343], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33343: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33344: operator = NormForward, inputs = [Tensor_15870], outputs = [Tensor_33345], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33345: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33346: operator = ReshapeForward, inputs = [Tensor_15860], outputs = [Tensor_33347], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 589824 stride: 1]
Tensor_33347: shape = [589824], stride = [1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33348: operator = NormForward, inputs = [Tensor_33347], outputs = [Tensor_33349], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33349: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33350: operator = NormForward, inputs = [Tensor_15862], outputs = [Tensor_33351], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33351: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33352: operator = ReshapeForward, inputs = [Tensor_15864], outputs = [Tensor_33353], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 589824 stride: 1]
Tensor_33353: shape = [589824], stride = [1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33354: operator = NormForward, inputs = [Tensor_33353], outputs = [Tensor_33355], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33355: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33356: operator = NormForward, inputs = [Tensor_15866], outputs = [Tensor_33357], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33357: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33358: operator = ReshapeForward, inputs = [Tensor_15856], outputs = [Tensor_33359], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 589824 stride: 1]
Tensor_33359: shape = [589824], stride = [1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33360: operator = NormForward, inputs = [Tensor_33359], outputs = [Tensor_33361], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33361: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33362: operator = NormForward, inputs = [Tensor_15858], outputs = [Tensor_33363], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33363: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33364: operator = NormForward, inputs = [Tensor_15090], outputs = [Tensor_33365], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33365: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33366: operator = NormForward, inputs = [Tensor_15091], outputs = [Tensor_33367], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33367: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33368: operator = NormForward, inputs = [Tensor_15174], outputs = [Tensor_33369], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33369: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33370: operator = NormForward, inputs = [Tensor_15175], outputs = [Tensor_33371], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33371: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33372: operator = ReshapeForward, inputs = [Tensor_15852], outputs = [Tensor_33373], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 2359296 stride: 1]
Tensor_33373: shape = [2359296], stride = [1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33374: operator = NormForward, inputs = [Tensor_33373], outputs = [Tensor_33375], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33375: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33376: operator = NormForward, inputs = [Tensor_15854], outputs = [Tensor_33377], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33377: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33378: operator = ReshapeForward, inputs = [Tensor_15848], outputs = [Tensor_33379], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 2359296 stride: 1]
Tensor_33379: shape = [2359296], stride = [1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33380: operator = NormForward, inputs = [Tensor_33379], outputs = [Tensor_33381], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33381: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33382: operator = NormForward, inputs = [Tensor_15850], outputs = [Tensor_33383], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33383: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33384: operator = NormForward, inputs = [Tensor_15047], outputs = [Tensor_33385], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33385: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33386: operator = NormForward, inputs = [Tensor_15048], outputs = [Tensor_33387], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33387: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33388: operator = NormForward, inputs = [Tensor_15069], outputs = [Tensor_33389], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33389: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33390: operator = NormForward, inputs = [Tensor_15070], outputs = [Tensor_33391], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33391: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33392: operator = ReshapeForward, inputs = [Tensor_15844], outputs = [Tensor_33393], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 589824 stride: 1]
Tensor_33393: shape = [589824], stride = [1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33394: operator = NormForward, inputs = [Tensor_33393], outputs = [Tensor_33395], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33395: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33396: operator = NormForward, inputs = [Tensor_15846], outputs = [Tensor_33397], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33397: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33398: operator = ReshapeForward, inputs = [Tensor_15836], outputs = [Tensor_33399], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 589824 stride: 1]
Tensor_33399: shape = [589824], stride = [1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33400: operator = NormForward, inputs = [Tensor_33399], outputs = [Tensor_33401], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33401: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33402: operator = NormForward, inputs = [Tensor_15838], outputs = [Tensor_33403], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33403: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33404: operator = ReshapeForward, inputs = [Tensor_15840], outputs = [Tensor_33405], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 589824 stride: 1]
Tensor_33405: shape = [589824], stride = [1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33406: operator = NormForward, inputs = [Tensor_33405], outputs = [Tensor_33407], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33407: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33408: operator = NormForward, inputs = [Tensor_15842], outputs = [Tensor_33409], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33409: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33410: operator = ReshapeForward, inputs = [Tensor_15832], outputs = [Tensor_33411], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 589824 stride: 1]
Tensor_33411: shape = [589824], stride = [1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33412: operator = NormForward, inputs = [Tensor_33411], outputs = [Tensor_33413], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33413: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33414: operator = NormForward, inputs = [Tensor_15834], outputs = [Tensor_33415], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33415: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33416: operator = NormForward, inputs = [Tensor_14940], outputs = [Tensor_33417], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33417: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33418: operator = NormForward, inputs = [Tensor_14941], outputs = [Tensor_33419], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33419: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33420: operator = NormForward, inputs = [Tensor_15024], outputs = [Tensor_33421], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33421: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33422: operator = NormForward, inputs = [Tensor_15025], outputs = [Tensor_33423], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33423: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33424: operator = ReshapeForward, inputs = [Tensor_15828], outputs = [Tensor_33425], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 2359296 stride: 1]
Tensor_33425: shape = [2359296], stride = [1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33426: operator = NormForward, inputs = [Tensor_33425], outputs = [Tensor_33427], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33427: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33428: operator = NormForward, inputs = [Tensor_15830], outputs = [Tensor_33429], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33429: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33430: operator = ReshapeForward, inputs = [Tensor_15824], outputs = [Tensor_33431], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 2359296 stride: 1]
Tensor_33431: shape = [2359296], stride = [1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33432: operator = NormForward, inputs = [Tensor_33431], outputs = [Tensor_33433], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33433: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33434: operator = NormForward, inputs = [Tensor_15826], outputs = [Tensor_33435], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33435: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33436: operator = NormForward, inputs = [Tensor_14897], outputs = [Tensor_33437], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33437: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33438: operator = NormForward, inputs = [Tensor_14898], outputs = [Tensor_33439], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33439: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33440: operator = NormForward, inputs = [Tensor_14919], outputs = [Tensor_33441], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33441: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33442: operator = NormForward, inputs = [Tensor_14920], outputs = [Tensor_33443], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33443: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33444: operator = ReshapeForward, inputs = [Tensor_15820], outputs = [Tensor_33445], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 589824 stride: 1]
Tensor_33445: shape = [589824], stride = [1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33446: operator = NormForward, inputs = [Tensor_33445], outputs = [Tensor_33447], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33447: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33448: operator = NormForward, inputs = [Tensor_15822], outputs = [Tensor_33449], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33449: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33450: operator = ReshapeForward, inputs = [Tensor_15812], outputs = [Tensor_33451], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 589824 stride: 1]
Tensor_33451: shape = [589824], stride = [1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33452: operator = NormForward, inputs = [Tensor_33451], outputs = [Tensor_33453], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33453: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33454: operator = NormForward, inputs = [Tensor_15814], outputs = [Tensor_33455], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33455: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33456: operator = ReshapeForward, inputs = [Tensor_15816], outputs = [Tensor_33457], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 589824 stride: 1]
Tensor_33457: shape = [589824], stride = [1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33458: operator = NormForward, inputs = [Tensor_33457], outputs = [Tensor_33459], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33459: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33460: operator = NormForward, inputs = [Tensor_15818], outputs = [Tensor_33461], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33461: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33462: operator = ReshapeForward, inputs = [Tensor_15808], outputs = [Tensor_33463], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 589824 stride: 1]
Tensor_33463: shape = [589824], stride = [1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33464: operator = NormForward, inputs = [Tensor_33463], outputs = [Tensor_33465], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33465: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33466: operator = NormForward, inputs = [Tensor_15810], outputs = [Tensor_33467], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33467: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33468: operator = NormForward, inputs = [Tensor_14790], outputs = [Tensor_33469], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33469: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33470: operator = NormForward, inputs = [Tensor_14791], outputs = [Tensor_33471], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33471: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33472: operator = NormForward, inputs = [Tensor_14874], outputs = [Tensor_33473], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33473: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33474: operator = NormForward, inputs = [Tensor_14875], outputs = [Tensor_33475], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33475: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33476: operator = ReshapeForward, inputs = [Tensor_15804], outputs = [Tensor_33477], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 2359296 stride: 1]
Tensor_33477: shape = [2359296], stride = [1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33478: operator = NormForward, inputs = [Tensor_33477], outputs = [Tensor_33479], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33479: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33480: operator = NormForward, inputs = [Tensor_15806], outputs = [Tensor_33481], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33481: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33482: operator = ReshapeForward, inputs = [Tensor_15800], outputs = [Tensor_33483], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 2359296 stride: 1]
Tensor_33483: shape = [2359296], stride = [1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33484: operator = NormForward, inputs = [Tensor_33483], outputs = [Tensor_33485], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33485: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33486: operator = NormForward, inputs = [Tensor_15802], outputs = [Tensor_33487], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33487: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33488: operator = NormForward, inputs = [Tensor_14747], outputs = [Tensor_33489], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33489: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33490: operator = NormForward, inputs = [Tensor_14748], outputs = [Tensor_33491], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33491: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33492: operator = NormForward, inputs = [Tensor_14769], outputs = [Tensor_33493], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33493: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33494: operator = NormForward, inputs = [Tensor_14770], outputs = [Tensor_33495], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33495: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33496: operator = ReshapeForward, inputs = [Tensor_15796], outputs = [Tensor_33497], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 589824 stride: 1]
Tensor_33497: shape = [589824], stride = [1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33498: operator = NormForward, inputs = [Tensor_33497], outputs = [Tensor_33499], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33499: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33500: operator = NormForward, inputs = [Tensor_15798], outputs = [Tensor_33501], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33501: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33502: operator = ReshapeForward, inputs = [Tensor_15788], outputs = [Tensor_33503], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 589824 stride: 1]
Tensor_33503: shape = [589824], stride = [1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33504: operator = NormForward, inputs = [Tensor_33503], outputs = [Tensor_33505], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33505: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33506: operator = NormForward, inputs = [Tensor_15790], outputs = [Tensor_33507], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33507: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33508: operator = ReshapeForward, inputs = [Tensor_15792], outputs = [Tensor_33509], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 589824 stride: 1]
Tensor_33509: shape = [589824], stride = [1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33510: operator = NormForward, inputs = [Tensor_33509], outputs = [Tensor_33511], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33511: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33512: operator = NormForward, inputs = [Tensor_15794], outputs = [Tensor_33513], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33513: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33514: operator = ReshapeForward, inputs = [Tensor_15784], outputs = [Tensor_33515], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 589824 stride: 1]
Tensor_33515: shape = [589824], stride = [1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33516: operator = NormForward, inputs = [Tensor_33515], outputs = [Tensor_33517], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33517: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33518: operator = NormForward, inputs = [Tensor_15786], outputs = [Tensor_33519], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33519: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33520: operator = NormForward, inputs = [Tensor_14640], outputs = [Tensor_33521], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33521: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33522: operator = NormForward, inputs = [Tensor_14641], outputs = [Tensor_33523], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33523: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33524: operator = NormForward, inputs = [Tensor_14724], outputs = [Tensor_33525], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33525: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33526: operator = NormForward, inputs = [Tensor_14725], outputs = [Tensor_33527], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33527: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33528: operator = ReshapeForward, inputs = [Tensor_15780], outputs = [Tensor_33529], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 2359296 stride: 1]
Tensor_33529: shape = [2359296], stride = [1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33530: operator = NormForward, inputs = [Tensor_33529], outputs = [Tensor_33531], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33531: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33532: operator = NormForward, inputs = [Tensor_15782], outputs = [Tensor_33533], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33533: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33534: operator = ReshapeForward, inputs = [Tensor_15776], outputs = [Tensor_33535], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 2359296 stride: 1]
Tensor_33535: shape = [2359296], stride = [1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33536: operator = NormForward, inputs = [Tensor_33535], outputs = [Tensor_33537], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33537: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33538: operator = NormForward, inputs = [Tensor_15778], outputs = [Tensor_33539], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33539: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33540: operator = NormForward, inputs = [Tensor_14597], outputs = [Tensor_33541], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33541: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33542: operator = NormForward, inputs = [Tensor_14598], outputs = [Tensor_33543], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33543: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33544: operator = NormForward, inputs = [Tensor_14619], outputs = [Tensor_33545], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33545: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33546: operator = NormForward, inputs = [Tensor_14620], outputs = [Tensor_33547], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33547: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33548: operator = ReshapeForward, inputs = [Tensor_15772], outputs = [Tensor_33549], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 589824 stride: 1]
Tensor_33549: shape = [589824], stride = [1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33550: operator = NormForward, inputs = [Tensor_33549], outputs = [Tensor_33551], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33551: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33552: operator = NormForward, inputs = [Tensor_15774], outputs = [Tensor_33553], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33553: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33554: operator = ReshapeForward, inputs = [Tensor_15764], outputs = [Tensor_33555], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 589824 stride: 1]
Tensor_33555: shape = [589824], stride = [1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33556: operator = NormForward, inputs = [Tensor_33555], outputs = [Tensor_33557], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33557: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33558: operator = NormForward, inputs = [Tensor_15766], outputs = [Tensor_33559], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33559: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33560: operator = ReshapeForward, inputs = [Tensor_15768], outputs = [Tensor_33561], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 589824 stride: 1]
Tensor_33561: shape = [589824], stride = [1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33562: operator = NormForward, inputs = [Tensor_33561], outputs = [Tensor_33563], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33563: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33564: operator = NormForward, inputs = [Tensor_15770], outputs = [Tensor_33565], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33565: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33566: operator = ReshapeForward, inputs = [Tensor_15760], outputs = [Tensor_33567], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 589824 stride: 1]
Tensor_33567: shape = [589824], stride = [1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33568: operator = NormForward, inputs = [Tensor_33567], outputs = [Tensor_33569], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33569: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33570: operator = NormForward, inputs = [Tensor_15762], outputs = [Tensor_33571], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33571: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33572: operator = NormForward, inputs = [Tensor_14490], outputs = [Tensor_33573], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33573: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33574: operator = NormForward, inputs = [Tensor_14491], outputs = [Tensor_33575], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33575: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33576: operator = NormForward, inputs = [Tensor_14574], outputs = [Tensor_33577], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33577: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33578: operator = NormForward, inputs = [Tensor_14575], outputs = [Tensor_33579], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33579: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33580: operator = ReshapeForward, inputs = [Tensor_15756], outputs = [Tensor_33581], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 2359296 stride: 1]
Tensor_33581: shape = [2359296], stride = [1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33582: operator = NormForward, inputs = [Tensor_33581], outputs = [Tensor_33583], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33583: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33584: operator = NormForward, inputs = [Tensor_15758], outputs = [Tensor_33585], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33585: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33586: operator = ReshapeForward, inputs = [Tensor_15752], outputs = [Tensor_33587], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 2359296 stride: 1]
Tensor_33587: shape = [2359296], stride = [1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33588: operator = NormForward, inputs = [Tensor_33587], outputs = [Tensor_33589], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33589: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33590: operator = NormForward, inputs = [Tensor_15754], outputs = [Tensor_33591], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33591: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33592: operator = NormForward, inputs = [Tensor_14447], outputs = [Tensor_33593], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33593: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33594: operator = NormForward, inputs = [Tensor_14448], outputs = [Tensor_33595], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33595: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33596: operator = NormForward, inputs = [Tensor_14469], outputs = [Tensor_33597], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33597: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33598: operator = NormForward, inputs = [Tensor_14470], outputs = [Tensor_33599], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33599: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33600: operator = ReshapeForward, inputs = [Tensor_15748], outputs = [Tensor_33601], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 589824 stride: 1]
Tensor_33601: shape = [589824], stride = [1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33602: operator = NormForward, inputs = [Tensor_33601], outputs = [Tensor_33603], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33603: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33604: operator = NormForward, inputs = [Tensor_15750], outputs = [Tensor_33605], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33605: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33606: operator = ReshapeForward, inputs = [Tensor_15740], outputs = [Tensor_33607], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 589824 stride: 1]
Tensor_33607: shape = [589824], stride = [1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33608: operator = NormForward, inputs = [Tensor_33607], outputs = [Tensor_33609], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33609: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33610: operator = NormForward, inputs = [Tensor_15742], outputs = [Tensor_33611], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33611: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33612: operator = ReshapeForward, inputs = [Tensor_15744], outputs = [Tensor_33613], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 589824 stride: 1]
Tensor_33613: shape = [589824], stride = [1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33614: operator = NormForward, inputs = [Tensor_33613], outputs = [Tensor_33615], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33615: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33616: operator = NormForward, inputs = [Tensor_15746], outputs = [Tensor_33617], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33617: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33618: operator = ReshapeForward, inputs = [Tensor_15736], outputs = [Tensor_33619], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 589824 stride: 1]
Tensor_33619: shape = [589824], stride = [1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33620: operator = NormForward, inputs = [Tensor_33619], outputs = [Tensor_33621], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33621: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33622: operator = NormForward, inputs = [Tensor_15738], outputs = [Tensor_33623], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33623: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33624: operator = NormForward, inputs = [Tensor_14340], outputs = [Tensor_33625], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33625: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33626: operator = NormForward, inputs = [Tensor_14341], outputs = [Tensor_33627], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33627: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33628: operator = NormForward, inputs = [Tensor_14424], outputs = [Tensor_33629], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33629: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33630: operator = NormForward, inputs = [Tensor_14425], outputs = [Tensor_33631], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33631: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33632: operator = ReshapeForward, inputs = [Tensor_15732], outputs = [Tensor_33633], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 2359296 stride: 1]
Tensor_33633: shape = [2359296], stride = [1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33634: operator = NormForward, inputs = [Tensor_33633], outputs = [Tensor_33635], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33635: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33636: operator = NormForward, inputs = [Tensor_15734], outputs = [Tensor_33637], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33637: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33638: operator = ReshapeForward, inputs = [Tensor_15728], outputs = [Tensor_33639], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 2359296 stride: 1]
Tensor_33639: shape = [2359296], stride = [1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33640: operator = NormForward, inputs = [Tensor_33639], outputs = [Tensor_33641], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33641: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33642: operator = NormForward, inputs = [Tensor_15730], outputs = [Tensor_33643], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33643: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33644: operator = NormForward, inputs = [Tensor_14297], outputs = [Tensor_33645], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33645: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)
Operation_33646: operator = NormForward, inputs = [Tensor_14298], outputs = [Tensor_33647], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_33647: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o1], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (?), attr<Storage> = (0: 0 Bnn)

Target (result) graph:
Tensor_35376: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28932: 12288 Bnn)
Tensor_35377: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28936: 12288 Bnn)
Tensor_35378: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (28997: 3072 Bnn)
Tensor_35379: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29001: 3072 Bnn)
Tensor_35380: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29103: 3072 Bnn)
Tensor_35381: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29107: 3072 Bnn)
Tensor_35382: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29137: 12288 Bnn)
Tensor_35383: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29141: 12288 Bnn)
Tensor_35384: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29169: 3072 Bnn)
Tensor_35385: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29173: 3072 Bnn)
Tensor_35386: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29202: 3072 Bnn)
Tensor_35387: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29206: 3072 Bnn)
Tensor_35388: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29308: 3072 Bnn)
Tensor_35389: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29312: 3072 Bnn)
Tensor_35390: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29342: 12288 Bnn)
Tensor_35391: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29346: 12288 Bnn)
Tensor_35392: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29374: 3072 Bnn)
Tensor_35393: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29378: 3072 Bnn)
Tensor_35394: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29407: 3072 Bnn)
Tensor_35395: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29411: 3072 Bnn)
Tensor_35396: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29513: 3072 Bnn)
Tensor_35397: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29517: 3072 Bnn)
Tensor_35398: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29547: 12288 Bnn)
Tensor_35399: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29551: 12288 Bnn)
Tensor_35400: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29579: 3072 Bnn)
Tensor_35401: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29583: 3072 Bnn)
Tensor_35402: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29612: 3072 Bnn)
Tensor_35403: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29616: 3072 Bnn)
Tensor_35404: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29718: 3072 Bnn)
Tensor_35405: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29722: 3072 Bnn)
Tensor_35406: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29752: 12288 Bnn)
Tensor_35407: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29756: 12288 Bnn)
Tensor_35408: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29784: 3072 Bnn)
Tensor_35409: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29788: 3072 Bnn)
Tensor_35410: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29817: 3072 Bnn)
Tensor_35411: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29821: 3072 Bnn)
Tensor_35412: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29923: 3072 Bnn)
Tensor_35413: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29927: 3072 Bnn)
Tensor_35414: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29957: 12288 Bnn)
Tensor_35415: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29961: 12288 Bnn)
Tensor_35416: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29989: 3072 Bnn)
Tensor_35417: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (29993: 3072 Bnn)
Tensor_35418: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30022: 3072 Bnn)
Tensor_35419: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30026: 3072 Bnn)
Tensor_35420: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30128: 3072 Bnn)
Tensor_35421: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30132: 3072 Bnn)
Tensor_35422: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30194: 3072 Bnn)
Tensor_35423: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30198: 3072 Bnn)
Tensor_35424: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30811: 9437184 Bnn)
Tensor_35425: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30812: 3072 Bnn)
Tensor_35426: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30813: 9437184 Bnn)
Tensor_35427: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30814: 12288 Bnn)
Tensor_35428: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30815: 2359296 Bnn)
Tensor_35429: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30816: 3072 Bnn)
Tensor_35430: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30817: 2359296 Bnn)
Tensor_35431: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30818: 3072 Bnn)
Tensor_35432: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30819: 2359296 Bnn)
Tensor_35433: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30820: 3072 Bnn)
Tensor_35434: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30821: 2359296 Bnn)
Tensor_35435: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30822: 3072 Bnn)
Tensor_35436: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30823: 9437184 Bnn)
Tensor_35437: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30825: 3072 Bnn)
Tensor_35438: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30827: 9437184 Bnn)
Tensor_35439: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30829: 12288 Bnn)
Tensor_35440: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30831: 2359296 Bnn)
Tensor_35441: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30833: 3072 Bnn)
Tensor_35442: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30835: 2359296 Bnn)
Tensor_35443: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30837: 3072 Bnn)
Tensor_35444: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30839: 2359296 Bnn)
Tensor_35445: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30841: 3072 Bnn)
Tensor_35446: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30843: 2359296 Bnn)
Tensor_35447: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30845: 3072 Bnn)
Tensor_35448: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30847: 9437184 Bnn)
Tensor_35449: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30849: 3072 Bnn)
Tensor_35450: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30851: 9437184 Bnn)
Tensor_35451: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30853: 12288 Bnn)
Tensor_35452: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30855: 2359296 Bnn)
Tensor_35453: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30857: 3072 Bnn)
Tensor_35454: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30859: 2359296 Bnn)
Tensor_35455: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30861: 3072 Bnn)
Tensor_35456: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30863: 2359296 Bnn)
Tensor_35457: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30865: 3072 Bnn)
Tensor_35458: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30867: 2359296 Bnn)
Tensor_35459: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30869: 3072 Bnn)
Tensor_35460: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30871: 9437184 Bnn)
Tensor_35461: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30873: 3072 Bnn)
Tensor_35462: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30875: 9437184 Bnn)
Tensor_35463: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30877: 12288 Bnn)
Tensor_35464: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30879: 2359296 Bnn)
Tensor_35465: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30881: 3072 Bnn)
Tensor_35466: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30883: 2359296 Bnn)
Tensor_35467: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30885: 3072 Bnn)
Tensor_35468: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30887: 2359296 Bnn)
Tensor_35469: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30889: 3072 Bnn)
Tensor_35470: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30891: 2359296 Bnn)
Tensor_35471: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30893: 3072 Bnn)
Tensor_35472: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30895: 9437184 Bnn)
Tensor_35473: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30897: 3072 Bnn)
Tensor_35474: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30899: 9437184 Bnn)
Tensor_35475: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30901: 12288 Bnn)
Tensor_35476: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30903: 2359296 Bnn)
Tensor_35477: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30905: 3072 Bnn)
Tensor_35478: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30907: 2359296 Bnn)
Tensor_35479: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30909: 3072 Bnn)
Tensor_35480: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30911: 2359296 Bnn)
Tensor_35481: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30913: 3072 Bnn)
Tensor_35482: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30915: 2359296 Bnn)
Tensor_35483: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30917: 3072 Bnn)
Tensor_35484: shape = [768, 3072], stride = [3072, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30919: 9437184 Bnn)
Tensor_35485: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30921: 3072 Bnn)
Tensor_35486: shape = [3072, 768], stride = [768, 1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30923: 9437184 Bnn)
Tensor_35487: shape = [3072], stride = [1], offset = 0, size = 3072, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30925: 12288 Bnn)
Tensor_35488: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30927: 2359296 Bnn)
Tensor_35489: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30929: 3072 Bnn)
Tensor_35490: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30931: 2359296 Bnn)
Tensor_35491: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30933: 3072 Bnn)
Tensor_35492: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30935: 2359296 Bnn)
Tensor_35493: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30937: 3072 Bnn)
Tensor_35494: shape = [768, 768], stride = [768, 1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30939: 2359296 Bnn)
Tensor_35495: shape = [768], stride = [1], offset = 0, size = 768, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30941: 3072 Bnn)
Operation_33336: operator = NormForward, inputs = [Tensor_35422], outputs = [Tensor_35497], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_35497: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (35497: 4 Ban)
Operation_33338: operator = NormForward, inputs = [Tensor_35423], outputs = [Tensor_35499], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_35499: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (35499: 4 Ban)
Operation_33340: operator = ReshapeForward, inputs = [Tensor_35494], outputs = [Tensor_35501], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 589824 stride: 1]
Tensor_35501: shape = [589824], stride = [1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30939: 2359296 Bnn)
Operation_33342: operator = NormForward, inputs = [Tensor_35501], outputs = [Tensor_35503], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_35503: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (35503: 4 Ban)
Operation_33344: operator = NormForward, inputs = [Tensor_35495], outputs = [Tensor_35505], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_35505: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (35505: 4 Ban)
Operation_33346: operator = ReshapeForward, inputs = [Tensor_35490], outputs = [Tensor_35507], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 589824 stride: 1]
Tensor_35507: shape = [589824], stride = [1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30931: 2359296 Bnn)
Operation_33348: operator = NormForward, inputs = [Tensor_35507], outputs = [Tensor_35509], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_35509: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (35509: 4 Ban)
Operation_33350: operator = NormForward, inputs = [Tensor_35491], outputs = [Tensor_35511], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_35511: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (35511: 4 Ban)
Operation_33352: operator = ReshapeForward, inputs = [Tensor_35492], outputs = [Tensor_35513], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 589824 stride: 1]
Tensor_35513: shape = [589824], stride = [1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30935: 2359296 Bnn)
Operation_33354: operator = NormForward, inputs = [Tensor_35513], outputs = [Tensor_35515], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_35515: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (35515: 4 Ban)
Operation_33356: operator = NormForward, inputs = [Tensor_35493], outputs = [Tensor_35517], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_35517: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (35517: 4 Ban)
Operation_33358: operator = ReshapeForward, inputs = [Tensor_35488], outputs = [Tensor_35519], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 589824 stride: 1]
Tensor_35519: shape = [589824], stride = [1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30927: 2359296 Bnn)
Operation_33360: operator = NormForward, inputs = [Tensor_35519], outputs = [Tensor_35521], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_35521: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (35521: 4 Ban)
Operation_33362: operator = NormForward, inputs = [Tensor_35489], outputs = [Tensor_35523], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_35523: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (35523: 4 Ban)
Operation_33364: operator = NormForward, inputs = [Tensor_35418], outputs = [Tensor_35525], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_35525: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (35525: 4 Ban)
Operation_33366: operator = NormForward, inputs = [Tensor_35419], outputs = [Tensor_35527], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_35527: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (35527: 4 Ban)
Operation_33368: operator = NormForward, inputs = [Tensor_35420], outputs = [Tensor_35529], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_35529: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (35529: 4 Ban)
Operation_33370: operator = NormForward, inputs = [Tensor_35421], outputs = [Tensor_35531], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_35531: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (35531: 4 Ban)
Operation_33372: operator = ReshapeForward, inputs = [Tensor_35486], outputs = [Tensor_35533], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 2359296 stride: 1]
Tensor_35533: shape = [2359296], stride = [1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30923: 9437184 Bnn)
Operation_33374: operator = NormForward, inputs = [Tensor_35533], outputs = [Tensor_35535], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_35535: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (35535: 4 Ban)
Operation_33376: operator = NormForward, inputs = [Tensor_35487], outputs = [Tensor_35537], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_35537: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (35537: 4 Ban)
Operation_33378: operator = ReshapeForward, inputs = [Tensor_35484], outputs = [Tensor_35539], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 2359296 stride: 1]
Tensor_35539: shape = [2359296], stride = [1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30919: 9437184 Bnn)
Operation_33380: operator = NormForward, inputs = [Tensor_35539], outputs = [Tensor_35541], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_35541: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (35541: 4 Ban)
Operation_33382: operator = NormForward, inputs = [Tensor_35485], outputs = [Tensor_35543], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_35543: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (35543: 4 Ban)
Operation_33384: operator = NormForward, inputs = [Tensor_35414], outputs = [Tensor_35545], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_35545: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (35545: 4 Ban)
Operation_33386: operator = NormForward, inputs = [Tensor_35415], outputs = [Tensor_35547], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_35547: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (35547: 4 Ban)
Operation_33388: operator = NormForward, inputs = [Tensor_35416], outputs = [Tensor_35549], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_35549: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (35549: 4 Ban)
Operation_33390: operator = NormForward, inputs = [Tensor_35417], outputs = [Tensor_35551], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_35551: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (35551: 4 Ban)
Operation_33392: operator = ReshapeForward, inputs = [Tensor_35482], outputs = [Tensor_35553], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 589824 stride: 1]
Tensor_35553: shape = [589824], stride = [1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30915: 2359296 Bnn)
Operation_33394: operator = NormForward, inputs = [Tensor_35553], outputs = [Tensor_35555], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_35555: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (35555: 4 Ban)
Operation_33396: operator = NormForward, inputs = [Tensor_35483], outputs = [Tensor_35557], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_35557: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (35557: 4 Ban)
Operation_33398: operator = ReshapeForward, inputs = [Tensor_35478], outputs = [Tensor_35559], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 589824 stride: 1]
Tensor_35559: shape = [589824], stride = [1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30907: 2359296 Bnn)
Operation_33400: operator = NormForward, inputs = [Tensor_35559], outputs = [Tensor_35561], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_35561: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (35561: 4 Ban)
Operation_33402: operator = NormForward, inputs = [Tensor_35479], outputs = [Tensor_35563], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_35563: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (35563: 4 Ban)
Operation_33404: operator = ReshapeForward, inputs = [Tensor_35480], outputs = [Tensor_35565], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 589824 stride: 1]
Tensor_35565: shape = [589824], stride = [1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30911: 2359296 Bnn)
Operation_33406: operator = NormForward, inputs = [Tensor_35565], outputs = [Tensor_35567], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_35567: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (35567: 4 Ban)
Operation_33408: operator = NormForward, inputs = [Tensor_35481], outputs = [Tensor_35569], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_35569: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (35569: 4 Ban)
Operation_33410: operator = ReshapeForward, inputs = [Tensor_35476], outputs = [Tensor_35571], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 589824 stride: 1]
Tensor_35571: shape = [589824], stride = [1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30903: 2359296 Bnn)
Operation_33412: operator = NormForward, inputs = [Tensor_35571], outputs = [Tensor_35573], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_35573: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (35573: 4 Ban)
Operation_33414: operator = NormForward, inputs = [Tensor_35477], outputs = [Tensor_35575], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_35575: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (35575: 4 Ban)
Operation_33416: operator = NormForward, inputs = [Tensor_35410], outputs = [Tensor_35577], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_35577: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (35577: 4 Ban)
Operation_33418: operator = NormForward, inputs = [Tensor_35411], outputs = [Tensor_35579], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_35579: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (35579: 4 Ban)
Operation_33420: operator = NormForward, inputs = [Tensor_35412], outputs = [Tensor_35581], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_35581: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (35581: 4 Ban)
Operation_33422: operator = NormForward, inputs = [Tensor_35413], outputs = [Tensor_35583], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_35583: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (35583: 4 Ban)
Operation_33424: operator = ReshapeForward, inputs = [Tensor_35474], outputs = [Tensor_35585], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 2359296 stride: 1]
Tensor_35585: shape = [2359296], stride = [1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30899: 9437184 Bnn)
Operation_33426: operator = NormForward, inputs = [Tensor_35585], outputs = [Tensor_35587], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_35587: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (35587: 4 Ban)
Operation_33428: operator = NormForward, inputs = [Tensor_35475], outputs = [Tensor_35589], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_35589: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (35589: 4 Ban)
Operation_33430: operator = ReshapeForward, inputs = [Tensor_35472], outputs = [Tensor_35591], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 2359296 stride: 1]
Tensor_35591: shape = [2359296], stride = [1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30895: 9437184 Bnn)
Operation_33432: operator = NormForward, inputs = [Tensor_35591], outputs = [Tensor_35593], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_35593: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (35593: 4 Ban)
Operation_33434: operator = NormForward, inputs = [Tensor_35473], outputs = [Tensor_35595], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_35595: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (35595: 4 Ban)
Operation_33436: operator = NormForward, inputs = [Tensor_35406], outputs = [Tensor_35597], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_35597: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (35597: 4 Ban)
Operation_33438: operator = NormForward, inputs = [Tensor_35407], outputs = [Tensor_35599], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_35599: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (35599: 4 Ban)
Operation_33440: operator = NormForward, inputs = [Tensor_35408], outputs = [Tensor_35601], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_35601: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (35601: 4 Ban)
Operation_33442: operator = NormForward, inputs = [Tensor_35409], outputs = [Tensor_35603], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_35603: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (35603: 4 Ban)
Operation_33444: operator = ReshapeForward, inputs = [Tensor_35470], outputs = [Tensor_35605], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 589824 stride: 1]
Tensor_35605: shape = [589824], stride = [1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30891: 2359296 Bnn)
Operation_33446: operator = NormForward, inputs = [Tensor_35605], outputs = [Tensor_35607], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_35607: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (35607: 4 Ban)
Operation_33448: operator = NormForward, inputs = [Tensor_35471], outputs = [Tensor_35609], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_35609: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (35609: 4 Ban)
Operation_33450: operator = ReshapeForward, inputs = [Tensor_35466], outputs = [Tensor_35611], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 589824 stride: 1]
Tensor_35611: shape = [589824], stride = [1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30883: 2359296 Bnn)
Operation_33452: operator = NormForward, inputs = [Tensor_35611], outputs = [Tensor_35613], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_35613: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (35613: 4 Ban)
Operation_33454: operator = NormForward, inputs = [Tensor_35467], outputs = [Tensor_35615], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_35615: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (35615: 4 Ban)
Operation_33456: operator = ReshapeForward, inputs = [Tensor_35468], outputs = [Tensor_35617], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 589824 stride: 1]
Tensor_35617: shape = [589824], stride = [1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30887: 2359296 Bnn)
Operation_33458: operator = NormForward, inputs = [Tensor_35617], outputs = [Tensor_35619], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_35619: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (35619: 4 Ban)
Operation_33460: operator = NormForward, inputs = [Tensor_35469], outputs = [Tensor_35621], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_35621: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (35621: 4 Ban)
Operation_33462: operator = ReshapeForward, inputs = [Tensor_35464], outputs = [Tensor_35623], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 589824 stride: 1]
Tensor_35623: shape = [589824], stride = [1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30879: 2359296 Bnn)
Operation_33464: operator = NormForward, inputs = [Tensor_35623], outputs = [Tensor_35625], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_35625: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (35625: 4 Ban)
Operation_33466: operator = NormForward, inputs = [Tensor_35465], outputs = [Tensor_35627], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_35627: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (35627: 4 Ban)
Operation_33468: operator = NormForward, inputs = [Tensor_35402], outputs = [Tensor_35629], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_35629: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (35629: 4 Ban)
Operation_33470: operator = NormForward, inputs = [Tensor_35403], outputs = [Tensor_35631], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_35631: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (35631: 4 Ban)
Operation_33472: operator = NormForward, inputs = [Tensor_35404], outputs = [Tensor_35633], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_35633: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (35633: 4 Ban)
Operation_33474: operator = NormForward, inputs = [Tensor_35405], outputs = [Tensor_35635], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_35635: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (35635: 4 Ban)
Operation_33476: operator = ReshapeForward, inputs = [Tensor_35462], outputs = [Tensor_35637], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 2359296 stride: 1]
Tensor_35637: shape = [2359296], stride = [1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30875: 9437184 Bnn)
Operation_33478: operator = NormForward, inputs = [Tensor_35637], outputs = [Tensor_35639], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_35639: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (35639: 4 Ban)
Operation_33480: operator = NormForward, inputs = [Tensor_35463], outputs = [Tensor_35641], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_35641: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (35641: 4 Ban)
Operation_33482: operator = ReshapeForward, inputs = [Tensor_35460], outputs = [Tensor_35643], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 2359296 stride: 1]
Tensor_35643: shape = [2359296], stride = [1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30871: 9437184 Bnn)
Operation_33484: operator = NormForward, inputs = [Tensor_35643], outputs = [Tensor_35645], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_35645: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (35645: 4 Ban)
Operation_33486: operator = NormForward, inputs = [Tensor_35461], outputs = [Tensor_35647], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_35647: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (35647: 4 Ban)
Operation_33488: operator = NormForward, inputs = [Tensor_35398], outputs = [Tensor_35649], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_35649: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (35649: 4 Ban)
Operation_33490: operator = NormForward, inputs = [Tensor_35399], outputs = [Tensor_35651], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_35651: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (35651: 4 Ban)
Operation_33492: operator = NormForward, inputs = [Tensor_35400], outputs = [Tensor_35653], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_35653: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (35653: 4 Ban)
Operation_33494: operator = NormForward, inputs = [Tensor_35401], outputs = [Tensor_35655], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_35655: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (35655: 4 Ban)
Operation_33496: operator = ReshapeForward, inputs = [Tensor_35458], outputs = [Tensor_35657], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 589824 stride: 1]
Tensor_35657: shape = [589824], stride = [1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30867: 2359296 Bnn)
Operation_33498: operator = NormForward, inputs = [Tensor_35657], outputs = [Tensor_35659], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_35659: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (35659: 4 Ban)
Operation_33500: operator = NormForward, inputs = [Tensor_35459], outputs = [Tensor_35661], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_35661: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (35661: 4 Ban)
Operation_33502: operator = ReshapeForward, inputs = [Tensor_35454], outputs = [Tensor_35663], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 589824 stride: 1]
Tensor_35663: shape = [589824], stride = [1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30859: 2359296 Bnn)
Operation_33504: operator = NormForward, inputs = [Tensor_35663], outputs = [Tensor_35665], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_35665: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (35665: 4 Ban)
Operation_33506: operator = NormForward, inputs = [Tensor_35455], outputs = [Tensor_35667], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_35667: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (35667: 4 Ban)
Operation_33508: operator = ReshapeForward, inputs = [Tensor_35456], outputs = [Tensor_35669], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 589824 stride: 1]
Tensor_35669: shape = [589824], stride = [1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30863: 2359296 Bnn)
Operation_33510: operator = NormForward, inputs = [Tensor_35669], outputs = [Tensor_35671], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_35671: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (35671: 4 Ban)
Operation_33512: operator = NormForward, inputs = [Tensor_35457], outputs = [Tensor_35673], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_35673: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (35673: 4 Ban)
Operation_33514: operator = ReshapeForward, inputs = [Tensor_35452], outputs = [Tensor_35675], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 589824 stride: 1]
Tensor_35675: shape = [589824], stride = [1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30855: 2359296 Bnn)
Operation_33516: operator = NormForward, inputs = [Tensor_35675], outputs = [Tensor_35677], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_35677: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (35677: 4 Ban)
Operation_33518: operator = NormForward, inputs = [Tensor_35453], outputs = [Tensor_35679], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_35679: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (35679: 4 Ban)
Operation_33520: operator = NormForward, inputs = [Tensor_35394], outputs = [Tensor_35681], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_35681: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (35681: 4 Ban)
Operation_33522: operator = NormForward, inputs = [Tensor_35395], outputs = [Tensor_35683], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_35683: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (35683: 4 Ban)
Operation_33524: operator = NormForward, inputs = [Tensor_35396], outputs = [Tensor_35685], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_35685: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (35685: 4 Ban)
Operation_33526: operator = NormForward, inputs = [Tensor_35397], outputs = [Tensor_35687], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_35687: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (35687: 4 Ban)
Operation_33528: operator = ReshapeForward, inputs = [Tensor_35450], outputs = [Tensor_35689], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 2359296 stride: 1]
Tensor_35689: shape = [2359296], stride = [1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30851: 9437184 Bnn)
Operation_33530: operator = NormForward, inputs = [Tensor_35689], outputs = [Tensor_35691], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_35691: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (35691: 4 Ban)
Operation_33532: operator = NormForward, inputs = [Tensor_35451], outputs = [Tensor_35693], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_35693: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (35693: 4 Ban)
Operation_33534: operator = ReshapeForward, inputs = [Tensor_35448], outputs = [Tensor_35695], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 2359296 stride: 1]
Tensor_35695: shape = [2359296], stride = [1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30847: 9437184 Bnn)
Operation_33536: operator = NormForward, inputs = [Tensor_35695], outputs = [Tensor_35697], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_35697: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (35697: 4 Ban)
Operation_33538: operator = NormForward, inputs = [Tensor_35449], outputs = [Tensor_35699], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_35699: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (35699: 4 Ban)
Operation_33540: operator = NormForward, inputs = [Tensor_35390], outputs = [Tensor_35701], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_35701: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (35701: 4 Ban)
Operation_33542: operator = NormForward, inputs = [Tensor_35391], outputs = [Tensor_35703], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_35703: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (35703: 4 Ban)
Operation_33544: operator = NormForward, inputs = [Tensor_35392], outputs = [Tensor_35705], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_35705: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (35705: 4 Ban)
Operation_33546: operator = NormForward, inputs = [Tensor_35393], outputs = [Tensor_35707], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_35707: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (35707: 4 Ban)
Operation_33548: operator = ReshapeForward, inputs = [Tensor_35446], outputs = [Tensor_35709], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 589824 stride: 1]
Tensor_35709: shape = [589824], stride = [1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30843: 2359296 Bnn)
Operation_33550: operator = NormForward, inputs = [Tensor_35709], outputs = [Tensor_35711], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_35711: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (35711: 4 Ban)
Operation_33552: operator = NormForward, inputs = [Tensor_35447], outputs = [Tensor_35713], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_35713: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (35713: 4 Ban)
Operation_33554: operator = ReshapeForward, inputs = [Tensor_35442], outputs = [Tensor_35715], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 589824 stride: 1]
Tensor_35715: shape = [589824], stride = [1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30835: 2359296 Bnn)
Operation_33556: operator = NormForward, inputs = [Tensor_35715], outputs = [Tensor_35717], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_35717: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (35717: 4 Ban)
Operation_33558: operator = NormForward, inputs = [Tensor_35443], outputs = [Tensor_35719], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_35719: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (35719: 4 Ban)
Operation_33560: operator = ReshapeForward, inputs = [Tensor_35444], outputs = [Tensor_35721], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 589824 stride: 1]
Tensor_35721: shape = [589824], stride = [1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30839: 2359296 Bnn)
Operation_33562: operator = NormForward, inputs = [Tensor_35721], outputs = [Tensor_35723], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_35723: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (35723: 4 Ban)
Operation_33564: operator = NormForward, inputs = [Tensor_35445], outputs = [Tensor_35725], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_35725: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (35725: 4 Ban)
Operation_33566: operator = ReshapeForward, inputs = [Tensor_35440], outputs = [Tensor_35727], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 589824 stride: 1]
Tensor_35727: shape = [589824], stride = [1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30831: 2359296 Bnn)
Operation_33568: operator = NormForward, inputs = [Tensor_35727], outputs = [Tensor_35729], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_35729: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (35729: 4 Ban)
Operation_33570: operator = NormForward, inputs = [Tensor_35441], outputs = [Tensor_35731], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_35731: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (35731: 4 Ban)
Operation_33572: operator = NormForward, inputs = [Tensor_35386], outputs = [Tensor_35733], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_35733: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (35733: 4 Ban)
Operation_33574: operator = NormForward, inputs = [Tensor_35387], outputs = [Tensor_35735], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_35735: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (35735: 4 Ban)
Operation_33576: operator = NormForward, inputs = [Tensor_35388], outputs = [Tensor_35737], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_35737: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (35737: 4 Ban)
Operation_33578: operator = NormForward, inputs = [Tensor_35389], outputs = [Tensor_35739], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_35739: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (35739: 4 Ban)
Operation_33580: operator = ReshapeForward, inputs = [Tensor_35438], outputs = [Tensor_35741], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 2359296 stride: 1]
Tensor_35741: shape = [2359296], stride = [1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30827: 9437184 Bnn)
Operation_33582: operator = NormForward, inputs = [Tensor_35741], outputs = [Tensor_35743], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_35743: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (35743: 4 Ban)
Operation_33584: operator = NormForward, inputs = [Tensor_35439], outputs = [Tensor_35745], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_35745: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (35745: 4 Ban)
Operation_33586: operator = ReshapeForward, inputs = [Tensor_35436], outputs = [Tensor_35747], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 2359296 stride: 1]
Tensor_35747: shape = [2359296], stride = [1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30823: 9437184 Bnn)
Operation_33588: operator = NormForward, inputs = [Tensor_35747], outputs = [Tensor_35749], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_35749: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (35749: 4 Ban)
Operation_33590: operator = NormForward, inputs = [Tensor_35437], outputs = [Tensor_35751], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_35751: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (35751: 4 Ban)
Operation_33592: operator = NormForward, inputs = [Tensor_35382], outputs = [Tensor_35753], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_35753: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (35753: 4 Ban)
Operation_33594: operator = NormForward, inputs = [Tensor_35383], outputs = [Tensor_35755], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_35755: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (35755: 4 Ban)
Operation_33596: operator = NormForward, inputs = [Tensor_35384], outputs = [Tensor_35757], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_35757: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (35757: 4 Ban)
Operation_33598: operator = NormForward, inputs = [Tensor_35385], outputs = [Tensor_35759], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_35759: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (35759: 4 Ban)
Operation_33600: operator = ReshapeForward, inputs = [Tensor_35434], outputs = [Tensor_35761], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 589824 stride: 1]
Tensor_35761: shape = [589824], stride = [1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30821: 2359296 Bnn)
Operation_33602: operator = NormForward, inputs = [Tensor_35761], outputs = [Tensor_35763], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_35763: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (35763: 4 Ban)
Operation_33604: operator = NormForward, inputs = [Tensor_35435], outputs = [Tensor_35765], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_35765: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (35765: 4 Ban)
Operation_33606: operator = ReshapeForward, inputs = [Tensor_35430], outputs = [Tensor_35767], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 589824 stride: 1]
Tensor_35767: shape = [589824], stride = [1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30817: 2359296 Bnn)
Operation_33608: operator = NormForward, inputs = [Tensor_35767], outputs = [Tensor_35769], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_35769: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (35769: 4 Ban)
Operation_33610: operator = NormForward, inputs = [Tensor_35431], outputs = [Tensor_35771], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_35771: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (35771: 4 Ban)
Operation_33612: operator = ReshapeForward, inputs = [Tensor_35432], outputs = [Tensor_35773], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 589824 stride: 1]
Tensor_35773: shape = [589824], stride = [1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30819: 2359296 Bnn)
Operation_33614: operator = NormForward, inputs = [Tensor_35773], outputs = [Tensor_35775], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_35775: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (35775: 4 Ban)
Operation_33616: operator = NormForward, inputs = [Tensor_35433], outputs = [Tensor_35777], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_35777: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (35777: 4 Ban)
Operation_33618: operator = ReshapeForward, inputs = [Tensor_35428], outputs = [Tensor_35779], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 589824 stride: 1]
Tensor_35779: shape = [589824], stride = [1], offset = 0, size = 589824, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30815: 2359296 Bnn)
Operation_33620: operator = NormForward, inputs = [Tensor_35779], outputs = [Tensor_35781], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_35781: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (35781: 4 Ban)
Operation_33622: operator = NormForward, inputs = [Tensor_35429], outputs = [Tensor_35783], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_35783: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (35783: 4 Ban)
Operation_33624: operator = NormForward, inputs = [Tensor_35378], outputs = [Tensor_35785], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_35785: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (35785: 4 Ban)
Operation_33626: operator = NormForward, inputs = [Tensor_35379], outputs = [Tensor_35787], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_35787: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (35787: 4 Ban)
Operation_33628: operator = NormForward, inputs = [Tensor_35380], outputs = [Tensor_35789], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_35789: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (35789: 4 Ban)
Operation_33630: operator = NormForward, inputs = [Tensor_35381], outputs = [Tensor_35791], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_35791: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (35791: 4 Ban)
Operation_33632: operator = ReshapeForward, inputs = [Tensor_35426], outputs = [Tensor_35793], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 2359296 stride: 1]
Tensor_35793: shape = [2359296], stride = [1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30813: 9437184 Bnn)
Operation_33634: operator = NormForward, inputs = [Tensor_35793], outputs = [Tensor_35795], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_35795: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (35795: 4 Ban)
Operation_33636: operator = NormForward, inputs = [Tensor_35427], outputs = [Tensor_35797], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_35797: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (35797: 4 Ban)
Operation_33638: operator = ReshapeForward, inputs = [Tensor_35424], outputs = [Tensor_35799], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [shape: 2359296 stride: 1]
Tensor_35799: shape = [2359296], stride = [1], offset = 0, size = 2359296, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v0, t0, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (30811: 9437184 Bnn)
Operation_33640: operator = NormForward, inputs = [Tensor_35799], outputs = [Tensor_35801], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_35801: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (35801: 4 Ban)
Operation_33642: operator = NormForward, inputs = [Tensor_35425], outputs = [Tensor_35803], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_35803: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (35803: 4 Ban)
Operation_33644: operator = NormForward, inputs = [Tensor_35376], outputs = [Tensor_35805], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_35805: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (35805: 4 Ban)
Operation_33646: operator = NormForward, inputs = [Tensor_35377], outputs = [Tensor_35807], # backwardable inputs = 1, state = executable, attr<ExplicitSync> = ()
  - Operand: [mode: NUMBER_NORM dims: 0 p: 2]
Tensor_35807: shape = [], stride = [], offset = 0, size = 1, datatype = float32, contiguous? = true, complete? = true, dense? = true, ref_cnt = [v1, t1, o0], attr<AutogradTensor> = (leaf !requires_grad !retains_grad), attr<Distribution> = (dup), attr<Storage> = (35807: 4 Ban)

Move-in tensors (source -> target):
Tensor_14297 -> Tensor_35376
Tensor_14298 -> Tensor_35377
Tensor_14340 -> Tensor_35378
Tensor_14341 -> Tensor_35379
Tensor_14424 -> Tensor_35380
Tensor_14425 -> Tensor_35381
Tensor_14447 -> Tensor_35382
Tensor_14448 -> Tensor_35383
Tensor_14469 -> Tensor_35384
Tensor_14470 -> Tensor_35385
Tensor_14490 -> Tensor_35386
Tensor_14491 -> Tensor_35387
Tensor_14574 -> Tensor_35388
Tensor_14575 -> Tensor_35389
Tensor_14597 -> Tensor_35390
Tensor_14598 -> Tensor_35391
Tensor_14619 -> Tensor_35392
Tensor_14620 -> Tensor_35393
Tensor_14640 -> Tensor_35394
Tensor_14641 -> Tensor_35395
Tensor_14724 -> Tensor_35396
Tensor_14725 -> Tensor_35397
Tensor_14747 -> Tensor_35398
Tensor_14748 -> Tensor_35399
Tensor_14769 -> Tensor_35400
Tensor_14770 -> Tensor_35401
Tensor_14790 -> Tensor_35402
Tensor_14791 -> Tensor_35403
Tensor_14874 -> Tensor_35404
Tensor_14875 -> Tensor_35405
Tensor_14897 -> Tensor_35406
Tensor_14898 -> Tensor_35407
Tensor_14919 -> Tensor_35408
Tensor_14920 -> Tensor_35409
Tensor_14940 -> Tensor_35410
Tensor_14941 -> Tensor_35411
Tensor_15024 -> Tensor_35412
Tensor_15025 -> Tensor_35413
Tensor_15047 -> Tensor_35414
Tensor_15048 -> Tensor_35415
Tensor_15069 -> Tensor_35416
Tensor_15070 -> Tensor_35417
Tensor_15090 -> Tensor_35418
Tensor_15091 -> Tensor_35419
Tensor_15174 -> Tensor_35420
Tensor_15175 -> Tensor_35421
Tensor_15219 -> Tensor_35422
Tensor_15220 -> Tensor_35423
Tensor_15728 -> Tensor_35424
Tensor_15730 -> Tensor_35425
Tensor_15732 -> Tensor_35426
Tensor_15734 -> Tensor_35427
Tensor_15736 -> Tensor_35428
Tensor_15738 -> Tensor_35429
Tensor_15740 -> Tensor_35430
Tensor_15742 -> Tensor_35431
Tensor_15744 -> Tensor_35432
Tensor_15746 -> Tensor_35433
Tensor_15748 -> Tensor_35434
Tensor_15750 -> Tensor_35435
Tensor_15752 -> Tensor_35436
Tensor_15754 -> Tensor_35437
Tensor_15756 -> Tensor_35438
Tensor_15758 -> Tensor_35439
Tensor_15760 -> Tensor_35440
Tensor_15762 -> Tensor_35441
Tensor_15764 -> Tensor_35442
Tensor_15766 -> Tensor_35443
Tensor_15768 -> Tensor_35444
Tensor_15770 -> Tensor_35445
Tensor_15772 -> Tensor_35446
Tensor_15774 -> Tensor_35447
Tensor_15776 -> Tensor_35448
Tensor_15778 -> Tensor_35449
Tensor_15780 -> Tensor_35450
Tensor_15782 -> Tensor_35451
Tensor_15784 -> Tensor_35452
Tensor_15786 -> Tensor_35453
Tensor_15788 -> Tensor_35454
Tensor_15790 -> Tensor_35455
Tensor_15792 -> Tensor_35456
Tensor_15794 -> Tensor_35457
Tensor_15796 -> Tensor_35458
Tensor_15798 -> Tensor_35459
Tensor_15800 -> Tensor_35460
Tensor_15802 -> Tensor_35461
Tensor_15804 -> Tensor_35462
Tensor_15806 -> Tensor_35463
Tensor_15808 -> Tensor_35464
Tensor_15810 -> Tensor_35465
Tensor_15812 -> Tensor_35466
Tensor_15814 -> Tensor_35467
Tensor_15816 -> Tensor_35468
Tensor_15818 -> Tensor_35469
Tensor_15820 -> Tensor_35470
Tensor_15822 -> Tensor_35471
Tensor_15824 -> Tensor_35472
Tensor_15826 -> Tensor_35473
Tensor_15828 -> Tensor_35474
Tensor_15830 -> Tensor_35475
Tensor_15832 -> Tensor_35476
Tensor_15834 -> Tensor_35477
Tensor_15836 -> Tensor_35478
Tensor_15838 -> Tensor_35479
Tensor_15840 -> Tensor_35480
Tensor_15842 -> Tensor_35481
Tensor_15844 -> Tensor_35482
Tensor_15846 -> Tensor_35483
Tensor_15848 -> Tensor_35484
Tensor_15850 -> Tensor_35485
Tensor_15852 -> Tensor_35486
Tensor_15854 -> Tensor_35487
Tensor_15856 -> Tensor_35488
Tensor_15858 -> Tensor_35489
Tensor_15860 -> Tensor_35490
Tensor_15862 -> Tensor_35491
Tensor_15864 -> Tensor_35492
Tensor_15866 -> Tensor_35493
Tensor_15868 -> Tensor_35494
Tensor_15870 -> Tensor_35495

Move-out tensors (source <- target):
Tensor_14297 <- Tensor_35376
Tensor_14298 <- Tensor_35377
Tensor_14340 <- Tensor_35378
Tensor_14341 <- Tensor_35379
Tensor_14424 <- Tensor_35380
Tensor_14425 <- Tensor_35381
Tensor_14447 <- Tensor_35382
Tensor_14448 <- Tensor_35383
Tensor_14469 <- Tensor_35384
Tensor_14470 <- Tensor_35385
Tensor_14490 <- Tensor_35386
Tensor_14491 <- Tensor_35387
Tensor_14574 <- Tensor_35388
Tensor_14575 <- Tensor_35389
Tensor_14597 <- Tensor_35390
Tensor_14598 <- Tensor_35391
Tensor_14619 <- Tensor_35392
Tensor_14620 <- Tensor_35393
Tensor_14640 <- Tensor_35394
Tensor_14641 <- Tensor_35395
Tensor_14724 <- Tensor_35396
Tensor_14725 <- Tensor_35397
Tensor_14747 <- Tensor_35398
Tensor_14748 <- Tensor_35399
Tensor_14769 <- Tensor_35400
Tensor_14770 <- Tensor_35401
Tensor_14790 <- Tensor_35402
Tensor_14791 <- Tensor_35403
Tensor_14874 <- Tensor_35404
Tensor_14875 <- Tensor_35405
Tensor_14897 <- Tensor_35406
Tensor_14898 <- Tensor_35407
Tensor_14919 <- Tensor_35408
Tensor_14920 <- Tensor_35409
Tensor_14940 <- Tensor_35410
Tensor_14941 <- Tensor_35411
Tensor_15024 <- Tensor_35412
Tensor_15025 <- Tensor_35413
Tensor_15047 <- Tensor_35414
Tensor_15048 <- Tensor_35415
Tensor_15069 <- Tensor_35416
Tensor_15070 <- Tensor_35417
Tensor_15090 <- Tensor_35418
Tensor_15091 <- Tensor_35419
Tensor_15174 <- Tensor_35420
Tensor_15175 <- Tensor_35421
Tensor_15219 <- Tensor_35422
Tensor_15220 <- Tensor_35423
Tensor_15728 <- Tensor_35424
Tensor_15730 <- Tensor_35425
Tensor_15732 <- Tensor_35426
Tensor_15734 <- Tensor_35427
Tensor_15736 <- Tensor_35428
Tensor_15738 <- Tensor_35429
Tensor_15740 <- Tensor_35430
Tensor_15742 <- Tensor_35431
Tensor_15744 <- Tensor_35432
Tensor_15746 <- Tensor_35433
Tensor_15748 <- Tensor_35434
Tensor_15750 <- Tensor_35435
Tensor_15752 <- Tensor_35436
Tensor_15754 <- Tensor_35437
Tensor_15756 <- Tensor_35438
Tensor_15758 <- Tensor_35439
Tensor_15760 <- Tensor_35440
Tensor_15762 <- Tensor_35441
Tensor_15764 <- Tensor_35442
Tensor_15766 <- Tensor_35443
Tensor_15768 <- Tensor_35444
Tensor_15770 <- Tensor_35445
Tensor_15772 <- Tensor_35446
Tensor_15774 <- Tensor_35447
Tensor_15776 <- Tensor_35448
Tensor_15778 <- Tensor_35449
Tensor_15780 <- Tensor_35450
Tensor_15782 <- Tensor_35451
Tensor_15784 <- Tensor_35452
Tensor_15786 <- Tensor_35453
Tensor_15788 <- Tensor_35454
Tensor_15790 <- Tensor_35455
Tensor_15792 <- Tensor_35456
Tensor_15794 <- Tensor_35457
Tensor_15796 <- Tensor_35458
Tensor_15798 <- Tensor_35459
Tensor_15800 <- Tensor_35460
Tensor_15802 <- Tensor_35461
Tensor_15804 <- Tensor_35462
Tensor_15806 <- Tensor_35463
Tensor_15808 <- Tensor_35464
Tensor_15810 <- Tensor_35465
Tensor_15812 <- Tensor_35466
Tensor_15814 <- Tensor_35467
Tensor_15816 <- Tensor_35468
Tensor_15818 <- Tensor_35469
Tensor_15820 <- Tensor_35470
Tensor_15822 <- Tensor_35471
Tensor_15824 <- Tensor_35472
Tensor_15826 <- Tensor_35473
Tensor_15828 <- Tensor_35474
Tensor_15830 <- Tensor_35475
Tensor_15832 <- Tensor_35476
Tensor_15834 <- Tensor_35477
Tensor_15836 <- Tensor_35478
Tensor_15838 <- Tensor_35479
Tensor_15840 <- Tensor_35480
Tensor_15842 <- Tensor_35481
Tensor_15844 <- Tensor_35482
Tensor_15846 <- Tensor_35483
Tensor_15848 <- Tensor_35484
Tensor_15850 <- Tensor_35485
Tensor_15852 <- Tensor_35486
Tensor_15854 <- Tensor_35487
Tensor_15856 <- Tensor_35488
Tensor_15858 <- Tensor_35489
Tensor_15860 <- Tensor_35490
Tensor_15862 <- Tensor_35491
Tensor_15864 <- Tensor_35492
Tensor_15866 <- Tensor_35493
Tensor_15868 <- Tensor_35494
Tensor_15870 <- Tensor_35495
Tensor_33337 <- Tensor_35497
Tensor_33339 <- Tensor_35499
Tensor_33343 <- Tensor_35503
Tensor_33345 <- Tensor_35505
Tensor_33349 <- Tensor_35509
Tensor_33351 <- Tensor_35511
Tensor_33355 <- Tensor_35515
Tensor_33357 <- Tensor_35517
Tensor_33361 <- Tensor_35521
Tensor_33363 <- Tensor_35523
Tensor_33365 <- Tensor_35525
Tensor_33367 <- Tensor_35527
Tensor_33369 <- Tensor_35529
Tensor_33371 <- Tensor_35531
Tensor_33375 <- Tensor_35535
Tensor_33377 <- Tensor_35537
Tensor_33381 <- Tensor_35541
Tensor_33383 <- Tensor_35543
Tensor_33385 <- Tensor_35545
Tensor_33387 <- Tensor_35547
Tensor_33389 <- Tensor_35549
Tensor_33391 <- Tensor_35551
Tensor_33395 <- Tensor_35555
Tensor_33397 <- Tensor_35557
Tensor_33401 <- Tensor_35561
Tensor_33403 <- Tensor_35563
Tensor_33407 <- Tensor_35567
Tensor_33409 <- Tensor_35569
Tensor_33413 <- Tensor_35573
Tensor_33415 <- Tensor_35575
Tensor_33417 <- Tensor_35577
Tensor_33419 <- Tensor_35579
Tensor_33421 <- Tensor_35581
Tensor_33423 <- Tensor_35583
Tensor_33427 <- Tensor_35587
Tensor_33429 <- Tensor_35589
Tensor_33433 <- Tensor_35593
Tensor_33435 <- Tensor_35595
Tensor_33437 <- Tensor_35597
Tensor_33439 <- Tensor_35599
Tensor_33441 <- Tensor_35601
Tensor_33443 <- Tensor_35603
Tensor_33447 <- Tensor_35607
Tensor_33449 <- Tensor_35609
Tensor_33453 <- Tensor_35613
Tensor_33455 <- Tensor_35615
Tensor_33459 <- Tensor_35619
Tensor_33461 <- Tensor_35621
Tensor_33465 <- Tensor_35625
Tensor_33467 <- Tensor_35627
Tensor_33469 <- Tensor_35629
Tensor_33471 <- Tensor_35631
Tensor_33473 <- Tensor_35633
Tensor_33475 <- Tensor_35635
Tensor_33479 <- Tensor_35639
Tensor_33481 <- Tensor_35641
Tensor_33485 <- Tensor_35645
Tensor_33487 <- Tensor_35647
Tensor_33489 <- Tensor_35649
Tensor_33491 <- Tensor_35651
Tensor_33493 <- Tensor_35653
Tensor_33495 <- Tensor_35655
Tensor_33499 <- Tensor_35659
Tensor_33501 <- Tensor_35661
Tensor_33505 <- Tensor_35665
Tensor_33507 <- Tensor_35667
Tensor_33511 <- Tensor_35671
Tensor_33513 <- Tensor_35673
Tensor_33517 <- Tensor_35677
Tensor_33519 <- Tensor_35679
Tensor_33521 <- Tensor_35681
Tensor_33523 <- Tensor_35683
Tensor_33525 <- Tensor_35685
Tensor_33527 <- Tensor_35687
Tensor_33531 <- Tensor_35691
Tensor_33533 <- Tensor_35693
Tensor_33537 <- Tensor_35697
Tensor_33539 <- Tensor_35699
Tensor_33541 <- Tensor_35701
Tensor_33543 <- Tensor_35703
Tensor_33545 <- Tensor_35705
Tensor_33547 <- Tensor_35707
Tensor_33551 <- Tensor_35711
Tensor_33553 <- Tensor_35713
Tensor_33557 <- Tensor_35717
Tensor_33559 <- Tensor_35719
Tensor_33563 <- Tensor_35723
Tensor_33565 <- Tensor_35725
Tensor_33569 <- Tensor_35729
Tensor_33571 <- Tensor_35731
Tensor_33573 <- Tensor_35733
Tensor_33575 <- Tensor_35735
Tensor_33577 <- Tensor_35737
Tensor_33579 <- Tensor_35739
Tensor_33583 <- Tensor_35743
Tensor_33585 <- Tensor_35745
Tensor_33589 <- Tensor_35749
Tensor_33591 <- Tensor_35751
Tensor_33593 <- Tensor_35753
Tensor_33595 <- Tensor_35755
Tensor_33597 <- Tensor_35757
Tensor_33599 <- Tensor_35759
Tensor_33603 <- Tensor_35763
Tensor_33605 <- Tensor_35765
Tensor_33609 <- Tensor_35769
Tensor_33611 <- Tensor_35771
Tensor_33615 <- Tensor_35775
Tensor_33617 <- Tensor_35777
Tensor_33621 <- Tensor_35781
Tensor_33623 <- Tensor_35783
Tensor_33625 <- Tensor_35785
Tensor_33627 <- Tensor_35787
Tensor_33629 <- Tensor_35789
Tensor_33631 <- Tensor_35791
Tensor_33635 <- Tensor_35795
Tensor_33637 <- Tensor_35797
Tensor_33641 <- Tensor_35801
Tensor_33643 <- Tensor_35803
Tensor_33645 <- Tensor_35805
Tensor_33647 <- Tensor_35807

