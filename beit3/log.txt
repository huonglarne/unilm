/nas/.conda/envs/hf/lib/python3.8/site-packages/torch/_M/driver/common/utils.py:2091: UserWarning: torch.triangular_solve is deprecated in favor of torch.linalg.solve_triangularand will be removed in a future PyTorch release.
torch.linalg.solve_triangular has its arguments reversed and does not return a copy of one of the inputs.
X = torch.triangular_solve(B, A).solution
should be replaced with
X = torch.linalg.solve_triangular(A, B). (Triggered internally at ../aten/src/ATen/native/BatchLinearAlgebra.cpp:2115.)
  'triangular_solve': type(dummy_tensor.triangular_solve(dummy_tensor)),
2023-07-28 17:17:01,259 - torch.distributed.nn.jit.instantiator - INFO - Created a temporary directory at /tmp/tmpy3is01zd
2023-07-28 17:17:01,259 - torch.distributed.nn.jit.instantiator - INFO - Writing /tmp/tmpy3is01zd/_remote_module_non_scriptable.py
[2023-07-28 17:17:17.614] [info] Requesting resources for KT AI Accelerator from the server...
[2023-07-28 17:17:18.634] [info] Initializing the worker daemon for KT AI Accelerator
[2023-07-28 17:17:20.302] [info] [1/1] Connecting to resources on the server (192.168.110.13:24162)...
[2023-07-28 17:17:20.313] [info] Establishing links to the resources...
[2023-07-28 17:17:20.458] [info] KT AI Accelerator is ready to use.
2023-07-28 17:17:21,570 - xformers - WARNING - WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:
    PyTorch 2.0.1+cu118 with CUDA 1108 (you have 1.13.1+cu116.moreh23.7.0)
    Python  3.8.16 (you have 3.8.16)
  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)
  Memory-efficient attention, SwiGLU, sparse and more won't be available.
  Set XFORMERS_MORE_DETAILS=1 for more details
2023-07-28 17:17:22,227 - xformers - WARNING - Blocksparse is not available: the current GPU does not expose Tensor cores
Not using distributed mode
Namespace(aa='rand-m9-mstd0.5-inc1', auto_resume=True, batch_size=128, captioning_mask_prob=0.6, checkpoint_activations=None, clip_grad=None, color_jitter=0.4, crop_pct=None, cutmix=1.0, cutmix_minmax=None, data_path='/nas/common_data/imagenet_100cls', device='cuda', dist_eval=True, dist_on_itp=False, dist_url='env://', distributed=False, drop_path=0.15, drop_worst_after=12000, drop_worst_ratio=0.2, enable_deepspeed=False, epochs=50, eval=False, eval_batch_size=None, finetune='model/beit3_base_patch16_224_in1k.pth', initial_scale_power=16, input_size=224, label_smoothing=0.1, layer_decay=0.65, length_penalty=0.6, local_rank=-1, log_dir='./log', lr=0.0007, min_lr=1e-06, mixup=0.8, mixup_mode='batch', mixup_prob=1.0, mixup_switch_prob=0.5, model='beit3_base_patch16_224', model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, model_key='model|module', model_prefix='', momentum=0.9, nb_classes=1000, num_beams=3, num_max_bpe_tokens=64, num_workers=10, opt='adamw', opt_betas=[0.9, 0.999], opt_eps=1e-08, output_dir='./model', pin_mem=True, randaug=False, recount=1, remode='pixel', reprob=0.25, resplit=False, resume='', save_ckpt=True, save_ckpt_freq=5, seed=42, sentencepiece_model='model/beit3.spm', smoothing=0.1, start_epoch=0, task='imagenet', task_cache_path='./model', task_head_lr_weight=0, train_interpolation='bicubic', update_freq=1, vocab_size=64010, warmup_epochs=5, warmup_lr=1e-06, warmup_steps=-1, weight_decay=0.05, world_size=1, zero_stage=0)
Load 129395 image-text pairs from /nas/common_data/imagenet_100cls/imagenet.train.index.jsonl. 
Load 5000 image-text pairs from /nas/common_data/imagenet_100cls/imagenet.val.index.jsonl. 
model_config = beit3_base_patch16_224_imageclassification
Load ckpt from model/beit3_base_patch16_224_in1k.pth
Load state_dict by model_key = model
Model = BEiT3ForImageClassification(
  (beit3): BEiT3(
    (text_embed): TextEmbedding(64010, 768)
    (vision_embed): VisionEmbedding(
      (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
    )
    (encoder): Encoder(
      (dropout_module): Dropout(p=0.0, inplace=False)
      (embed_positions): MutliwayEmbedding(
        (A): PositionalEmbedding(199, 768)
        (B): PositionalEmbedding(1024, 768)
      )
      (layers): ModuleList(
        (0): EncoderLayer(
          (self_attn): MultiheadAttention(
            (k_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (v_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (q_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (out_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (inner_attn_ln): MultiwayNetwork(
              (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (dropout_module): Dropout(p=0.0, inplace=False)
          )
          (self_attn_layer_norm): MultiwayNetwork(
            (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (dropout_module): Dropout(p=0.0, inplace=False)
          (drop_path): DropPath(p=0.0)
          (ffn): MultiwayNetwork(
            (A): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.0, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
            )
            (B): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.0, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
            )
          )
          (final_layer_norm): MultiwayNetwork(
            (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
        (1): EncoderLayer(
          (self_attn): MultiheadAttention(
            (k_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (v_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (q_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (out_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (inner_attn_ln): MultiwayNetwork(
              (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (dropout_module): Dropout(p=0.0, inplace=False)
          )
          (self_attn_layer_norm): MultiwayNetwork(
            (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (dropout_module): Dropout(p=0.0, inplace=False)
          (drop_path): DropPath(p=0.013636363636363636)
          (ffn): MultiwayNetwork(
            (A): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.0, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
            )
            (B): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.0, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
            )
          )
          (final_layer_norm): MultiwayNetwork(
            (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
        (2): EncoderLayer(
          (self_attn): MultiheadAttention(
            (k_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (v_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (q_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (out_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (inner_attn_ln): MultiwayNetwork(
              (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (dropout_module): Dropout(p=0.0, inplace=False)
          )
          (self_attn_layer_norm): MultiwayNetwork(
            (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (dropout_module): Dropout(p=0.0, inplace=False)
          (drop_path): DropPath(p=0.02727272727272727)
          (ffn): MultiwayNetwork(
            (A): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.0, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
            )
            (B): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.0, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
            )
          )
          (final_layer_norm): MultiwayNetwork(
            (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
        (3): EncoderLayer(
          (self_attn): MultiheadAttention(
            (k_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (v_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (q_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (out_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (inner_attn_ln): MultiwayNetwork(
              (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (dropout_module): Dropout(p=0.0, inplace=False)
          )
          (self_attn_layer_norm): MultiwayNetwork(
            (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (dropout_module): Dropout(p=0.0, inplace=False)
          (drop_path): DropPath(p=0.04090909090909091)
          (ffn): MultiwayNetwork(
            (A): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.0, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
            )
            (B): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.0, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
            )
          )
          (final_layer_norm): MultiwayNetwork(
            (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
        (4): EncoderLayer(
          (self_attn): MultiheadAttention(
            (k_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (v_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (q_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (out_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (inner_attn_ln): MultiwayNetwork(
              (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (dropout_module): Dropout(p=0.0, inplace=False)
          )
          (self_attn_layer_norm): MultiwayNetwork(
            (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (dropout_module): Dropout(p=0.0, inplace=False)
          (drop_path): DropPath(p=0.05454545454545454)
          (ffn): MultiwayNetwork(
            (A): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.0, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
            )
            (B): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.0, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
            )
          )
          (final_layer_norm): MultiwayNetwork(
            (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
        (5): EncoderLayer(
          (self_attn): MultiheadAttention(
            (k_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (v_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (q_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (out_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (inner_attn_ln): MultiwayNetwork(
              (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (dropout_module): Dropout(p=0.0, inplace=False)
          )
          (self_attn_layer_norm): MultiwayNetwork(
            (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (dropout_module): Dropout(p=0.0, inplace=False)
          (drop_path): DropPath(p=0.06818181818181818)
          (ffn): MultiwayNetwork(
            (A): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.0, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
            )
            (B): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.0, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
            )
          )
          (final_layer_norm): MultiwayNetwork(
            (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
        (6): EncoderLayer(
          (self_attn): MultiheadAttention(
            (k_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (v_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (q_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (out_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (inner_attn_ln): MultiwayNetwork(
              (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (dropout_module): Dropout(p=0.0, inplace=False)
          )
          (self_attn_layer_norm): MultiwayNetwork(
            (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (dropout_module): Dropout(p=0.0, inplace=False)
          (drop_path): DropPath(p=0.08181818181818182)
          (ffn): MultiwayNetwork(
            (A): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.0, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
            )
            (B): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.0, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
            )
          )
          (final_layer_norm): MultiwayNetwork(
            (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
        (7): EncoderLayer(
          (self_attn): MultiheadAttention(
            (k_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (v_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (q_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (out_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (inner_attn_ln): MultiwayNetwork(
              (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (dropout_module): Dropout(p=0.0, inplace=False)
          )
          (self_attn_layer_norm): MultiwayNetwork(
            (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (dropout_module): Dropout(p=0.0, inplace=False)
          (drop_path): DropPath(p=0.09545454545454544)
          (ffn): MultiwayNetwork(
            (A): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.0, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
            )
            (B): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.0, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
            )
          )
          (final_layer_norm): MultiwayNetwork(
            (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
        (8): EncoderLayer(
          (self_attn): MultiheadAttention(
            (k_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (v_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (q_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (out_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (inner_attn_ln): MultiwayNetwork(
              (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (dropout_module): Dropout(p=0.0, inplace=False)
          )
          (self_attn_layer_norm): MultiwayNetwork(
            (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (dropout_module): Dropout(p=0.0, inplace=False)
          (drop_path): DropPath(p=0.10909090909090909)
          (ffn): MultiwayNetwork(
            (A): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.0, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
            )
            (B): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.0, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
            )
          )
          (final_layer_norm): MultiwayNetwork(
            (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
        (9): EncoderLayer(
          (self_attn): MultiheadAttention(
            (k_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (v_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (q_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (out_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (inner_attn_ln): MultiwayNetwork(
              (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (dropout_module): Dropout(p=0.0, inplace=False)
          )
          (self_attn_layer_norm): MultiwayNetwork(
            (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (dropout_module): Dropout(p=0.0, inplace=False)
          (drop_path): DropPath(p=0.12272727272727273)
          (ffn): MultiwayNetwork(
            (A): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.0, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
            )
            (B): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.0, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
            )
          )
          (final_layer_norm): MultiwayNetwork(
            (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
        (10): EncoderLayer(
          (self_attn): MultiheadAttention(
            (k_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (v_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (q_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (out_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (inner_attn_ln): MultiwayNetwork(
              (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (dropout_module): Dropout(p=0.0, inplace=False)
          )
          (self_attn_layer_norm): MultiwayNetwork(
            (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (dropout_module): Dropout(p=0.0, inplace=False)
          (drop_path): DropPath(p=0.13636363636363635)
          (ffn): MultiwayNetwork(
            (A): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.0, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
            )
            (B): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.0, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
            )
          )
          (final_layer_norm): MultiwayNetwork(
            (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
        (11): EncoderLayer(
          (self_attn): MultiheadAttention(
            (k_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (v_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (q_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (out_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (inner_attn_ln): MultiwayNetwork(
              (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (dropout_module): Dropout(p=0.0, inplace=False)
          )
          (self_attn_layer_norm): MultiwayNetwork(
            (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (dropout_module): Dropout(p=0.0, inplace=False)
          (drop_path): DropPath(p=0.15)
          (ffn): MultiwayNetwork(
            (A): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.0, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
            )
            (B): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.0, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
            )
          )
          (final_layer_norm): MultiwayNetwork(
            (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
  )
  (fc_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (head): Linear(in_features=768, out_features=1000, bias=True)
)
number of params: 221754856
LR = 0.00070000
Batch size = 128
Update frequent = 1
Number of training examples = 129395
Number of training training per epoch = 1010
Assigned values = [0.003697205891018715, 0.005688009063105715, 0.008750783174008792, 0.013462743344628911, 0.02071191283789063, 0.03186448128906251, 0.049022278906250015, 0.07541889062500001, 0.11602906250000002, 0.17850625000000003, 0.274625, 0.42250000000000004, 0.65, 1.0]
Param groups = {
  "layer_0_decay": {
    "weight_decay": 0.05,
    "params": [
      "beit3.text_embed.weight",
      "beit3.vision_embed.mask_token",
      "beit3.vision_embed.proj.weight",
      "beit3.encoder.embed_positions.B.weight"
    ],
    "lr_scale": 0.003697205891018715
  },
  "layer_0_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "beit3.vision_embed.cls_token",
      "beit3.vision_embed.proj.bias",
      "beit3.encoder.embed_positions.A.weight"
    ],
    "lr_scale": 0.003697205891018715
  },
  "layer_1_decay": {
    "weight_decay": 0.05,
    "params": [
      "beit3.encoder.layers.0.self_attn.k_proj.A.weight",
      "beit3.encoder.layers.0.self_attn.k_proj.B.weight",
      "beit3.encoder.layers.0.self_attn.v_proj.A.weight",
      "beit3.encoder.layers.0.self_attn.v_proj.B.weight",
      "beit3.encoder.layers.0.self_attn.q_proj.A.weight",
      "beit3.encoder.layers.0.self_attn.q_proj.B.weight",
      "beit3.encoder.layers.0.self_attn.out_proj.A.weight",
      "beit3.encoder.layers.0.self_attn.out_proj.B.weight",
      "beit3.encoder.layers.0.ffn.A.fc1.weight",
      "beit3.encoder.layers.0.ffn.A.fc2.weight",
      "beit3.encoder.layers.0.ffn.B.fc1.weight",
      "beit3.encoder.layers.0.ffn.B.fc2.weight"
    ],
    "lr_scale": 0.005688009063105715
  },
  "layer_1_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "beit3.encoder.layers.0.self_attn.k_proj.A.bias",
      "beit3.encoder.layers.0.self_attn.k_proj.B.bias",
      "beit3.encoder.layers.0.self_attn.v_proj.A.bias",
      "beit3.encoder.layers.0.self_attn.v_proj.B.bias",
      "beit3.encoder.layers.0.self_attn.q_proj.A.bias",
      "beit3.encoder.layers.0.self_attn.q_proj.B.bias",
      "beit3.encoder.layers.0.self_attn.out_proj.A.bias",
      "beit3.encoder.layers.0.self_attn.out_proj.B.bias",
      "beit3.encoder.layers.0.self_attn.inner_attn_ln.A.weight",
      "beit3.encoder.layers.0.self_attn.inner_attn_ln.A.bias",
      "beit3.encoder.layers.0.self_attn.inner_attn_ln.B.weight",
      "beit3.encoder.layers.0.self_attn.inner_attn_ln.B.bias",
      "beit3.encoder.layers.0.self_attn_layer_norm.A.weight",
      "beit3.encoder.layers.0.self_attn_layer_norm.A.bias",
      "beit3.encoder.layers.0.self_attn_layer_norm.B.weight",
      "beit3.encoder.layers.0.self_attn_layer_norm.B.bias",
      "beit3.encoder.layers.0.ffn.A.fc1.bias",
      "beit3.encoder.layers.0.ffn.A.fc2.bias",
      "beit3.encoder.layers.0.ffn.A.ffn_layernorm.weight",
      "beit3.encoder.layers.0.ffn.A.ffn_layernorm.bias",
      "beit3.encoder.layers.0.ffn.B.fc1.bias",
      "beit3.encoder.layers.0.ffn.B.fc2.bias",
      "beit3.encoder.layers.0.ffn.B.ffn_layernorm.weight",
      "beit3.encoder.layers.0.ffn.B.ffn_layernorm.bias",
      "beit3.encoder.layers.0.final_layer_norm.A.weight",
      "beit3.encoder.layers.0.final_layer_norm.A.bias",
      "beit3.encoder.layers.0.final_layer_norm.B.weight",
      "beit3.encoder.layers.0.final_layer_norm.B.bias"
    ],
    "lr_scale": 0.005688009063105715
  },
  "layer_2_decay": {
    "weight_decay": 0.05,
    "params": [
      "beit3.encoder.layers.1.self_attn.k_proj.A.weight",
      "beit3.encoder.layers.1.self_attn.k_proj.B.weight",
      "beit3.encoder.layers.1.self_attn.v_proj.A.weight",
      "beit3.encoder.layers.1.self_attn.v_proj.B.weight",
      "beit3.encoder.layers.1.self_attn.q_proj.A.weight",
      "beit3.encoder.layers.1.self_attn.q_proj.B.weight",
      "beit3.encoder.layers.1.self_attn.out_proj.A.weight",
      "beit3.encoder.layers.1.self_attn.out_proj.B.weight",
      "beit3.encoder.layers.1.ffn.A.fc1.weight",
      "beit3.encoder.layers.1.ffn.A.fc2.weight",
      "beit3.encoder.layers.1.ffn.B.fc1.weight",
      "beit3.encoder.layers.1.ffn.B.fc2.weight"
    ],
    "lr_scale": 0.008750783174008792
  },
  "layer_2_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "beit3.encoder.layers.1.self_attn.k_proj.A.bias",
      "beit3.encoder.layers.1.self_attn.k_proj.B.bias",
      "beit3.encoder.layers.1.self_attn.v_proj.A.bias",
      "beit3.encoder.layers.1.self_attn.v_proj.B.bias",
      "beit3.encoder.layers.1.self_attn.q_proj.A.bias",
      "beit3.encoder.layers.1.self_attn.q_proj.B.bias",
      "beit3.encoder.layers.1.self_attn.out_proj.A.bias",
      "beit3.encoder.layers.1.self_attn.out_proj.B.bias",
      "beit3.encoder.layers.1.self_attn.inner_attn_ln.A.weight",
      "beit3.encoder.layers.1.self_attn.inner_attn_ln.A.bias",
      "beit3.encoder.layers.1.self_attn.inner_attn_ln.B.weight",
      "beit3.encoder.layers.1.self_attn.inner_attn_ln.B.bias",
      "beit3.encoder.layers.1.self_attn_layer_norm.A.weight",
      "beit3.encoder.layers.1.self_attn_layer_norm.A.bias",
      "beit3.encoder.layers.1.self_attn_layer_norm.B.weight",
      "beit3.encoder.layers.1.self_attn_layer_norm.B.bias",
      "beit3.encoder.layers.1.ffn.A.fc1.bias",
      "beit3.encoder.layers.1.ffn.A.fc2.bias",
      "beit3.encoder.layers.1.ffn.A.ffn_layernorm.weight",
      "beit3.encoder.layers.1.ffn.A.ffn_layernorm.bias",
      "beit3.encoder.layers.1.ffn.B.fc1.bias",
      "beit3.encoder.layers.1.ffn.B.fc2.bias",
      "beit3.encoder.layers.1.ffn.B.ffn_layernorm.weight",
      "beit3.encoder.layers.1.ffn.B.ffn_layernorm.bias",
      "beit3.encoder.layers.1.final_layer_norm.A.weight",
      "beit3.encoder.layers.1.final_layer_norm.A.bias",
      "beit3.encoder.layers.1.final_layer_norm.B.weight",
      "beit3.encoder.layers.1.final_layer_norm.B.bias"
    ],
    "lr_scale": 0.008750783174008792
  },
  "layer_3_decay": {
    "weight_decay": 0.05,
    "params": [
      "beit3.encoder.layers.2.self_attn.k_proj.A.weight",
      "beit3.encoder.layers.2.self_attn.k_proj.B.weight",
      "beit3.encoder.layers.2.self_attn.v_proj.A.weight",
      "beit3.encoder.layers.2.self_attn.v_proj.B.weight",
      "beit3.encoder.layers.2.self_attn.q_proj.A.weight",
      "beit3.encoder.layers.2.self_attn.q_proj.B.weight",
      "beit3.encoder.layers.2.self_attn.out_proj.A.weight",
      "beit3.encoder.layers.2.self_attn.out_proj.B.weight",
      "beit3.encoder.layers.2.ffn.A.fc1.weight",
      "beit3.encoder.layers.2.ffn.A.fc2.weight",
      "beit3.encoder.layers.2.ffn.B.fc1.weight",
      "beit3.encoder.layers.2.ffn.B.fc2.weight"
    ],
    "lr_scale": 0.013462743344628911
  },
  "layer_3_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "beit3.encoder.layers.2.self_attn.k_proj.A.bias",
      "beit3.encoder.layers.2.self_attn.k_proj.B.bias",
      "beit3.encoder.layers.2.self_attn.v_proj.A.bias",
      "beit3.encoder.layers.2.self_attn.v_proj.B.bias",
      "beit3.encoder.layers.2.self_attn.q_proj.A.bias",
      "beit3.encoder.layers.2.self_attn.q_proj.B.bias",
      "beit3.encoder.layers.2.self_attn.out_proj.A.bias",
      "beit3.encoder.layers.2.self_attn.out_proj.B.bias",
      "beit3.encoder.layers.2.self_attn.inner_attn_ln.A.weight",
      "beit3.encoder.layers.2.self_attn.inner_attn_ln.A.bias",
      "beit3.encoder.layers.2.self_attn.inner_attn_ln.B.weight",
      "beit3.encoder.layers.2.self_attn.inner_attn_ln.B.bias",
      "beit3.encoder.layers.2.self_attn_layer_norm.A.weight",
      "beit3.encoder.layers.2.self_attn_layer_norm.A.bias",
      "beit3.encoder.layers.2.self_attn_layer_norm.B.weight",
      "beit3.encoder.layers.2.self_attn_layer_norm.B.bias",
      "beit3.encoder.layers.2.ffn.A.fc1.bias",
      "beit3.encoder.layers.2.ffn.A.fc2.bias",
      "beit3.encoder.layers.2.ffn.A.ffn_layernorm.weight",
      "beit3.encoder.layers.2.ffn.A.ffn_layernorm.bias",
      "beit3.encoder.layers.2.ffn.B.fc1.bias",
      "beit3.encoder.layers.2.ffn.B.fc2.bias",
      "beit3.encoder.layers.2.ffn.B.ffn_layernorm.weight",
      "beit3.encoder.layers.2.ffn.B.ffn_layernorm.bias",
      "beit3.encoder.layers.2.final_layer_norm.A.weight",
      "beit3.encoder.layers.2.final_layer_norm.A.bias",
      "beit3.encoder.layers.2.final_layer_norm.B.weight",
      "beit3.encoder.layers.2.final_layer_norm.B.bias"
    ],
    "lr_scale": 0.013462743344628911
  },
  "layer_4_decay": {
    "weight_decay": 0.05,
    "params": [
      "beit3.encoder.layers.3.self_attn.k_proj.A.weight",
      "beit3.encoder.layers.3.self_attn.k_proj.B.weight",
      "beit3.encoder.layers.3.self_attn.v_proj.A.weight",
      "beit3.encoder.layers.3.self_attn.v_proj.B.weight",
      "beit3.encoder.layers.3.self_attn.q_proj.A.weight",
      "beit3.encoder.layers.3.self_attn.q_proj.B.weight",
      "beit3.encoder.layers.3.self_attn.out_proj.A.weight",
      "beit3.encoder.layers.3.self_attn.out_proj.B.weight",
      "beit3.encoder.layers.3.ffn.A.fc1.weight",
      "beit3.encoder.layers.3.ffn.A.fc2.weight",
      "beit3.encoder.layers.3.ffn.B.fc1.weight",
      "beit3.encoder.layers.3.ffn.B.fc2.weight"
    ],
    "lr_scale": 0.02071191283789063
  },
  "layer_4_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "beit3.encoder.layers.3.self_attn.k_proj.A.bias",
      "beit3.encoder.layers.3.self_attn.k_proj.B.bias",
      "beit3.encoder.layers.3.self_attn.v_proj.A.bias",
      "beit3.encoder.layers.3.self_attn.v_proj.B.bias",
      "beit3.encoder.layers.3.self_attn.q_proj.A.bias",
      "beit3.encoder.layers.3.self_attn.q_proj.B.bias",
      "beit3.encoder.layers.3.self_attn.out_proj.A.bias",
      "beit3.encoder.layers.3.self_attn.out_proj.B.bias",
      "beit3.encoder.layers.3.self_attn.inner_attn_ln.A.weight",
      "beit3.encoder.layers.3.self_attn.inner_attn_ln.A.bias",
      "beit3.encoder.layers.3.self_attn.inner_attn_ln.B.weight",
      "beit3.encoder.layers.3.self_attn.inner_attn_ln.B.bias",
      "beit3.encoder.layers.3.self_attn_layer_norm.A.weight",
      "beit3.encoder.layers.3.self_attn_layer_norm.A.bias",
      "beit3.encoder.layers.3.self_attn_layer_norm.B.weight",
      "beit3.encoder.layers.3.self_attn_layer_norm.B.bias",
      "beit3.encoder.layers.3.ffn.A.fc1.bias",
      "beit3.encoder.layers.3.ffn.A.fc2.bias",
      "beit3.encoder.layers.3.ffn.A.ffn_layernorm.weight",
      "beit3.encoder.layers.3.ffn.A.ffn_layernorm.bias",
      "beit3.encoder.layers.3.ffn.B.fc1.bias",
      "beit3.encoder.layers.3.ffn.B.fc2.bias",
      "beit3.encoder.layers.3.ffn.B.ffn_layernorm.weight",
      "beit3.encoder.layers.3.ffn.B.ffn_layernorm.bias",
      "beit3.encoder.layers.3.final_layer_norm.A.weight",
      "beit3.encoder.layers.3.final_layer_norm.A.bias",
      "beit3.encoder.layers.3.final_layer_norm.B.weight",
      "beit3.encoder.layers.3.final_layer_norm.B.bias"
    ],
    "lr_scale": 0.02071191283789063
  },
  "layer_5_decay": {
    "weight_decay": 0.05,
    "params": [
      "beit3.encoder.layers.4.self_attn.k_proj.A.weight",
      "beit3.encoder.layers.4.self_attn.k_proj.B.weight",
      "beit3.encoder.layers.4.self_attn.v_proj.A.weight",
      "beit3.encoder.layers.4.self_attn.v_proj.B.weight",
      "beit3.encoder.layers.4.self_attn.q_proj.A.weight",
      "beit3.encoder.layers.4.self_attn.q_proj.B.weight",
      "beit3.encoder.layers.4.self_attn.out_proj.A.weight",
      "beit3.encoder.layers.4.self_attn.out_proj.B.weight",
      "beit3.encoder.layers.4.ffn.A.fc1.weight",
      "beit3.encoder.layers.4.ffn.A.fc2.weight",
      "beit3.encoder.layers.4.ffn.B.fc1.weight",
      "beit3.encoder.layers.4.ffn.B.fc2.weight"
    ],
    "lr_scale": 0.03186448128906251
  },
  "layer_5_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "beit3.encoder.layers.4.self_attn.k_proj.A.bias",
      "beit3.encoder.layers.4.self_attn.k_proj.B.bias",
      "beit3.encoder.layers.4.self_attn.v_proj.A.bias",
      "beit3.encoder.layers.4.self_attn.v_proj.B.bias",
      "beit3.encoder.layers.4.self_attn.q_proj.A.bias",
      "beit3.encoder.layers.4.self_attn.q_proj.B.bias",
      "beit3.encoder.layers.4.self_attn.out_proj.A.bias",
      "beit3.encoder.layers.4.self_attn.out_proj.B.bias",
      "beit3.encoder.layers.4.self_attn.inner_attn_ln.A.weight",
      "beit3.encoder.layers.4.self_attn.inner_attn_ln.A.bias",
      "beit3.encoder.layers.4.self_attn.inner_attn_ln.B.weight",
      "beit3.encoder.layers.4.self_attn.inner_attn_ln.B.bias",
      "beit3.encoder.layers.4.self_attn_layer_norm.A.weight",
      "beit3.encoder.layers.4.self_attn_layer_norm.A.bias",
      "beit3.encoder.layers.4.self_attn_layer_norm.B.weight",
      "beit3.encoder.layers.4.self_attn_layer_norm.B.bias",
      "beit3.encoder.layers.4.ffn.A.fc1.bias",
      "beit3.encoder.layers.4.ffn.A.fc2.bias",
      "beit3.encoder.layers.4.ffn.A.ffn_layernorm.weight",
      "beit3.encoder.layers.4.ffn.A.ffn_layernorm.bias",
      "beit3.encoder.layers.4.ffn.B.fc1.bias",
      "beit3.encoder.layers.4.ffn.B.fc2.bias",
      "beit3.encoder.layers.4.ffn.B.ffn_layernorm.weight",
      "beit3.encoder.layers.4.ffn.B.ffn_layernorm.bias",
      "beit3.encoder.layers.4.final_layer_norm.A.weight",
      "beit3.encoder.layers.4.final_layer_norm.A.bias",
      "beit3.encoder.layers.4.final_layer_norm.B.weight",
      "beit3.encoder.layers.4.final_layer_norm.B.bias"
    ],
    "lr_scale": 0.03186448128906251
  },
  "layer_6_decay": {
    "weight_decay": 0.05,
    "params": [
      "beit3.encoder.layers.5.self_attn.k_proj.A.weight",
      "beit3.encoder.layers.5.self_attn.k_proj.B.weight",
      "beit3.encoder.layers.5.self_attn.v_proj.A.weight",
      "beit3.encoder.layers.5.self_attn.v_proj.B.weight",
      "beit3.encoder.layers.5.self_attn.q_proj.A.weight",
      "beit3.encoder.layers.5.self_attn.q_proj.B.weight",
      "beit3.encoder.layers.5.self_attn.out_proj.A.weight",
      "beit3.encoder.layers.5.self_attn.out_proj.B.weight",
      "beit3.encoder.layers.5.ffn.A.fc1.weight",
      "beit3.encoder.layers.5.ffn.A.fc2.weight",
      "beit3.encoder.layers.5.ffn.B.fc1.weight",
      "beit3.encoder.layers.5.ffn.B.fc2.weight"
    ],
    "lr_scale": 0.049022278906250015
  },
  "layer_6_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "beit3.encoder.layers.5.self_attn.k_proj.A.bias",
      "beit3.encoder.layers.5.self_attn.k_proj.B.bias",
      "beit3.encoder.layers.5.self_attn.v_proj.A.bias",
      "beit3.encoder.layers.5.self_attn.v_proj.B.bias",
      "beit3.encoder.layers.5.self_attn.q_proj.A.bias",
      "beit3.encoder.layers.5.self_attn.q_proj.B.bias",
      "beit3.encoder.layers.5.self_attn.out_proj.A.bias",
      "beit3.encoder.layers.5.self_attn.out_proj.B.bias",
      "beit3.encoder.layers.5.self_attn.inner_attn_ln.A.weight",
      "beit3.encoder.layers.5.self_attn.inner_attn_ln.A.bias",
      "beit3.encoder.layers.5.self_attn.inner_attn_ln.B.weight",
      "beit3.encoder.layers.5.self_attn.inner_attn_ln.B.bias",
      "beit3.encoder.layers.5.self_attn_layer_norm.A.weight",
      "beit3.encoder.layers.5.self_attn_layer_norm.A.bias",
      "beit3.encoder.layers.5.self_attn_layer_norm.B.weight",
      "beit3.encoder.layers.5.self_attn_layer_norm.B.bias",
      "beit3.encoder.layers.5.ffn.A.fc1.bias",
      "beit3.encoder.layers.5.ffn.A.fc2.bias",
      "beit3.encoder.layers.5.ffn.A.ffn_layernorm.weight",
      "beit3.encoder.layers.5.ffn.A.ffn_layernorm.bias",
      "beit3.encoder.layers.5.ffn.B.fc1.bias",
      "beit3.encoder.layers.5.ffn.B.fc2.bias",
      "beit3.encoder.layers.5.ffn.B.ffn_layernorm.weight",
      "beit3.encoder.layers.5.ffn.B.ffn_layernorm.bias",
      "beit3.encoder.layers.5.final_layer_norm.A.weight",
      "beit3.encoder.layers.5.final_layer_norm.A.bias",
      "beit3.encoder.layers.5.final_layer_norm.B.weight",
      "beit3.encoder.layers.5.final_layer_norm.B.bias"
    ],
    "lr_scale": 0.049022278906250015
  },
  "layer_7_decay": {
    "weight_decay": 0.05,
    "params": [
      "beit3.encoder.layers.6.self_attn.k_proj.A.weight",
      "beit3.encoder.layers.6.self_attn.k_proj.B.weight",
      "beit3.encoder.layers.6.self_attn.v_proj.A.weight",
      "beit3.encoder.layers.6.self_attn.v_proj.B.weight",
      "beit3.encoder.layers.6.self_attn.q_proj.A.weight",
      "beit3.encoder.layers.6.self_attn.q_proj.B.weight",
      "beit3.encoder.layers.6.self_attn.out_proj.A.weight",
      "beit3.encoder.layers.6.self_attn.out_proj.B.weight",
      "beit3.encoder.layers.6.ffn.A.fc1.weight",
      "beit3.encoder.layers.6.ffn.A.fc2.weight",
      "beit3.encoder.layers.6.ffn.B.fc1.weight",
      "beit3.encoder.layers.6.ffn.B.fc2.weight"
    ],
    "lr_scale": 0.07541889062500001
  },
  "layer_7_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "beit3.encoder.layers.6.self_attn.k_proj.A.bias",
      "beit3.encoder.layers.6.self_attn.k_proj.B.bias",
      "beit3.encoder.layers.6.self_attn.v_proj.A.bias",
      "beit3.encoder.layers.6.self_attn.v_proj.B.bias",
      "beit3.encoder.layers.6.self_attn.q_proj.A.bias",
      "beit3.encoder.layers.6.self_attn.q_proj.B.bias",
      "beit3.encoder.layers.6.self_attn.out_proj.A.bias",
      "beit3.encoder.layers.6.self_attn.out_proj.B.bias",
      "beit3.encoder.layers.6.self_attn.inner_attn_ln.A.weight",
      "beit3.encoder.layers.6.self_attn.inner_attn_ln.A.bias",
      "beit3.encoder.layers.6.self_attn.inner_attn_ln.B.weight",
      "beit3.encoder.layers.6.self_attn.inner_attn_ln.B.bias",
      "beit3.encoder.layers.6.self_attn_layer_norm.A.weight",
      "beit3.encoder.layers.6.self_attn_layer_norm.A.bias",
      "beit3.encoder.layers.6.self_attn_layer_norm.B.weight",
      "beit3.encoder.layers.6.self_attn_layer_norm.B.bias",
      "beit3.encoder.layers.6.ffn.A.fc1.bias",
      "beit3.encoder.layers.6.ffn.A.fc2.bias",
      "beit3.encoder.layers.6.ffn.A.ffn_layernorm.weight",
      "beit3.encoder.layers.6.ffn.A.ffn_layernorm.bias",
      "beit3.encoder.layers.6.ffn.B.fc1.bias",
      "beit3.encoder.layers.6.ffn.B.fc2.bias",
      "beit3.encoder.layers.6.ffn.B.ffn_layernorm.weight",
      "beit3.encoder.layers.6.ffn.B.ffn_layernorm.bias",
      "beit3.encoder.layers.6.final_layer_norm.A.weight",
      "beit3.encoder.layers.6.final_layer_norm.A.bias",
      "beit3.encoder.layers.6.final_layer_norm.B.weight",
      "beit3.encoder.layers.6.final_layer_norm.B.bias"
    ],
    "lr_scale": 0.07541889062500001
  },
  "layer_8_decay": {
    "weight_decay": 0.05,
    "params": [
      "beit3.encoder.layers.7.self_attn.k_proj.A.weight",
      "beit3.encoder.layers.7.self_attn.k_proj.B.weight",
      "beit3.encoder.layers.7.self_attn.v_proj.A.weight",
      "beit3.encoder.layers.7.self_attn.v_proj.B.weight",
      "beit3.encoder.layers.7.self_attn.q_proj.A.weight",
      "beit3.encoder.layers.7.self_attn.q_proj.B.weight",
      "beit3.encoder.layers.7.self_attn.out_proj.A.weight",
      "beit3.encoder.layers.7.self_attn.out_proj.B.weight",
      "beit3.encoder.layers.7.ffn.A.fc1.weight",
      "beit3.encoder.layers.7.ffn.A.fc2.weight",
      "beit3.encoder.layers.7.ffn.B.fc1.weight",
      "beit3.encoder.layers.7.ffn.B.fc2.weight"
    ],
    "lr_scale": 0.11602906250000002
  },
  "layer_8_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "beit3.encoder.layers.7.self_attn.k_proj.A.bias",
      "beit3.encoder.layers.7.self_attn.k_proj.B.bias",
      "beit3.encoder.layers.7.self_attn.v_proj.A.bias",
      "beit3.encoder.layers.7.self_attn.v_proj.B.bias",
      "beit3.encoder.layers.7.self_attn.q_proj.A.bias",
      "beit3.encoder.layers.7.self_attn.q_proj.B.bias",
      "beit3.encoder.layers.7.self_attn.out_proj.A.bias",
      "beit3.encoder.layers.7.self_attn.out_proj.B.bias",
      "beit3.encoder.layers.7.self_attn.inner_attn_ln.A.weight",
      "beit3.encoder.layers.7.self_attn.inner_attn_ln.A.bias",
      "beit3.encoder.layers.7.self_attn.inner_attn_ln.B.weight",
      "beit3.encoder.layers.7.self_attn.inner_attn_ln.B.bias",
      "beit3.encoder.layers.7.self_attn_layer_norm.A.weight",
      "beit3.encoder.layers.7.self_attn_layer_norm.A.bias",
      "beit3.encoder.layers.7.self_attn_layer_norm.B.weight",
      "beit3.encoder.layers.7.self_attn_layer_norm.B.bias",
      "beit3.encoder.layers.7.ffn.A.fc1.bias",
      "beit3.encoder.layers.7.ffn.A.fc2.bias",
      "beit3.encoder.layers.7.ffn.A.ffn_layernorm.weight",
      "beit3.encoder.layers.7.ffn.A.ffn_layernorm.bias",
      "beit3.encoder.layers.7.ffn.B.fc1.bias",
      "beit3.encoder.layers.7.ffn.B.fc2.bias",
      "beit3.encoder.layers.7.ffn.B.ffn_layernorm.weight",
      "beit3.encoder.layers.7.ffn.B.ffn_layernorm.bias",
      "beit3.encoder.layers.7.final_layer_norm.A.weight",
      "beit3.encoder.layers.7.final_layer_norm.A.bias",
      "beit3.encoder.layers.7.final_layer_norm.B.weight",
      "beit3.encoder.layers.7.final_layer_norm.B.bias"
    ],
    "lr_scale": 0.11602906250000002
  },
  "layer_9_decay": {
    "weight_decay": 0.05,
    "params": [
      "beit3.encoder.layers.8.self_attn.k_proj.A.weight",
      "beit3.encoder.layers.8.self_attn.k_proj.B.weight",
      "beit3.encoder.layers.8.self_attn.v_proj.A.weight",
      "beit3.encoder.layers.8.self_attn.v_proj.B.weight",
      "beit3.encoder.layers.8.self_attn.q_proj.A.weight",
      "beit3.encoder.layers.8.self_attn.q_proj.B.weight",
      "beit3.encoder.layers.8.self_attn.out_proj.A.weight",
      "beit3.encoder.layers.8.self_attn.out_proj.B.weight",
      "beit3.encoder.layers.8.ffn.A.fc1.weight",
      "beit3.encoder.layers.8.ffn.A.fc2.weight",
      "beit3.encoder.layers.8.ffn.B.fc1.weight",
      "beit3.encoder.layers.8.ffn.B.fc2.weight"
    ],
    "lr_scale": 0.17850625000000003
  },
  "layer_9_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "beit3.encoder.layers.8.self_attn.k_proj.A.bias",
      "beit3.encoder.layers.8.self_attn.k_proj.B.bias",
      "beit3.encoder.layers.8.self_attn.v_proj.A.bias",
      "beit3.encoder.layers.8.self_attn.v_proj.B.bias",
      "beit3.encoder.layers.8.self_attn.q_proj.A.bias",
      "beit3.encoder.layers.8.self_attn.q_proj.B.bias",
      "beit3.encoder.layers.8.self_attn.out_proj.A.bias",
      "beit3.encoder.layers.8.self_attn.out_proj.B.bias",
      "beit3.encoder.layers.8.self_attn.inner_attn_ln.A.weight",
      "beit3.encoder.layers.8.self_attn.inner_attn_ln.A.bias",
      "beit3.encoder.layers.8.self_attn.inner_attn_ln.B.weight",
      "beit3.encoder.layers.8.self_attn.inner_attn_ln.B.bias",
      "beit3.encoder.layers.8.self_attn_layer_norm.A.weight",
      "beit3.encoder.layers.8.self_attn_layer_norm.A.bias",
      "beit3.encoder.layers.8.self_attn_layer_norm.B.weight",
      "beit3.encoder.layers.8.self_attn_layer_norm.B.bias",
      "beit3.encoder.layers.8.ffn.A.fc1.bias",
      "beit3.encoder.layers.8.ffn.A.fc2.bias",
      "beit3.encoder.layers.8.ffn.A.ffn_layernorm.weight",
      "beit3.encoder.layers.8.ffn.A.ffn_layernorm.bias",
      "beit3.encoder.layers.8.ffn.B.fc1.bias",
      "beit3.encoder.layers.8.ffn.B.fc2.bias",
      "beit3.encoder.layers.8.ffn.B.ffn_layernorm.weight",
      "beit3.encoder.layers.8.ffn.B.ffn_layernorm.bias",
      "beit3.encoder.layers.8.final_layer_norm.A.weight",
      "beit3.encoder.layers.8.final_layer_norm.A.bias",
      "beit3.encoder.layers.8.final_layer_norm.B.weight",
      "beit3.encoder.layers.8.final_layer_norm.B.bias"
    ],
    "lr_scale": 0.17850625000000003
  },
  "layer_10_decay": {
    "weight_decay": 0.05,
    "params": [
      "beit3.encoder.layers.9.self_attn.k_proj.A.weight",
      "beit3.encoder.layers.9.self_attn.k_proj.B.weight",
      "beit3.encoder.layers.9.self_attn.v_proj.A.weight",
      "beit3.encoder.layers.9.self_attn.v_proj.B.weight",
      "beit3.encoder.layers.9.self_attn.q_proj.A.weight",
      "beit3.encoder.layers.9.self_attn.q_proj.B.weight",
      "beit3.encoder.layers.9.self_attn.out_proj.A.weight",
      "beit3.encoder.layers.9.self_attn.out_proj.B.weight",
      "beit3.encoder.layers.9.ffn.A.fc1.weight",
      "beit3.encoder.layers.9.ffn.A.fc2.weight",
      "beit3.encoder.layers.9.ffn.B.fc1.weight",
      "beit3.encoder.layers.9.ffn.B.fc2.weight"
    ],
    "lr_scale": 0.274625
  },
  "layer_10_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "beit3.encoder.layers.9.self_attn.k_proj.A.bias",
      "beit3.encoder.layers.9.self_attn.k_proj.B.bias",
      "beit3.encoder.layers.9.self_attn.v_proj.A.bias",
      "beit3.encoder.layers.9.self_attn.v_proj.B.bias",
      "beit3.encoder.layers.9.self_attn.q_proj.A.bias",
      "beit3.encoder.layers.9.self_attn.q_proj.B.bias",
      "beit3.encoder.layers.9.self_attn.out_proj.A.bias",
      "beit3.encoder.layers.9.self_attn.out_proj.B.bias",
      "beit3.encoder.layers.9.self_attn.inner_attn_ln.A.weight",
      "beit3.encoder.layers.9.self_attn.inner_attn_ln.A.bias",
      "beit3.encoder.layers.9.self_attn.inner_attn_ln.B.weight",
      "beit3.encoder.layers.9.self_attn.inner_attn_ln.B.bias",
      "beit3.encoder.layers.9.self_attn_layer_norm.A.weight",
      "beit3.encoder.layers.9.self_attn_layer_norm.A.bias",
      "beit3.encoder.layers.9.self_attn_layer_norm.B.weight",
      "beit3.encoder.layers.9.self_attn_layer_norm.B.bias",
      "beit3.encoder.layers.9.ffn.A.fc1.bias",
      "beit3.encoder.layers.9.ffn.A.fc2.bias",
      "beit3.encoder.layers.9.ffn.A.ffn_layernorm.weight",
      "beit3.encoder.layers.9.ffn.A.ffn_layernorm.bias",
      "beit3.encoder.layers.9.ffn.B.fc1.bias",
      "beit3.encoder.layers.9.ffn.B.fc2.bias",
      "beit3.encoder.layers.9.ffn.B.ffn_layernorm.weight",
      "beit3.encoder.layers.9.ffn.B.ffn_layernorm.bias",
      "beit3.encoder.layers.9.final_layer_norm.A.weight",
      "beit3.encoder.layers.9.final_layer_norm.A.bias",
      "beit3.encoder.layers.9.final_layer_norm.B.weight",
      "beit3.encoder.layers.9.final_layer_norm.B.bias"
    ],
    "lr_scale": 0.274625
  },
  "layer_11_decay": {
    "weight_decay": 0.05,
    "params": [
      "beit3.encoder.layers.10.self_attn.k_proj.A.weight",
      "beit3.encoder.layers.10.self_attn.k_proj.B.weight",
      "beit3.encoder.layers.10.self_attn.v_proj.A.weight",
      "beit3.encoder.layers.10.self_attn.v_proj.B.weight",
      "beit3.encoder.layers.10.self_attn.q_proj.A.weight",
      "beit3.encoder.layers.10.self_attn.q_proj.B.weight",
      "beit3.encoder.layers.10.self_attn.out_proj.A.weight",
      "beit3.encoder.layers.10.self_attn.out_proj.B.weight",
      "beit3.encoder.layers.10.ffn.A.fc1.weight",
      "beit3.encoder.layers.10.ffn.A.fc2.weight",
      "beit3.encoder.layers.10.ffn.B.fc1.weight",
      "beit3.encoder.layers.10.ffn.B.fc2.weight"
    ],
    "lr_scale": 0.42250000000000004
  },
  "layer_11_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "beit3.encoder.layers.10.self_attn.k_proj.A.bias",
      "beit3.encoder.layers.10.self_attn.k_proj.B.bias",
      "beit3.encoder.layers.10.self_attn.v_proj.A.bias",
      "beit3.encoder.layers.10.self_attn.v_proj.B.bias",
      "beit3.encoder.layers.10.self_attn.q_proj.A.bias",
      "beit3.encoder.layers.10.self_attn.q_proj.B.bias",
      "beit3.encoder.layers.10.self_attn.out_proj.A.bias",
      "beit3.encoder.layers.10.self_attn.out_proj.B.bias",
      "beit3.encoder.layers.10.self_attn.inner_attn_ln.A.weight",
      "beit3.encoder.layers.10.self_attn.inner_attn_ln.A.bias",
      "beit3.encoder.layers.10.self_attn.inner_attn_ln.B.weight",
      "beit3.encoder.layers.10.self_attn.inner_attn_ln.B.bias",
      "beit3.encoder.layers.10.self_attn_layer_norm.A.weight",
      "beit3.encoder.layers.10.self_attn_layer_norm.A.bias",
      "beit3.encoder.layers.10.self_attn_layer_norm.B.weight",
      "beit3.encoder.layers.10.self_attn_layer_norm.B.bias",
      "beit3.encoder.layers.10.ffn.A.fc1.bias",
      "beit3.encoder.layers.10.ffn.A.fc2.bias",
      "beit3.encoder.layers.10.ffn.A.ffn_layernorm.weight",
      "beit3.encoder.layers.10.ffn.A.ffn_layernorm.bias",
      "beit3.encoder.layers.10.ffn.B.fc1.bias",
      "beit3.encoder.layers.10.ffn.B.fc2.bias",
      "beit3.encoder.layers.10.ffn.B.ffn_layernorm.weight",
      "beit3.encoder.layers.10.ffn.B.ffn_layernorm.bias",
      "beit3.encoder.layers.10.final_layer_norm.A.weight",
      "beit3.encoder.layers.10.final_layer_norm.A.bias",
      "beit3.encoder.layers.10.final_layer_norm.B.weight",
      "beit3.encoder.layers.10.final_layer_norm.B.bias"
    ],
    "lr_scale": 0.42250000000000004
  },
  "layer_12_decay": {
    "weight_decay": 0.05,
    "params": [
      "beit3.encoder.layers.11.self_attn.k_proj.A.weight",
      "beit3.encoder.layers.11.self_attn.k_proj.B.weight",
      "beit3.encoder.layers.11.self_attn.v_proj.A.weight",
      "beit3.encoder.layers.11.self_attn.v_proj.B.weight",
      "beit3.encoder.layers.11.self_attn.q_proj.A.weight",
      "beit3.encoder.layers.11.self_attn.q_proj.B.weight",
      "beit3.encoder.layers.11.self_attn.out_proj.A.weight",
      "beit3.encoder.layers.11.self_attn.out_proj.B.weight",
      "beit3.encoder.layers.11.ffn.A.fc1.weight",
      "beit3.encoder.layers.11.ffn.A.fc2.weight",
      "beit3.encoder.layers.11.ffn.B.fc1.weight",
      "beit3.encoder.layers.11.ffn.B.fc2.weight"
    ],
    "lr_scale": 0.65
  },
  "layer_12_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "beit3.encoder.layers.11.self_attn.k_proj.A.bias",
      "beit3.encoder.layers.11.self_attn.k_proj.B.bias",
      "beit3.encoder.layers.11.self_attn.v_proj.A.bias",
      "beit3.encoder.layers.11.self_attn.v_proj.B.bias",
      "beit3.encoder.layers.11.self_attn.q_proj.A.bias",
      "beit3.encoder.layers.11.self_attn.q_proj.B.bias",
      "beit3.encoder.layers.11.self_attn.out_proj.A.bias",
      "beit3.encoder.layers.11.self_attn.out_proj.B.bias",
      "beit3.encoder.layers.11.self_attn.inner_attn_ln.A.weight",
      "beit3.encoder.layers.11.self_attn.inner_attn_ln.A.bias",
      "beit3.encoder.layers.11.self_attn.inner_attn_ln.B.weight",
      "beit3.encoder.layers.11.self_attn.inner_attn_ln.B.bias",
      "beit3.encoder.layers.11.self_attn_layer_norm.A.weight",
      "beit3.encoder.layers.11.self_attn_layer_norm.A.bias",
      "beit3.encoder.layers.11.self_attn_layer_norm.B.weight",
      "beit3.encoder.layers.11.self_attn_layer_norm.B.bias",
      "beit3.encoder.layers.11.ffn.A.fc1.bias",
      "beit3.encoder.layers.11.ffn.A.fc2.bias",
      "beit3.encoder.layers.11.ffn.A.ffn_layernorm.weight",
      "beit3.encoder.layers.11.ffn.A.ffn_layernorm.bias",
      "beit3.encoder.layers.11.ffn.B.fc1.bias",
      "beit3.encoder.layers.11.ffn.B.fc2.bias",
      "beit3.encoder.layers.11.ffn.B.ffn_layernorm.weight",
      "beit3.encoder.layers.11.ffn.B.ffn_layernorm.bias",
      "beit3.encoder.layers.11.final_layer_norm.A.weight",
      "beit3.encoder.layers.11.final_layer_norm.A.bias",
      "beit3.encoder.layers.11.final_layer_norm.B.weight",
      "beit3.encoder.layers.11.final_layer_norm.B.bias"
    ],
    "lr_scale": 0.65
  },
  "layer_13_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "fc_norm.weight",
      "fc_norm.bias",
      "head.bias"
    ],
    "lr_scale": 1.0
  },
  "layer_13_decay": {
    "weight_decay": 0.05,
    "params": [
      "head.weight"
    ],
    "lr_scale": 1.0
  }
}
Set warmup steps = 5050
Auto resume checkpoint: 
Mixup is activated!
Start training for 50 epochs
 Traceback (most recent call last) 
 /nas/huong/projects/unilm/beit3/run_beit3_finetuning.py:450 in <module>                          
                                                                                                  
   447    opts, ds_init = get_args()                                                             
   448    if opts.output_dir:                                                                    
   449       Path(opts.output_dir).mkdir(parents=True, exist_ok=True)                           
  450    main(opts, ds_init)                                                                    
   451                                                                                            
                                                                                                  
 /nas/huong/projects/unilm/beit3/run_beit3_finetuning.py:388 in main                              
                                                                                                  
   385          data_loader_train.sampler.set_epoch(epoch)                                     
   386       if log_writer is not None:                                                         
   387          log_writer.set_step(epoch * num_training_steps_per_epoch * args.update_freq)   
  388       train_stats = train_one_epoch(                                                     
   389          model, data_loader_train, optimizer, device, task_handler, epoch,              
   390          epoch * num_training_steps_per_epoch, lr_schedule_values, loss_scaler,         
   391          args.clip_grad, args.update_freq, model_ema, log_writer, args.task, mixup_fn   
                                                                                                  
 /nas/huong/projects/unilm/beit3/engine_for_finetuning.py:527 in train_one_epoch                  
                                                                                                  
   524          # this attribute is added by timm on one optimizer (adahessian)                
   525          is_second_order = hasattr(optimizer, 'is_second_order') and optimizer.is_sec   
   526          loss /= update_freq                                                            
  527          grad_norm = loss_scaler(loss, optimizer, clip_grad=max_norm,                   
   528                            parameters=model.parameters(), create_graph=is_secon   
   529                            update_grad=(data_iter_step + 1) % update_freq == 0)   
   530          if (data_iter_step + 1) % update_freq == 0:                                    
                                                                                                  
 /nas/huong/projects/unilm/beit3/utils.py:391 in __call__                                         
                                                                                                  
   388          else:                                                                          
   389             self._scaler.unscale_(optimizer)                                           
   390             norm = get_grad_norm_(parameters)                                          
  391          self._scaler.step(optimizer)                                                   
   392          self._scaler.update()                                                          
   393       else:                                                                              
   394          norm = None                                                                    
                                                                                                  
 /nas/.conda/envs/hf/lib/python3.8/site-packages/torch/_M/driver/wrapper/moreh_wrapper.py:126 in  
 wrapper                                                                                          
                                                                                                  
   123                   f'{type(instance).__name__}: {instance}')                           
   124             if 'hasattr' not in traceback_str:                                         
   125                frontend.push_message_to_loki(msg)                                     
  126             raise instance                                                             
   127                                                                                         
   128          except Exception as instance:                                                  
   129             traceback_str = ''.join(traceback.format_stack()[:-1])                     
                                                                                                  
 /nas/.conda/envs/hf/lib/python3.8/site-packages/torch/_M/driver/wrapper/moreh_wrapper.py:74 in   
 wrapper                                                                                          
                                                                                                  
    71                      funcname, *args, **kwargs):                                    
    72                   return torch_function(*args, **kwargs)                             
    73                                                                                       
   74                return moreh_function(                                                 
    75                   *args, **handle_axis_and_keepdims(funcname, **kwargs))             
    76                                                                                        
    77             except moreh_exceptions.FallBackException:                                 
                                                                                                  
 /nas/.conda/envs/hf/lib/python3.8/site-packages/torch/_M/driver/cuda/amp/grad_scaler.py:200 in   
 step                                                                                             
                                                                                                  
   197       if _is_moreh_predefined_optimizer(optimizer):                                      
   198          enable = torch.bitwise_not(                                                    
   199             optimizer_state['found_inf_per_device'].type(torch.bool))                  
  200          retval = _moreh_predefined_optimizer_step(optimizer, enable,                   
   201                                          self._scale, *args,                  
   202                                          **kwargs)                            
   203       elif optimizer_state['found_inf_per_device'] == 0:                                 
                                                                                                  
 /nas/.conda/envs/hf/lib/python3.8/site-packages/torch/_M/driver/cuda/amp/grad_scaler.py:49 in    
 _moreh_predefined_optimizer_step                                                                 
                                                                                                  
    46 def _moreh_predefined_optimizer_step(optimizer, enable, scale, *args, **kwargs):           
    47    new_step = types.MethodType(_get_moreh_predefined_step_method(optimizer),              
    48                         optimizer)                                                 
   49    return new_step(enable, scale, *args, **kwargs)                                        
    50                                                                                            
    51                                                                                            
    52 def _moreh_amp_update_scale(growth_tracker, current_scale, found_inf,                      
                                                                                                  
 /nas/.conda/envs/hf/lib/python3.8/site-packages/torch/_M/driver/cuda/amp/moreh_steps.py:201 in   
 step_optim_AdamW                                                                                 
                                                                                                  
   198          group['step'] = 1                                                              
   199                                                                                          
   200       for p in group['params']:                                                          
  201          if p.grad.data.is_sparse:                                                      
   202             raise RuntimeError(                                                        
   203                'FusedAdamW does not support sparse gradients, please consider Spars   
   204             )                                                                          

AttributeError: 'NoneType' object has no attribute 'data'
